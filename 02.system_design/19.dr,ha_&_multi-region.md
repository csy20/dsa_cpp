# Disaster Recovery, High Availability & Multi-Region

## What Is Disaster Recovery & High Availability?

**Ensure systems survive failures and disasters**

```
Disaster Recovery (DR): Recover from catastrophic failures
â”œâ”€ Data center fire, flood, earthquake
â”œâ”€ Entire region outage (AWS us-east-1 down)
â”œâ”€ Ransomware, data corruption
â””â”€ Human error (deleted production database)

High Availability (HA): Minimize downtime from failures
â”œâ”€ Server crash, disk failure
â”œâ”€ Network partition, switch failure
â”œâ”€ Deployment errors
â””â”€ Planned maintenance (upgrades)

Key metrics:
â”œâ”€ RPO (Recovery Point Objective): Max data loss (time)
â”œâ”€ RTO (Recovery Time Objective): Max downtime (time)
â”œâ”€ Availability: Uptime percentage (99.9%, 99.99%)
â””â”€ MTTR (Mean Time To Recovery): Average recovery time

Availability table:
99%       = 3.65 days downtime/year
99.9%     = 8.76 hours downtime/year
99.95%    = 4.38 hours downtime/year
99.99%    = 52.56 minutes downtime/year (4 nines)
99.999%   = 5.26 minutes downtime/year (5 nines)
99.9999%  = 31.56 seconds downtime/year (6 nines)
```

---

## 1. Recovery Objectives

### 1.1 RPO (Recovery Point Objective)

**Maximum acceptable data loss**

```
RPO: How much data can we lose?

Example scenarios:

RPO = 0 (zero data loss):
â”œâ”€ Synchronous replication (every write replicated)
â”œâ”€ Use case: Financial transactions, payment systems
â””â”€ Cost: High (performance impact, expensive)

RPO = 15 minutes:
â”œâ”€ Asynchronous replication (writes replicated every 15 min)
â”œâ”€ Use case: E-commerce, social media
â””â”€ Cost: Medium (some data loss acceptable)

RPO = 24 hours:
â”œâ”€ Daily backups (snapshot every night)
â”œâ”€ Use case: Analytics, reporting systems
â””â”€ Cost: Low (cheap storage)

Disaster at 10:30 AM:

RPO = 0:        Lose 0 data (replicated immediately)
RPO = 15 min:   Lose data from 10:15-10:30 (15 min)
RPO = 24 hours: Lose data from yesterday's backup (max 24h)

Lower RPO = More expensive (frequent backups/replication)
```

### 1.2 RTO (Recovery Time Objective)

**Maximum acceptable downtime**

```
RTO: How quickly must we recover?

Example scenarios:

RTO = 0 (zero downtime):
â”œâ”€ Active-active multi-region (instant failover)
â”œâ”€ Use case: Critical services (911, healthcare)
â””â”€ Cost: Very high (duplicate infrastructure)

RTO = 1 hour:
â”œâ”€ Hot standby (running, ready to take traffic)
â”œâ”€ Use case: E-commerce, SaaS applications
â””â”€ Cost: High (standby servers running)

RTO = 4 hours:
â”œâ”€ Warm standby (infrastructure ready, data needs restore)
â”œâ”€ Use case: Internal tools, non-critical apps
â””â”€ Cost: Medium (some infrastructure pre-provisioned)

RTO = 24 hours:
â”œâ”€ Cold standby (restore from backups)
â”œâ”€ Use case: Archived data, compliance systems
â””â”€ Cost: Low (only backup storage)

Disaster at 10:00 AM:

RTO = 0:        Service never down (automatic failover)
RTO = 1 hour:   Service restored by 11:00 AM
RTO = 4 hours:  Service restored by 2:00 PM
RTO = 24 hours: Service restored by 10:00 AM next day

Lower RTO = More expensive (faster recovery infrastructure)
```

**RPO vs RTO Trade-offs:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RPO vs RTO Matrix                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚             â”‚  RTO (Downtime)       â”‚
â”‚  RPO        â”œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  (Data      â”‚ Low  â”‚ Med  â”‚  High   â”‚
â”‚   Loss)     â”‚ (min)â”‚(hours)â”‚ (days) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Low (0-15m) â”‚ $$$$ â”‚ $$$  â”‚  $$    â”‚
â”‚             â”‚Activeâ”‚Hot   â”‚ Warm   â”‚
â”‚             â”‚Activeâ”‚Stanbyâ”‚Standby â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Med (1-4h)  â”‚ $$$  â”‚ $$   â”‚  $     â”‚
â”‚             â”‚Hot   â”‚Warm  â”‚ Cold   â”‚
â”‚             â”‚Stanbyâ”‚Stanbyâ”‚Standby â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ High (24h+) â”‚ $$   â”‚ $    â”‚  $     â”‚
â”‚             â”‚Warm  â”‚Cold  â”‚ Backup â”‚
â”‚             â”‚Stanbyâ”‚Stanbyâ”‚  Only  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Choose based on business requirements:
â”œâ”€ Payment processing: RPO=0, RTO=0 (active-active)
â”œâ”€ E-commerce: RPO=15min, RTO=1hour (hot standby)
â”œâ”€ Analytics: RPO=24hours, RTO=4hours (warm standby)
â””â”€ Archives: RPO=7days, RTO=24hours (cold standby)
```

---

## 2. Backup & Restore Strategies

### 2.1 Backup Types

```
Backup strategies:

1. Full Backup (complete copy):
   â”œâ”€ Copy all data
   â”œâ”€ Slowest, largest
   â””â”€ Example: Sunday night full backup

2. Incremental Backup (changes since last backup):
   â”œâ”€ Copy only changed data since last backup
   â”œâ”€ Fast, small
   â”œâ”€ Restore: Full + all incrementals (slow)
   â””â”€ Example: Monday incremental, Tuesday incremental

3. Differential Backup (changes since last full):
   â”œâ”€ Copy changed data since last full backup
   â”œâ”€ Medium speed, medium size
   â”œâ”€ Restore: Full + latest differential (faster)
   â””â”€ Example: Full Sunday, differential Mon-Sat

Backup schedule example:
Sunday:    Full backup (100 GB)
Monday:    Incremental (5 GB) - changes since Sunday
Tuesday:   Incremental (3 GB) - changes since Monday
Wednesday: Incremental (4 GB) - changes since Tuesday

Restore Wednesday:
â”œâ”€ Incremental: Full + Mon + Tue + Wed (4 restores)
â””â”€ Differential: Full + Wed (2 restores)
```

**Database Backups (PostgreSQL):**

```bash
# Full backup (pg_dump)
pg_dump -h localhost -U admin -d mydb -F c -f /backups/mydb_full_$(date +%Y%m%d).dump

# Restore
pg_restore -h localhost -U admin -d mydb /backups/mydb_full_20231123.dump

# Continuous archiving (WAL archiving for PITR)
# postgresql.conf
wal_level = replica
archive_mode = on
archive_command = 'cp %p /backups/wal/%f'

# Base backup (full backup while database running)
pg_basebackup -h localhost -U replication -D /backups/base -Fp -Xs -P

# Point-in-Time Recovery (PITR)
# Restore to specific timestamp (e.g., before accidental DELETE)
# 1. Restore base backup
# 2. Apply WAL files until target time
# recovery.conf
restore_command = 'cp /backups/wal/%f %p'
recovery_target_time = '2023-11-23 14:30:00'
```

**Automated Backup Script:**

```python
import boto3
import subprocess
from datetime import datetime, timedelta

def backup_database():
    """Automated database backup to S3"""
    
    # Generate backup filename
    timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
    backup_file = f'/tmp/db_backup_{timestamp}.dump'
    
    # Run pg_dump
    subprocess.run([
        'pg_dump',
        '-h', 'localhost',
        '-U', 'admin',
        '-d', 'production',
        '-F', 'c',
        '-f', backup_file
    ], check=True)
    
    # Upload to S3
    s3 = boto3.client('s3')
    s3.upload_file(
        backup_file,
        'myapp-backups',
        f'database/{timestamp}/backup.dump'
    )
    
    # Cleanup local file
    os.remove(backup_file)
    
    print(f"Backup completed: {timestamp}")

def cleanup_old_backups():
    """Delete backups older than 30 days"""
    s3 = boto3.client('s3')
    
    cutoff_date = datetime.utcnow() - timedelta(days=30)
    
    # List all backups
    response = s3.list_objects_v2(
        Bucket='myapp-backups',
        Prefix='database/'
    )
    
    for obj in response.get('Contents', []):
        # Parse timestamp from key
        timestamp_str = obj['Key'].split('/')[1]
        timestamp = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')
        
        # Delete if older than 30 days
        if timestamp < cutoff_date:
            s3.delete_object(Bucket='myapp-backups', Key=obj['Key'])
            print(f"Deleted old backup: {obj['Key']}")

# Run daily (cron job)
# 0 2 * * * python backup_database.py
```

### 2.2 Point-in-Time Recovery (PITR)

**Restore to specific point in time**

```
PITR: Recover to exact moment before disaster

Use cases:
â”œâ”€ Accidental DELETE (restore to 1 minute before)
â”œâ”€ Bad deployment (restore to before deploy)
â”œâ”€ Ransomware (restore to before encryption)
â””â”€ Data corruption (restore to last known good state)

How PITR works:

1. Base backup (full backup at midnight)
2. Transaction logs (every change recorded)
3. Restore process:
   â”œâ”€ Restore base backup
   â”œâ”€ Replay transaction logs up to target time
   â””â”€ Database in exact state at target time

Timeline:
Midnight:     Base backup
00:01-09:00:  Transaction logs
09:00:        Accidental DELETE
09:30:        Discovered issue

PITR to 08:59 (1 minute before DELETE):
â”œâ”€ Restore base backup (midnight)
â”œâ”€ Replay logs from 00:01 to 08:59
â””â”€ Database has all data, DELETE never happened

RPO with PITR = Log retention (e.g., 5 minutes)
```

**PITR with AWS RDS:**

```python
import boto3

def restore_to_point_in_time():
    """Restore RDS database to specific time"""
    
    rds = boto3.client('rds')
    
    # Restore to 1 hour ago
    target_time = datetime.utcnow() - timedelta(hours=1)
    
    response = rds.restore_db_instance_to_point_in_time(
        SourceDBInstanceIdentifier='production-db',
        TargetDBInstanceIdentifier='production-db-restored',
        RestoreTime=target_time,
        UseLatestRestorableTime=False  # Use specific time
    )
    
    print(f"Restoring to {target_time}...")
    
    # Wait for restore to complete
    waiter = rds.get_waiter('db_instance_available')
    waiter.wait(DBInstanceIdentifier='production-db-restored')
    
    print("Restore completed!")
    
    # Verify data
    # Connect to production-db-restored and check data
    # If correct, promote to production

# Restore to latest restorable time (most recent)
def restore_to_latest():
    rds = boto3.client('rds')
    
    response = rds.restore_db_instance_to_point_in_time(
        SourceDBInstanceIdentifier='production-db',
        TargetDBInstanceIdentifier='production-db-restored',
        UseLatestRestorableTime=True  # Most recent possible
    )
```

### 2.3 Backup Best Practices

```
Backup strategy:
âœ“ 3-2-1 rule:
  â”œâ”€ 3 copies of data (production + 2 backups)
  â”œâ”€ 2 different media types (disk + tape/cloud)
  â””â”€ 1 offsite backup (different location)

âœ“ Automated backups (no manual intervention)
âœ“ Encrypted backups (protect sensitive data)
âœ“ Test restores (verify backups work)
âœ“ Monitor backup jobs (alert on failures)
âœ“ Retention policy (keep backups 30-90 days)
âœ“ Incremental backups (reduce storage cost)

Backup testing:
â”œâ”€ Monthly: Restore backup to staging
â”œâ”€ Quarterly: Full disaster recovery drill
â””â”€ Annually: Test restore from oldest backup
```

---

## 3. Standby Strategies

### 3.1 Cold Standby

**Restore from backups when needed**

```
Cold Standby:

Primary Region:
â”œâ”€ Running application
â”œâ”€ Production database
â””â”€ Daily backups to S3

Disaster Recovery Region:
â”œâ”€ No infrastructure (or minimal)
â”œâ”€ Backups stored in S3
â””â”€ Runbooks for manual recovery

Disaster occurs:
1. Provision infrastructure (EC2, RDS)
2. Restore latest backup
3. Update DNS (point to DR region)
4. Service restored (hours)

RTO: 4-24 hours (manual provisioning + restore)
RPO: Hours to days (last backup)
Cost: $ (only backup storage)

Use case: Non-critical systems, low budget
```

### 3.2 Warm Standby

**Infrastructure ready, minimal traffic**

```
Warm Standby:

Primary Region:
â”œâ”€ Running application (full capacity)
â”œâ”€ Production database
â””â”€ Continuous replication to DR

Disaster Recovery Region:
â”œâ”€ Infrastructure provisioned (minimal capacity)
â”œâ”€ Database replica (async replication)
â””â”€ Application running (no traffic)

Disaster occurs:
1. Scale up DR infrastructure (increase capacity)
2. Promote replica to primary
3. Update DNS (point to DR region)
4. Service restored (minutes to hours)

RTO: 1-4 hours (scale up + promote)
RPO: Minutes (async replication lag)
Cost: $$ (infrastructure + replication)

Use case: Important systems, moderate budget
```

**Warm Standby Setup (AWS):**

```python
# Primary region: us-east-1
# DR region: us-west-2

# 1. Database replication (RDS read replica)
import boto3

rds = boto3.client('rds', region_name='us-east-1')

# Create read replica in DR region
response = rds.create_db_instance_read_replica(
    DBInstanceIdentifier='production-db-dr',
    SourceDBInstanceIdentifier='production-db',
    DBInstanceClass='db.t3.small',  # Smaller instance (warm)
    AvailabilityZone='us-west-2a',
    PubliclyAccessible=False
)

# 2. Application in DR region (minimal capacity)
# Auto Scaling Group with min=1, max=10 (vs primary min=10, max=50)

# 3. Disaster: Scale up DR
autoscaling = boto3.client('autoscaling', region_name='us-west-2')

autoscaling.update_auto_scaling_group(
    AutoScalingGroupName='app-dr',
    MinSize=10,
    MaxSize=50,
    DesiredCapacity=10
)

# 4. Promote read replica to primary
rds_dr = boto3.client('rds', region_name='us-west-2')

rds_dr.promote_read_replica(
    DBInstanceIdentifier='production-db-dr'
)

# 5. Update DNS (Route 53)
route53 = boto3.client('route53')

route53.change_resource_record_sets(
    HostedZoneId='Z123456789',
    ChangeBatch={
        'Changes': [{
            'Action': 'UPSERT',
            'ResourceRecordSet': {
                'Name': 'api.example.com',
                'Type': 'A',
                'TTL': 60,
                'ResourceRecords': [{'Value': 'DR_LOAD_BALANCER_IP'}]
            }
        }]
    }
)
```

### 3.3 Hot Standby

**Full infrastructure, ready for traffic**

```
Hot Standby:

Primary Region:
â”œâ”€ Running application (full capacity)
â”œâ”€ Production database
â””â”€ Synchronous replication to DR

Disaster Recovery Region:
â”œâ”€ Full infrastructure (same capacity as primary)
â”œâ”€ Database replica (sync or async replication)
â””â”€ Application running (ready for traffic)

Disaster occurs:
1. Promote replica to primary (automatic or manual)
2. Update DNS (point to DR region)
3. Service restored (seconds to minutes)

RTO: Minutes (promote + DNS propagation)
RPO: Seconds to minutes (replication lag)
Cost: $$$ (duplicate infrastructure)

Use case: Critical systems, high availability requirements
```

**Hot Standby with Auto-Failover:**

```python
# Health check + automatic failover (Route 53)

route53 = boto3.client('route53')

# Create health check for primary region
health_check = route53.create_health_check(
    HealthCheckConfig={
        'Type': 'HTTPS',
        'ResourcePath': '/health',
        'FullyQualifiedDomainName': 'primary-lb.example.com',
        'Port': 443,
        'RequestInterval': 30,
        'FailureThreshold': 3
    }
)

# Failover routing (primary -> DR)
route53.change_resource_record_sets(
    HostedZoneId='Z123456789',
    ChangeBatch={
        'Changes': [
            {
                'Action': 'CREATE',
                'ResourceRecordSet': {
                    'Name': 'api.example.com',
                    'Type': 'A',
                    'SetIdentifier': 'Primary',
                    'Failover': 'PRIMARY',
                    'AliasTarget': {
                        'HostedZoneId': 'PRIMARY_LB_ZONE',
                        'DNSName': 'primary-lb.example.com',
                        'EvaluateTargetHealth': True
                    },
                    'HealthCheckId': health_check['HealthCheck']['Id']
                }
            },
            {
                'Action': 'CREATE',
                'ResourceRecordSet': {
                    'Name': 'api.example.com',
                    'Type': 'A',
                    'SetIdentifier': 'Secondary',
                    'Failover': 'SECONDARY',
                    'AliasTarget': {
                        'HostedZoneId': 'DR_LB_ZONE',
                        'DNSName': 'dr-lb.example.com',
                        'EvaluateTargetHealth': False
                    }
                }
            }
        ]
    }
)

# Automatic failover:
# 1. Primary health check fails (3 consecutive failures)
# 2. Route 53 automatically routes to Secondary (DR)
# 3. No manual intervention
```

---

## 4. Multi-Region Strategies

### 4.1 Active-Passive

**One region serves traffic, other on standby**

```
Active-Passive (traditional DR):

Region 1 (Active):
â”œâ”€ 100% traffic
â”œâ”€ Read + write operations
â””â”€ Replicates to passive region

Region 2 (Passive):
â”œâ”€ 0% traffic (standby)
â”œâ”€ Receives replication
â””â”€ Promoted to active on failure

Failover:
Region 1 fails â†’ Region 2 becomes active

Benefits:
+ Simple (one active region)
+ No conflict resolution (one writer)
+ Lower cost (passive is minimal)

Drawbacks:
- Wasted resources (passive idle)
- Failover time (RTO > 0)
- Data loss risk (replication lag)

Architecture:

    Users
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Region 1    â”‚ (Active)
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  App   â”‚  â”‚ â† 100% traffic
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   DB   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“ (replication)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Region 2    â”‚ (Passive)
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  App   â”‚  â”‚ â† 0% traffic
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ DB (R)  â”‚ â”‚ (replica)
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Active-Active

**Multiple regions serve traffic simultaneously**

```
Active-Active (multi-region):

Region 1:
â”œâ”€ 50% traffic
â”œâ”€ Read + write operations
â””â”€ Replicates to Region 2

Region 2:
â”œâ”€ 50% traffic
â”œâ”€ Read + write operations
â””â”€ Replicates to Region 1

Benefits:
+ No wasted resources (both regions active)
+ Better performance (users routed to nearest region)
+ Zero downtime failover (traffic shifts automatically)
+ Higher availability (one region fails, other continues)

Challenges:
- Conflict resolution (concurrent writes to same data)
- Data consistency (eventual consistency)
- Complexity (distributed coordination)

Architecture:

    Users
   â†™    â†˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â†â†’  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Region 1    â”‚ (bi-directional) â”‚  Region 2    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  replication â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  App   â”‚  â”‚      â”‚  â”‚  App   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   DB   â”‚  â”‚ â†â”€â”€â†’ â”‚  â”‚   DB   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†‘                      â†‘
   50% traffic          50% traffic
```

**Active-Active Routing (Geolocation):**

```python
# Route 53 geolocation routing
route53 = boto3.client('route53')

route53.change_resource_record_sets(
    HostedZoneId='Z123456789',
    ChangeBatch={
        'Changes': [
            {
                'Action': 'CREATE',
                'ResourceRecordSet': {
                    'Name': 'api.example.com',
                    'Type': 'A',
                    'SetIdentifier': 'US-East',
                    'GeoLocation': {
                        'ContinentCode': 'NA'  # North America
                    },
                    'AliasTarget': {
                        'HostedZoneId': 'US_EAST_LB_ZONE',
                        'DNSName': 'us-east-lb.example.com',
                        'EvaluateTargetHealth': True
                    }
                }
            },
            {
                'Action': 'CREATE',
                'ResourceRecordSet': {
                    'Name': 'api.example.com',
                    'Type': 'A',
                    'SetIdentifier': 'EU-West',
                    'GeoLocation': {
                        'ContinentCode': 'EU'  # Europe
                    },
                    'AliasTarget': {
                        'HostedZoneId': 'EU_WEST_LB_ZONE',
                        'DNSName': 'eu-west-lb.example.com',
                        'EvaluateTargetHealth': True
                    }
                }
            },
            {
                'Action': 'CREATE',
                'ResourceRecordSet': {
                    'Name': 'api.example.com',
                    'Type': 'A',
                    'SetIdentifier': 'Default',
                    'GeoLocation': {
                        'ContinentCode': '*'  # Default (all other locations)
                    },
                    'AliasTarget': {
                        'HostedZoneId': 'US_EAST_LB_ZONE',
                        'DNSName': 'us-east-lb.example.com',
                        'EvaluateTargetHealth': True
                    }
                }
            }
        ]
    }
)

# Users in North America â†’ US-East region
# Users in Europe â†’ EU-West region
# Users elsewhere â†’ Default (US-East)
```

### 4.3 Conflict Resolution (Active-Active)

**Handle concurrent writes to same data**

```
Conflict scenario:

User A (US): Update user.name = "Alice" at 10:00:00
User B (EU): Update user.name = "Bob"   at 10:00:01

Both regions have different values!

Conflict resolution strategies:

1. Last-Write-Wins (LWW):
   â”œâ”€ Use timestamp to determine winner
   â”œâ”€ Latest write wins (Bob at 10:00:01)
   â””â”€ Simple, but data loss (Alice's write lost)

2. Version Vectors (Causal Consistency):
   â”œâ”€ Track causality (which update saw which)
   â”œâ”€ Detect concurrent writes
   â””â”€ Application resolves conflict

3. CRDTs (Conflict-Free Replicated Data Types):
   â”œâ”€ Math guarantees convergence
   â”œâ”€ All replicas converge to same value
   â””â”€ Example: Counter, Set, Map

4. Application-Level Resolution:
   â”œâ”€ Store both values
   â”œâ”€ Show conflict to user
   â””â”€ User decides which to keep
```

**Last-Write-Wins (DynamoDB):**

```python
import boto3
from decimal import Decimal

dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('users')

# Update with timestamp
def update_user(user_id, name):
    timestamp = Decimal(time.time())
    
    table.update_item(
        Key={'user_id': user_id},
        UpdateExpression='SET #name = :name, #timestamp = :timestamp',
        ExpressionAttributeNames={
            '#name': 'name',
            '#timestamp': 'updated_at'
        },
        ExpressionAttributeValues={
            ':name': name,
            ':timestamp': timestamp
        },
        # Only update if new timestamp is greater (LWW)
        ConditionExpression='attribute_not_exists(updated_at) OR updated_at < :timestamp'
    )

# DynamoDB Global Tables (multi-region replication):
# - Automatic replication across regions
# - Last-Write-Wins conflict resolution
# - Eventual consistency (seconds)
```

**CRDTs (Conflict-Free Replicated Data Types):**

```python
# G-Counter (Grow-only Counter) CRDT
class GCounter:
    def __init__(self, replica_id):
        self.replica_id = replica_id
        self.counts = {}  # {replica_id: count}
    
    def increment(self, amount=1):
        """Increment counter (local)"""
        if self.replica_id not in self.counts:
            self.counts[self.replica_id] = 0
        self.counts[self.replica_id] += amount
    
    def value(self):
        """Get current value (sum of all replicas)"""
        return sum(self.counts.values())
    
    def merge(self, other):
        """Merge with another replica"""
        for replica_id, count in other.counts.items():
            if replica_id not in self.counts:
                self.counts[replica_id] = count
            else:
                # Max (since counter only grows)
                self.counts[replica_id] = max(self.counts[replica_id], count)

# Usage (active-active)
# Region 1
counter1 = GCounter('us-east-1')
counter1.increment(5)  # counts = {'us-east-1': 5}

# Region 2
counter2 = GCounter('eu-west-1')
counter2.increment(3)  # counts = {'eu-west-1': 3}

# Replicate: Region 1 â† Region 2
counter1.merge(counter2)
print(counter1.value())  # 8 (5 + 3)

# Replicate: Region 2 â† Region 1
counter2.merge(counter1)
print(counter2.value())  # 8 (converged!)

# No conflicts, both regions converge to same value
```

**Application-Level Conflict Resolution:**

```python
# Store conflicting versions
def update_user(user_id, name, region):
    # Check for concurrent update
    current = db.users.find_one({'_id': user_id})
    
    if current and current.get('conflict'):
        # Conflict already exists, add another version
        db.users.update_one(
            {'_id': user_id},
            {'$push': {'versions': {'name': name, 'region': region, 'timestamp': time.time()}}}
        )
    elif current and current['region'] != region:
        # Concurrent update from different region
        db.users.update_one(
            {'_id': user_id},
            {
                '$set': {
                    'conflict': True,
                    'versions': [
                        {'name': current['name'], 'region': current['region'], 'timestamp': current['timestamp']},
                        {'name': name, 'region': region, 'timestamp': time.time()}
                    ]
                }
            }
        )
    else:
        # No conflict
        db.users.update_one(
            {'_id': user_id},
            {'$set': {'name': name, 'region': region, 'timestamp': time.time()}}
        )

# Resolve conflict (user chooses)
@app.route('/users/<user_id>/resolve-conflict', methods=['POST'])
def resolve_conflict(user_id):
    chosen_version = request.json['version_index']
    
    user = db.users.find_one({'_id': user_id})
    selected = user['versions'][chosen_version]
    
    db.users.update_one(
        {'_id': user_id},
        {
            '$set': {
                'name': selected['name'],
                'region': selected['region'],
                'timestamp': selected['timestamp'],
                'conflict': False
            },
            '$unset': {'versions': ''}
        }
    )
    
    return jsonify({'message': 'Conflict resolved'})
```

### 4.4 Data Locality

**Optimize performance for regional users**

```
Data locality: Store data close to users

Strategies:

1. Geographic partitioning:
   â”œâ”€ US users â†’ US region
   â”œâ”€ EU users â†’ EU region
   â””â”€ Cross-region access slow (high latency)

2. Replicate all data (active-active):
   â”œâ”€ All data in all regions
   â”œâ”€ Fast reads (local)
   â””â”€ Slow writes (replicate to all regions)

3. Home region + caching:
   â”œâ”€ Primary data in home region
   â”œâ”€ Cache in other regions (read replicas)
   â””â”€ Eventual consistency (cache lag)

4. Data classification:
   â”œâ”€ Hot data: Replicate to all regions (frequent access)
   â”œâ”€ Warm data: Replicate to nearest regions
   â””â”€ Cold data: Store in one region (archive)

Example:
â”œâ”€ User profile: Replicate to all regions (frequent access)
â”œâ”€ User posts: Store in home region + CDN (less frequent)
â””â”€ Analytics data: Store in one region (infrequent access)
```

**Data Locality Implementation:**

```python
# User home region (geographic partitioning)
def create_user(email, name, country):
    # Determine home region based on country
    home_region = get_home_region(country)
    
    # Store user in home region database
    if home_region == 'us-east-1':
        db = get_db_connection('us-east-1')
    elif home_region == 'eu-west-1':
        db = get_db_connection('eu-west-1')
    else:
        db = get_db_connection('ap-southeast-1')
    
    user_id = db.users.insert_one({
        'email': email,
        'name': name,
        'country': country,
        'home_region': home_region
    }).inserted_id
    
    return user_id

# Cross-region read (higher latency)
def get_user(user_id):
    # Try local region first (cache)
    user = local_cache.get(f'user:{user_id}')
    
    if user:
        return user
    
    # Fetch from user's home region
    user = db.users.find_one({'_id': user_id})
    home_region = user['home_region']
    
    if home_region != current_region():
        # Cross-region fetch (slow)
        remote_db = get_db_connection(home_region)
        user = remote_db.users.find_one({'_id': user_id})
    
    # Cache locally (for future reads)
    local_cache.set(f'user:{user_id}', user, ttl=300)
    
    return user

# Write to home region + replicate
def update_user(user_id, updates):
    user = db.users.find_one({'_id': user_id})
    home_region = user['home_region']
    
    # Write to home region
    home_db = get_db_connection(home_region)
    home_db.users.update_one({'_id': user_id}, {'$set': updates})
    
    # Async replication to other regions
    replicate_to_regions(user_id, updates, exclude=[home_region])
```

**CDN for Data Locality:**

```python
# Store media files close to users (CloudFront)
import boto3

s3 = boto3.client('s3')
cloudfront = boto3.client('cloudfront')

# Upload image to S3 (one region)
s3.put_object(
    Bucket='myapp-media',
    Key='users/123/profile.jpg',
    Body=image_data,
    ContentType='image/jpeg'
)

# CloudFront distributes to edge locations worldwide
# Users download from nearest edge (low latency)
image_url = 'https://d123456.cloudfront.net/users/123/profile.jpg'

# Edge caching:
# - User in US â†’ Downloads from US edge (fast)
# - User in EU â†’ Downloads from EU edge (fast)
# - User in Asia â†’ Downloads from Asia edge (fast)
```

---

## Best Practices Summary

```
RPO/RTO:
âœ“ Define based on business requirements (not technical preferences)
âœ“ Critical systems: RPO=0, RTO=minutes (active-active)
âœ“ Important systems: RPO=15min, RTO=1hour (hot standby)
âœ“ Non-critical: RPO=24hours, RTO=4hours (warm/cold standby)
âœ— Don't over-engineer (99.999% availability expensive)
âœ— Don't under-engineer (data loss = customer loss)

Backups:
âœ“ 3-2-1 rule (3 copies, 2 media types, 1 offsite)
âœ“ Automated backups (daily full, hourly incremental)
âœ“ Encrypted backups (protect sensitive data)
âœ“ Test restores regularly (monthly drills)
âœ“ Monitor backup jobs (alert on failures)
âœ“ PITR for critical data (transaction logs)
âœ— Don't rely on single backup (corruption risk)
âœ— Don't skip restore testing (untested backup = no backup)
âœ— Don't store backups in same region (disaster affects both)

Disaster Recovery:
âœ“ Document runbooks (step-by-step recovery)
âœ“ Automate failover (reduce RTO)
âœ“ Practice drills (quarterly DR tests)
âœ“ Monitor health checks (detect failures early)
âœ“ Gradual failover (canary to DR region)
âœ— Don't wait for disaster to test (too late)
âœ— Don't assume backups work (test regularly)
âœ— Don't forget DNS TTL (low TTL for fast failover)

Multi-Region:
âœ“ Active-passive for simplicity (one writer)
âœ“ Active-active for performance (users â†’ nearest region)
âœ“ Conflict resolution strategy (LWW, CRDTs, application-level)
âœ“ Data locality (replicate hot data, partition cold data)
âœ“ Monitor replication lag (alert if >threshold)
âœ— Don't use active-active without conflict resolution
âœ— Don't replicate all data (expensive, unnecessary)
âœ— Don't ignore network latency (cross-region slow)

High Availability:
âœ“ Redundancy at every layer (servers, databases, networks)
âœ“ Health checks + auto-restart (self-healing)
âœ“ Graceful degradation (partial functionality > no functionality)
âœ“ Circuit breakers (prevent cascading failures)
âœ“ Load balancing (distribute traffic)
âœ— Don't have single points of failure (SPOF)
âœ— Don't ignore monitoring (blind to failures)
âœ— Don't deploy all at once (gradual rollout)
```

Complete disaster recovery and multi-region guide! ğŸŒğŸ’¾ğŸ”„