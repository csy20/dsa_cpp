# Platform Topics

## What Are Platform Topics?

**Common system components and capabilities**

```
Platform topics: Building blocks for modern applications

Categories:
1. Real-time communication (WebSockets, SSE)
2. Notifications (push, email, SMS)
3. Scheduling (cron jobs, task queues)
4. ML serving (inference, feature stores)
5. Experimentation (A/B testing, feature flags)

These are infrastructure concerns that:
â”œâ”€ Every application needs
â”œâ”€ Complex to build correctly
â”œâ”€ Better to use existing solutions
â””â”€ Enable product features
```

---

## 1. Real-Time Communication

### 1.1 WebSockets

**Bidirectional, persistent connection**

```
WebSocket: Full-duplex communication over single TCP connection

HTTP (request-response):
Client â†’ Server: GET /data
Client â† Server: 200 OK {data}
Connection closed

WebSocket (persistent):
Client â†’ Server: Upgrade to WebSocket
Client â†” Server: Messages (both directions)
Connection stays open (minutes, hours, days)

Use cases:
â”œâ”€ Chat applications (instant messaging)
â”œâ”€ Live notifications (new message, friend request)
â”œâ”€ Real-time dashboards (stock prices, monitoring)
â”œâ”€ Multiplayer games (player positions, actions)
â””â”€ Collaborative editing (Google Docs)

Benefits:
+ Low latency (no connection overhead)
+ Server push (server initiates messages)
+ Bidirectional (client and server send)

Drawbacks:
- Stateful (server tracks connections)
- Scaling complex (sticky sessions needed)
- Load balancing tricky (long-lived connections)
```

**WebSocket Server (Python):**

```python
from flask import Flask
from flask_socketio import SocketIO, emit, join_room, leave_room

app = Flask(__name__)
socketio = SocketIO(app, cors_allowed_origins="*")

# Track connected users
connected_users = {}

@socketio.on('connect')
def handle_connect():
    """Client connects"""
    print(f"Client connected: {request.sid}")
    emit('connected', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    """Client disconnects"""
    print(f"Client disconnected: {request.sid}")
    
    # Clean up user
    user_id = connected_users.pop(request.sid, None)
    if user_id:
        # Notify others user went offline
        emit('user_offline', {'user_id': user_id}, broadcast=True)

@socketio.on('join_chat')
def handle_join_chat(data):
    """User joins chat room"""
    room = data['room']
    user_id = data['user_id']
    
    join_room(room)
    connected_users[request.sid] = user_id
    
    # Notify room members
    emit('user_joined', {
        'user_id': user_id,
        'room': room
    }, room=room, skip_sid=request.sid)

@socketio.on('send_message')
def handle_message(data):
    """User sends message"""
    room = data['room']
    message = data['message']
    user_id = connected_users.get(request.sid)
    
    # Save message to database
    db.messages.insert_one({
        'room': room,
        'user_id': user_id,
        'message': message,
        'timestamp': datetime.utcnow()
    })
    
    # Broadcast to room (including sender)
    emit('new_message', {
        'user_id': user_id,
        'message': message,
        'timestamp': datetime.utcnow().isoformat()
    }, room=room)

@socketio.on('typing')
def handle_typing(data):
    """User is typing"""
    room = data['room']
    user_id = connected_users.get(request.sid)
    
    # Notify others (not sender)
    emit('user_typing', {
        'user_id': user_id
    }, room=room, skip_sid=request.sid)

if __name__ == '__main__':
    socketio.run(app, host='0.0.0.0', port=5000)
```

**WebSocket Client (JavaScript):**

```javascript
// Connect to WebSocket server
const socket = io('http://localhost:5000');

// Listen for connection
socket.on('connected', (data) => {
  console.log(data.message);
  
  // Join chat room
  socket.emit('join_chat', {
    room: 'general',
    user_id: 'user123'
  });
});

// Listen for new messages
socket.on('new_message', (data) => {
  console.log(`${data.user_id}: ${data.message}`);
  
  // Display message in UI
  appendMessage(data.user_id, data.message);
});

// Listen for user joined
socket.on('user_joined', (data) => {
  console.log(`${data.user_id} joined ${data.room}`);
  showNotification(`${data.user_id} joined the chat`);
});

// Listen for typing indicator
socket.on('user_typing', (data) => {
  showTypingIndicator(data.user_id);
});

// Send message
function sendMessage(message) {
  socket.emit('send_message', {
    room: 'general',
    message: message
  });
}

// Typing indicator
let typingTimeout;
document.getElementById('message-input').addEventListener('input', () => {
  clearTimeout(typingTimeout);
  
  socket.emit('typing', { room: 'general' });
  
  // Stop typing indicator after 3 seconds
  typingTimeout = setTimeout(() => {
    socket.emit('stop_typing', { room: 'general' });
  }, 3000);
});
```

**WebSocket Scaling:**

```python
# Scale WebSockets with Redis (pub/sub)
from flask_socketio import SocketIO
import redis

app = Flask(__name__)

# Use Redis for message queue (multi-server support)
socketio = SocketIO(app, message_queue='redis://localhost:6379')

# Now multiple WebSocket servers can share state:
# Server 1: User A connects
# Server 2: User B connects
# User A sends message â†’ Redis pub/sub â†’ Server 2 â†’ User B receives

# Load balancer with sticky sessions (IP hash):
# All connections from same IP go to same server
# nginx.conf
upstream websocket_servers {
    ip_hash;  # Sticky sessions
    server ws1.example.com:5000;
    server ws2.example.com:5000;
    server ws3.example.com:5000;
}

server {
    location / {
        proxy_pass http://websocket_servers;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
}
```

### 1.2 Server-Sent Events (SSE)

**Server pushes updates to client (unidirectional)**

```
SSE: Server â†’ Client (one-way)

Comparison:
â”œâ”€ HTTP: Client polls server (inefficient)
â”œâ”€ WebSocket: Client â†” Server (bidirectional, complex)
â””â”€ SSE: Server â†’ Client (simpler than WebSocket)

Use cases:
â”œâ”€ Live updates (news feed, stock prices)
â”œâ”€ Notifications (new email, like, comment)
â”œâ”€ Progress tracking (file upload, job status)
â””â”€ Monitoring dashboards (metrics, logs)

Benefits:
+ Simple (built on HTTP)
+ Auto-reconnect (browser handles)
+ Event types (named events)
+ Lightweight (less overhead than WebSocket)

Drawbacks:
- One-way (server â†’ client only)
- Text only (no binary data)
- Limited browser connections (6 per domain)
```

**SSE Server (Python):**

```python
from flask import Flask, Response
import time
import json

app = Flask(__name__)

@app.route('/events')
def events():
    """SSE endpoint"""
    def generate():
        # Send initial event
        yield f"data: {json.dumps({'message': 'Connected'})}\n\n"
        
        # Send updates every 5 seconds
        count = 0
        while True:
            count += 1
            
            # Fetch latest data
            data = {
                'count': count,
                'timestamp': datetime.utcnow().isoformat(),
                'value': random.randint(1, 100)
            }
            
            # Send event (data: prefix required)
            yield f"data: {json.dumps(data)}\n\n"
            
            time.sleep(5)
    
    return Response(generate(), mimetype='text/event-stream')

@app.route('/notifications/<user_id>')
def user_notifications(user_id):
    """User-specific notification stream"""
    def generate():
        # Subscribe to user notifications (Redis pub/sub)
        pubsub = redis_client.pubsub()
        pubsub.subscribe(f'notifications:{user_id}')
        
        for message in pubsub.listen():
            if message['type'] == 'message':
                # Send notification to client
                notification = json.loads(message['data'])
                yield f"event: notification\n"
                yield f"data: {json.dumps(notification)}\n\n"
    
    return Response(generate(), mimetype='text/event-stream')

# Publish notification (from another service)
def send_notification(user_id, notification):
    """Send notification to user via SSE"""
    redis_client.publish(
        f'notifications:{user_id}',
        json.dumps(notification)
    )

# Example: Send notification when user gets new message
@app.route('/messages', methods=['POST'])
def create_message():
    data = request.json
    
    # Save message
    db.messages.insert_one({
        'from': data['from_user'],
        'to': data['to_user'],
        'message': data['message']
    })
    
    # Send real-time notification via SSE
    send_notification(data['to_user'], {
        'type': 'new_message',
        'from': data['from_user'],
        'message': data['message']
    })
    
    return jsonify({'status': 'sent'})
```

**SSE Client (JavaScript):**

```javascript
// Connect to SSE endpoint
const eventSource = new EventSource('http://localhost:5000/events');

// Listen for messages (default event)
eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);
  console.log('Update:', data);
  
  // Update UI
  document.getElementById('value').textContent = data.value;
};

// Listen for named events
eventSource.addEventListener('notification', (event) => {
  const notification = JSON.parse(event.data);
  console.log('Notification:', notification);
  
  // Show notification
  showNotification(notification.type, notification.message);
});

// Handle errors (auto-reconnects)
eventSource.onerror = (error) => {
  console.error('SSE error:', error);
  // Browser automatically reconnects
};

// Close connection
function disconnect() {
  eventSource.close();
}

// Progress tracking example
const uploadProgress = new EventSource('/upload/123/progress');

uploadProgress.onmessage = (event) => {
  const progress = JSON.parse(event.data);
  
  // Update progress bar
  document.getElementById('progress').style.width = `${progress.percentage}%`;
  document.getElementById('status').textContent = progress.status;
  
  // Close when complete
  if (progress.percentage === 100) {
    uploadProgress.close();
  }
};
```

---

## 2. Notifications

### 2.1 Push Notifications (Mobile/Web)

**Send notifications to user devices**

```
Push notification flow:

1. User grants permission (browser/app)
2. Device registers with push service (FCM, APNs)
3. App stores device token in database
4. Server sends notification to push service
5. Push service delivers to device
6. Device displays notification

Push services:
â”œâ”€ Firebase Cloud Messaging (FCM): Android, iOS, Web
â”œâ”€ Apple Push Notification (APNs): iOS, macOS
â””â”€ Web Push API: Chrome, Firefox, Safari

Use cases:
â”œâ”€ New message, comment, like
â”œâ”€ Order status updates
â”œâ”€ Breaking news, alerts
â””â”€ Reminders, scheduled events
```

**Push Notifications with Firebase:**

```python
import firebase_admin
from firebase_admin import credentials, messaging

# Initialize Firebase
cred = credentials.Certificate('firebase-credentials.json')
firebase_admin.initialize_app(cred)

def send_push_notification(device_token, title, body, data=None):
    """Send push notification to device"""
    
    message = messaging.Message(
        notification=messaging.Notification(
            title=title,
            body=body
        ),
        data=data or {},  # Custom data
        token=device_token
    )
    
    # Send notification
    response = messaging.send(message)
    print(f"Sent notification: {response}")

# Send to multiple devices
def send_to_multiple_devices(device_tokens, title, body):
    """Send to multiple devices (batch)"""
    
    message = messaging.MulticastMessage(
        notification=messaging.Notification(
            title=title,
            body=body
        ),
        tokens=device_tokens  # List of tokens (max 500)
    )
    
    response = messaging.send_multicast(message)
    print(f"Sent to {response.success_count}/{len(device_tokens)} devices")

# Topic-based notifications (subscribe/unsubscribe)
def subscribe_to_topic(device_tokens, topic):
    """Subscribe devices to topic"""
    messaging.subscribe_to_topic(device_tokens, topic)

def send_to_topic(topic, title, body):
    """Send notification to all devices subscribed to topic"""
    
    message = messaging.Message(
        notification=messaging.Notification(
            title=title,
            body=body
        ),
        topic=topic
    )
    
    messaging.send(message)

# Example: Send notification when user gets new message
@app.route('/messages', methods=['POST'])
def create_message():
    data = request.json
    
    # Save message
    message_id = db.messages.insert_one({
        'from': data['from_user'],
        'to': data['to_user'],
        'message': data['message']
    }).inserted_id
    
    # Get recipient's device tokens
    user = db.users.find_one({'_id': data['to_user']})
    device_tokens = user.get('device_tokens', [])
    
    if device_tokens:
        # Send push notification
        send_to_multiple_devices(
            device_tokens,
            title=f"New message from {data['from_user']}",
            body=data['message']
        )
    
    return jsonify({'id': str(message_id)})
```

### 2.2 Email Notifications

**Send transactional and marketing emails**

```
Email types:

1. Transactional (high priority):
   â”œâ”€ Account verification, password reset
   â”œâ”€ Order confirmation, shipping updates
   â”œâ”€ Receipts, invoices
   â””â”€ Must deliver (no unsubscribe)

2. Marketing (bulk):
   â”œâ”€ Newsletters, promotions
   â”œâ”€ Product announcements
   â””â”€ Can unsubscribe (required by law)

Email services:
â”œâ”€ SendGrid (transactional + marketing)
â”œâ”€ AWS SES (low cost, high volume)
â”œâ”€ Mailgun (developer-friendly)
â””â”€ Postmark (transactional focus)

Best practices:
â”œâ”€ Use templates (consistent branding)
â”œâ”€ Personalization (user name, preferences)
â”œâ”€ Unsubscribe link (marketing emails)
â”œâ”€ Track deliverability (open rate, bounce rate)
â””â”€ Respect opt-outs (don't spam)
```

**Email with SendGrid:**

```python
from sendgrid import SendGridAPIClient
from sendgrid.helpers.mail import Mail, Email, To, Content

sg = SendGridAPIClient(api_key=os.environ.get('SENDGRID_API_KEY'))

def send_email(to_email, subject, body_html):
    """Send email"""
    
    message = Mail(
        from_email=Email('noreply@example.com', 'Example App'),
        to_emails=To(to_email),
        subject=subject,
        html_content=Content("text/html", body_html)
    )
    
    response = sg.send(message)
    print(f"Email sent: {response.status_code}")

# Template-based email
def send_template_email(to_email, template_id, template_data):
    """Send email using template"""
    
    message = Mail(
        from_email='noreply@example.com',
        to_emails=to_email
    )
    
    message.template_id = template_id
    message.dynamic_template_data = template_data
    
    sg.send(message)

# Example: Welcome email
def send_welcome_email(user):
    send_template_email(
        to_email=user['email'],
        template_id='d-123456789',
        template_data={
            'name': user['name'],
            'verification_link': f'https://example.com/verify/{user["token"]}'
        }
    )

# Example: Order confirmation
def send_order_confirmation(user, order):
    send_template_email(
        to_email=user['email'],
        template_id='d-987654321',
        template_data={
            'name': user['name'],
            'order_id': order['id'],
            'items': order['items'],
            'total': order['total'],
            'tracking_url': f'https://example.com/orders/{order["id"]}/track'
        }
    )
```

### 2.3 SMS Notifications

**Send text messages**

```python
from twilio.rest import Client

twilio_client = Client(
    account_sid='your_account_sid',
    auth_token='your_auth_token'
)

def send_sms(phone_number, message):
    """Send SMS"""
    
    message = twilio_client.messages.create(
        body=message,
        from_='+1234567890',  # Your Twilio phone number
        to=phone_number
    )
    
    print(f"SMS sent: {message.sid}")

# Example: 2FA code
def send_verification_code(user):
    code = generate_verification_code()
    
    # Save code to database
    db.verification_codes.insert_one({
        'user_id': user['_id'],
        'code': code,
        'expires_at': datetime.utcnow() + timedelta(minutes=5)
    })
    
    # Send SMS
    send_sms(
        user['phone'],
        f"Your verification code is: {code}. Valid for 5 minutes."
    )
```

---

## 3. Scheduling & Background Jobs

### 3.1 Cron Jobs

**Schedule periodic tasks**

```
Cron: Time-based job scheduler

Cron syntax: minute hour day month weekday command
0 0 * * * /backup.sh           # Daily at midnight
0 */6 * * * /cleanup.sh         # Every 6 hours
0 9 * * 1 /report.sh            # Monday at 9 AM
*/5 * * * * /check.sh           # Every 5 minutes

Examples:
â”œâ”€ 0 2 * * * â†’ Daily at 2 AM (backups)
â”œâ”€ 0 */1 * * * â†’ Every hour (cleanup)
â”œâ”€ 0 0 1 * * â†’ 1st of month (billing)
â””â”€ */15 * * * * â†’ Every 15 minutes (health check)

Use cases:
â”œâ”€ Database backups
â”œâ”€ Data cleanup (delete old records)
â”œâ”€ Report generation
â”œâ”€ Cache warming
â””â”€ Metrics aggregation
```

**Cron with APScheduler (Python):**

```python
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger

scheduler = BackgroundScheduler()

# Daily backup at 2 AM
@scheduler.scheduled_job(CronTrigger(hour=2, minute=0))
def daily_backup():
    """Run daily backup"""
    print("Running backup...")
    
    # Backup database
    subprocess.run(['pg_dump', '-h', 'localhost', '-U', 'admin', '-d', 'production', '-f', f'/backups/backup_{datetime.now().strftime("%Y%m%d")}.sql'])
    
    print("Backup complete")

# Cleanup old data every hour
@scheduler.scheduled_job(CronTrigger(hour='*', minute=0))
def cleanup_old_data():
    """Delete old records"""
    cutoff_date = datetime.utcnow() - timedelta(days=90)
    
    result = db.logs.delete_many({'timestamp': {'$lt': cutoff_date}})
    print(f"Deleted {result.deleted_count} old logs")

# Generate daily report at 9 AM
@scheduler.scheduled_job(CronTrigger(hour=9, minute=0))
def generate_daily_report():
    """Generate and email daily report"""
    
    # Calculate metrics
    metrics = {
        'users': db.users.count_documents({}),
        'orders': db.orders.count_documents({'date': datetime.utcnow().date()}),
        'revenue': db.orders.aggregate([
            {'$match': {'date': datetime.utcnow().date()}},
            {'$group': {'_id': None, 'total': {'$sum': '$total'}}}
        ]).next()['total']
    }
    
    # Send email
    send_email(
        'admin@example.com',
        'Daily Report',
        f"Users: {metrics['users']}<br>Orders: {metrics['orders']}<br>Revenue: ${metrics['revenue']}"
    )

# Start scheduler
scheduler.start()
```

**Kubernetes CronJob:**

```yaml
# cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-backup
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: postgres:15
            command:
            - /bin/sh
            - -c
            - |
              pg_dump -h $DB_HOST -U $DB_USER -d $DB_NAME -f /backups/backup_$(date +%Y%m%d).sql
              aws s3 cp /backups/backup_$(date +%Y%m%d).sql s3://backups/
            env:
            - name: DB_HOST
              value: "postgres.default.svc.cluster.local"
            - name: DB_USER
              valueFrom:
                secretKeyRef:
                  name: db-credentials
                  key: username
            - name: DB_NAME
              value: "production"
          restartPolicy: OnFailure
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
```

### 3.2 Task Queues (Celery)

**Asynchronous background processing**

```
Task queue: Decouple long-running tasks from web requests

Problem:
â”œâ”€ User uploads video (5 minutes to process)
â”œâ”€ Web request times out after 30 seconds
â””â”€ User sees error, even though processing continues

Solution:
â”œâ”€ Web request: Accept upload, queue task, return immediately
â”œâ”€ Background worker: Process video asynchronously
â””â”€ User: Receives notification when complete

Task queue components:
â”œâ”€ Producer: Web server (queues tasks)
â”œâ”€ Broker: Message queue (Redis, RabbitMQ)
â”œâ”€ Worker: Background process (executes tasks)
â””â”€ Result backend: Store task results (Redis, database)

Use cases:
â”œâ”€ Video/image processing
â”œâ”€ Email sending (bulk)
â”œâ”€ Report generation
â”œâ”€ Data imports/exports
â””â”€ Long-running computations
```

**Celery Setup:**

```python
# celery_app.py
from celery import Celery

# Initialize Celery
celery_app = Celery(
    'myapp',
    broker='redis://localhost:6379/0',      # Message queue
    backend='redis://localhost:6379/1'      # Result storage
)

# Configure
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
)

# tasks.py
from celery_app import celery_app
import time

@celery_app.task
def process_video(video_id):
    """Process video (long-running task)"""
    
    # Fetch video from database
    video = db.videos.find_one({'_id': video_id})
    
    # Download from S3
    s3.download_file('videos', video['s3_key'], f'/tmp/{video_id}.mp4')
    
    # Process video (transcoding, thumbnail generation)
    subprocess.run([
        'ffmpeg',
        '-i', f'/tmp/{video_id}.mp4',
        '-vf', 'scale=1280:720',
        f'/tmp/{video_id}_720p.mp4'
    ])
    
    # Upload processed video
    s3.upload_file(f'/tmp/{video_id}_720p.mp4', 'videos', f'{video_id}_720p.mp4')
    
    # Update database
    db.videos.update_one(
        {'_id': video_id},
        {'$set': {'status': 'processed', 'processed_url': f'{video_id}_720p.mp4'}}
    )
    
    # Send notification
    send_notification(video['user_id'], {
        'type': 'video_processed',
        'video_id': video_id
    })
    
    return {'status': 'success', 'video_id': video_id}

@celery_app.task
def send_bulk_emails(user_ids, subject, body):
    """Send emails to multiple users"""
    
    for user_id in user_ids:
        user = db.users.find_one({'_id': user_id})
        send_email(user['email'], subject, body)
        time.sleep(0.1)  # Rate limiting
    
    return {'sent': len(user_ids)}

# app.py (Flask)
from tasks import process_video, send_bulk_emails

@app.route('/videos', methods=['POST'])
def upload_video():
    file = request.files['video']
    
    # Save to S3
    s3_key = f'{uuid.uuid4()}.mp4'
    s3.upload_fileobj(file, 'videos', s3_key)
    
    # Save to database
    video_id = db.videos.insert_one({
        'user_id': get_current_user_id(),
        's3_key': s3_key,
        'status': 'processing'
    }).inserted_id
    
    # Queue processing task (asynchronous)
    task = process_video.delay(video_id)
    
    # Return immediately (don't wait for processing)
    return jsonify({
        'video_id': str(video_id),
        'task_id': task.id,
        'status': 'processing'
    }), 202  # Accepted

@app.route('/tasks/<task_id>')
def task_status(task_id):
    """Check task status"""
    task = celery_app.AsyncResult(task_id)
    
    return jsonify({
        'task_id': task_id,
        'state': task.state,  # PENDING, STARTED, SUCCESS, FAILURE
        'result': task.result if task.state == 'SUCCESS' else None
    })

# Run Celery worker
# celery -A celery_app worker --loglevel=info
```

---

## 4. Machine Learning Serving

### 4.1 Model Serving

**Serve ML models in production**

```
ML serving: Make predictions in real-time

Workflow:
1. Train model offline (hours, days)
2. Save model (pickle, SavedModel, ONNX)
3. Load model in server (startup)
4. Serve predictions (API endpoint)

Serving patterns:

1. Model in application:
   â”œâ”€ Load model in web server
   â”œâ”€ Predict in request handler
   â””â”€ Simple, but couples model to app

2. Model as microservice:
   â”œâ”€ Separate service for predictions
   â”œâ”€ REST/gRPC API
   â””â”€ Decoupled, scalable

3. Batch prediction:
   â”œâ”€ Precompute predictions (offline)
   â”œâ”€ Store in database
   â””â”€ Serve from cache (fast, no compute)

Performance considerations:
â”œâ”€ Latency: p99 < 100ms (real-time)
â”œâ”€ Throughput: 1000 req/s (high traffic)
â”œâ”€ Model size: < 100 MB (fast loading)
â””â”€ GPU: Use for large models (transformers, CNNs)
```

**Model Serving with Flask:**

```python
from flask import Flask, request, jsonify
import pickle
import numpy as np

app = Flask(__name__)

# Load model at startup (once)
with open('model.pkl', 'rb') as f:
    model = pickle.load(f)

@app.route('/predict', methods=['POST'])
def predict():
    """Serve predictions"""
    
    # Extract features from request
    data = request.json
    features = np.array([[
        data['age'],
        data['income'],
        data['credit_score']
    ]])
    
    # Make prediction
    prediction = model.predict(features)[0]
    probability = model.predict_proba(features)[0][1]
    
    return jsonify({
        'prediction': int(prediction),  # 0 or 1
        'probability': float(probability),  # 0.0 to 1.0
        'approved': bool(prediction == 1)
    })

@app.route('/predict/batch', methods=['POST'])
def predict_batch():
    """Batch predictions (more efficient)"""
    
    data = request.json
    features = np.array([[
        row['age'],
        row['income'],
        row['credit_score']
    ] for row in data])
    
    predictions = model.predict(features)
    probabilities = model.predict_proba(features)[:, 1]
    
    return jsonify([
        {
            'prediction': int(pred),
            'probability': float(prob)
        }
        for pred, prob in zip(predictions, probabilities)
    ])

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

**Model Serving with TensorFlow Serving:**

```bash
# Save model (TensorFlow)
import tensorflow as tf

model = tf.keras.models.Sequential([...])
model.compile(...)
model.fit(...)

# Save in SavedModel format
model.save('/models/my_model/1')  # Version 1

# Serve with TensorFlow Serving (Docker)
docker run -p 8501:8501 \
  --mount type=bind,source=/models/my_model,target=/models/my_model \
  -e MODEL_NAME=my_model \
  tensorflow/serving

# Make prediction (REST API)
curl -X POST http://localhost:8501/v1/models/my_model:predict \
  -H 'Content-Type: application/json' \
  -d '{
    "instances": [
      {"age": 35, "income": 50000, "credit_score": 720}
    ]
  }'

# Response
{
  "predictions": [[0.92]]  # 92% probability
}
```

### 4.2 Feature Stores

**Centralized feature management**

```
Feature store: Repository for ML features

Problems without feature store:
â”œâ”€ Feature duplication (training vs serving code)
â”œâ”€ Training/serving skew (different features)
â”œâ”€ Hard to share features (between teams)
â””â”€ Difficult to track lineage (where did feature come from?)

Feature store benefits:
â”œâ”€ Centralized features (single source of truth)
â”œâ”€ Feature reuse (share across models)
â”œâ”€ Consistency (same features in training/serving)
â”œâ”€ Feature versioning (track changes)
â””â”€ Online + offline (serving + training)

Feature types:
â”œâ”€ Online features: Real-time serving (low latency)
â””â”€ Offline features: Training (batch)

Popular feature stores:
â”œâ”€ Feast (open source)
â”œâ”€ Tecton (managed)
â””â”€ AWS SageMaker Feature Store
```

**Feature Store with Feast:**

```python
# feature_repo/features.py
from feast import Entity, Feature, FeatureView, ValueType
from feast.data_source import FileSource

# Define entity (user)
user = Entity(
    name="user_id",
    value_type=ValueType.INT64,
    description="User ID"
)

# Define data source (offline)
user_features_source = FileSource(
    path="data/user_features.parquet",
    event_timestamp_column="event_timestamp"
)

# Define feature view
user_features = FeatureView(
    name="user_features",
    entities=["user_id"],
    features=[
        Feature(name="age", dtype=ValueType.INT64),
        Feature(name="income", dtype=ValueType.FLOAT),
        Feature(name="credit_score", dtype=ValueType.INT64),
    ],
    online=True,  # Enable online serving
    source=user_features_source,
    ttl=timedelta(days=1)
)

# Initialize Feast
feast_client = feast.Client()

# Get features for serving (online)
def get_user_features(user_id):
    """Get features for real-time prediction"""
    
    features = feast_client.get_online_features(
        features=[
            "user_features:age",
            "user_features:income",
            "user_features:credit_score"
        ],
        entity_rows=[{"user_id": user_id}]
    ).to_dict()
    
    return {
        'age': features['age'][0],
        'income': features['income'][0],
        'credit_score': features['credit_score'][0]
    }

# Use in prediction
@app.route('/predict/<user_id>', methods=['POST'])
def predict(user_id):
    # Get features from feature store
    features = get_user_features(user_id)
    
    # Make prediction
    prediction = model.predict([[
        features['age'],
        features['income'],
        features['credit_score']
    ]])[0]
    
    return jsonify({'prediction': int(prediction)})

# Get features for training (offline)
def get_training_dataset():
    """Get features for model training"""
    
    feature_refs = [
        "user_features:age",
        "user_features:income",
        "user_features:credit_score"
    ]
    
    # Get historical features
    training_df = feast_client.get_historical_features(
        feature_refs=feature_refs,
        entity_df=pd.read_csv('data/training_entities.csv')
    ).to_df()
    
    return training_df
```

---

## 5. A/B Testing & Experimentation

### 5.1 A/B Testing

**Compare variants to measure impact**

```
A/B test: Randomly assign users to variants

Example: New checkout flow
â”œâ”€ Control (A): Old checkout (50% users)
â”œâ”€ Treatment (B): New checkout (50% users)
â””â”€ Metric: Conversion rate (% who complete purchase)

Results:
â”œâ”€ Control: 10% conversion (100 conversions / 1000 users)
â”œâ”€ Treatment: 12% conversion (120 conversions / 1000 users)
â””â”€ Improvement: +20% relative (2% absolute)

Statistical significance:
â”œâ”€ p-value < 0.05: Result is significant (not random)
â””â”€ Confidence: 95% confident Treatment is better

A/B test process:
1. Hypothesis: New checkout increases conversions
2. Design: 50/50 split, 2 weeks, 10,000 users
3. Implement: Feature flag, random assignment
4. Measure: Track conversions
5. Analyze: Statistical test (t-test, chi-square)
6. Decide: Ship Treatment or revert to Control
```

**A/B Testing Implementation:**

```python
import hashlib
import random

# Deterministic assignment (same user always gets same variant)
def assign_variant(user_id, experiment_name):
    """Assign user to variant (A or B)"""
    
    # Hash user_id + experiment_name
    hash_input = f"{user_id}:{experiment_name}"
    hash_value = int(hashlib.md5(hash_input.encode()).hexdigest(), 16)
    
    # Modulo to get 0-99
    bucket = hash_value % 100
    
    # 50/50 split
    if bucket < 50:
        return 'control'
    else:
        return 'treatment'

# Track experiment events
def track_event(user_id, experiment_name, event_name, properties=None):
    """Track experiment event"""
    
    variant = assign_variant(user_id, experiment_name)
    
    db.experiment_events.insert_one({
        'experiment': experiment_name,
        'user_id': user_id,
        'variant': variant,
        'event': event_name,
        'properties': properties or {},
        'timestamp': datetime.utcnow()
    })

# Use in application
@app.route('/checkout')
def checkout():
    user_id = get_current_user_id()
    
    # Assign to variant
    variant = assign_variant(user_id, 'new_checkout_flow')
    
    # Track exposure
    track_event(user_id, 'new_checkout_flow', 'viewed_checkout')
    
    # Show appropriate UI
    if variant == 'control':
        return render_template('checkout_old.html')
    else:
        return render_template('checkout_new.html')

@app.route('/checkout/complete', methods=['POST'])
def complete_checkout():
    user_id = get_current_user_id()
    
    # Track conversion
    track_event(user_id, 'new_checkout_flow', 'completed_checkout', {
        'order_value': request.json['total']
    })
    
    return jsonify({'status': 'success'})

# Analyze results
def analyze_experiment(experiment_name):
    """Analyze A/B test results"""
    
    # Get conversion counts
    pipeline = [
        {'$match': {'experiment': experiment_name}},
        {'$group': {
            '_id': {
                'variant': '$variant',
                'event': '$event'
            },
            'count': {'$sum': 1}
        }}
    ]
    
    results = list(db.experiment_events.aggregate(pipeline))
    
    # Calculate conversion rates
    control_views = next((r['count'] for r in results if r['_id']['variant'] == 'control' and r['_id']['event'] == 'viewed_checkout'), 0)
    control_conversions = next((r['count'] for r in results if r['_id']['variant'] == 'control' and r['_id']['event'] == 'completed_checkout'), 0)
    
    treatment_views = next((r['count'] for r in results if r['_id']['variant'] == 'treatment' and r['_id']['event'] == 'viewed_checkout'), 0)
    treatment_conversions = next((r['count'] for r in results if r['_id']['variant'] == 'treatment' and r['_id']['event'] == 'completed_checkout'), 0)
    
    control_rate = control_conversions / control_views if control_views > 0 else 0
    treatment_rate = treatment_conversions / treatment_views if treatment_views > 0 else 0
    
    # Statistical test (chi-square)
    from scipy.stats import chi2_contingency
    
    observed = [
        [control_conversions, control_views - control_conversions],
        [treatment_conversions, treatment_views - treatment_conversions]
    ]
    
    chi2, p_value, dof, expected = chi2_contingency(observed)
    
    print(f"Experiment: {experiment_name}")
    print(f"Control: {control_conversions}/{control_views} ({control_rate:.2%})")
    print(f"Treatment: {treatment_conversions}/{treatment_views} ({treatment_rate:.2%})")
    print(f"Lift: {(treatment_rate - control_rate) / control_rate:.2%}")
    print(f"p-value: {p_value:.4f}")
    
    if p_value < 0.05:
        print("âœ“ Statistically significant (p < 0.05)")
        if treatment_rate > control_rate:
            print("â†’ Ship Treatment")
        else:
            print("â†’ Keep Control")
    else:
        print("âœ— Not statistically significant")
        print("â†’ Need more data or no difference")
```

### 5.2 Multivariate Testing

**Test multiple variables simultaneously**

```python
# Multivariate test: Test multiple elements
def assign_multivariate(user_id, experiment_name):
    """Assign user to variant combination"""
    
    hash_value = int(hashlib.md5(f"{user_id}:{experiment_name}".encode()).hexdigest(), 16)
    bucket = hash_value % 100
    
    # Test: Headline (A/B) Ã— Button color (Red/Green/Blue)
    # 6 variants total
    
    if bucket < 17:
        return {'headline': 'A', 'button': 'red'}
    elif bucket < 34:
        return {'headline': 'A', 'button': 'green'}
    elif bucket < 51:
        return {'headline': 'A', 'button': 'blue'}
    elif bucket < 68:
        return {'headline': 'B', 'button': 'red'}
    elif bucket < 85:
        return {'headline': 'B', 'button': 'green'}
    else:
        return {'headline': 'B', 'button': 'blue'}

@app.route('/landing')
def landing_page():
    user_id = get_current_user_id()
    
    variant = assign_multivariate(user_id, 'landing_page_test')
    
    return render_template('landing.html',
        headline=variant['headline'],
        button_color=variant['button']
    )
```

---

## Best Practices Summary

```
Real-Time Communication:
âœ“ WebSockets for bidirectional (chat, multiplayer)
âœ“ SSE for server push (notifications, updates)
âœ“ Use Redis pub/sub for scaling WebSockets
âœ“ Sticky sessions for WebSocket load balancing
âœ“ Graceful reconnection (handle disconnects)
âœ— Don't poll (inefficient, use push instead)
âœ— Don't use WebSockets for simple updates (SSE simpler)

Notifications:
âœ“ Push notifications for mobile/web (timely, engaging)
âœ“ Email for transactional (receipts, confirmations)
âœ“ SMS for critical (2FA, urgent alerts)
âœ“ Use templates (consistent branding)
âœ“ Track deliverability (open rate, bounce rate)
âœ— Don't spam (respect unsubscribe)
âœ— Don't send marketing without consent (illegal)

Scheduling:
âœ“ Cron for periodic tasks (backups, cleanup)
âœ“ Task queues for async work (video processing)
âœ“ Retry failed tasks (idempotent operations)
âœ“ Monitor job health (alert on failures)
âœ— Don't run long tasks in web request (timeout)
âœ— Don't forget error handling (jobs fail silently)

ML Serving:
âœ“ Separate model serving (microservice)
âœ“ Feature store for consistency (training/serving)
âœ“ Batch predictions when possible (cheaper)
âœ“ Cache predictions (avoid recompute)
âœ“ Monitor model performance (accuracy drift)
âœ— Don't load model per request (slow)
âœ— Don't use large models in real-time (latency)

A/B Testing:
âœ“ Statistical significance (p < 0.05)
âœ“ Sufficient sample size (power analysis)
âœ“ Deterministic assignment (same user, same variant)
âœ“ Track both exposure and conversion
âœ“ Analyze by segment (new vs returning users)
âœ— Don't stop early (false positives)
âœ— Don't run too many tests (dilutes traffic)
âœ— Don't ignore negative results (learn from failures)
```

Complete platform topics guide! ðŸ“¡ðŸ“¬âš™ï¸ðŸ¤–ðŸ“Š