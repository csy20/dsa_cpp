# From Requirements to API Design

## 1. Use-Cases & Traffic Shape

### Defining Use-Cases

**Use-case** = A specific way users interact with the system

#### Example: Instagram-like System

**Primary Use-Cases:**
```
1. Upload photo
   - Actor: User
   - Trigger: User clicks upload
   - Flow: Select photo ‚Üí Add caption ‚Üí Add filters ‚Üí Post
   - Frequency: 2-3 times/day per active user

2. View feed
   - Actor: User
   - Trigger: Open app
   - Flow: Load latest photos from following users
   - Frequency: 50 times/day per active user

3. Like/comment
   - Actor: User
   - Trigger: User interacts with post
   - Flow: Click like/comment ‚Üí Update counts
   - Frequency: 20 times/day per active user

4. Search users/hashtags
   - Actor: User
   - Trigger: User types in search box
   - Flow: Type query ‚Üí Get results
   - Frequency: 5 times/day per active user

5. Follow/unfollow
   - Actor: User
   - Trigger: User clicks follow button
   - Flow: Update relationship graph
   - Frequency: 2-3 times/day per active user
```

**Secondary Use-Cases:**
- Edit profile
- Direct messages
- Stories
- Explore page

### Traffic Shape Analysis

#### Temporal Patterns

**Daily Pattern:**
```
Traffic throughout day (normalized):

12am-6am:  ‚ñÅ‚ñÅ‚ñÅ‚ñÅ (20% - night)
6am-9am:   ‚ñÉ‚ñÉ‚ñÖ‚ñÖ (60% - morning commute)
9am-12pm:  ‚ñÖ‚ñÖ‚ñÖ‚ñÖ (70% - work hours)
12pm-2pm:  ‚ñá‚ñá‚ñá‚ñá (90% - lunch peak)
2pm-6pm:   ‚ñÖ‚ñÖ‚ñÖ‚ñÖ (70% - afternoon)
6pm-9pm:   ‚ñà‚ñà‚ñà‚ñà (100% - evening PEAK)
9pm-12am:  ‚ñÜ‚ñÜ‚ñÖ‚ñÖ (80% - night browsing)

Peak/Average ratio: ~3-5√ó
```

**Weekly Pattern:**
```
Mon-Thu: 100% (normal)
Fri:     120% (TGIF effect)
Sat-Sun: 150% (weekend spike)
```

**Event-Driven Spikes:**
```
Normal:     ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ (baseline)
Product launch: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (10√ó spike)
Black Friday:   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (50√ó spike)
Super Bowl:     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (100√ó spike)

Design for 3√ó normal, plan for 10√ó spikes
```

#### Geographic Distribution

```
If global app:
‚îú‚îÄ Americas: 40% traffic (8am-11pm EST)
‚îú‚îÄ Europe:   30% traffic (8am-11pm CET)
‚îú‚îÄ Asia:     25% traffic (8am-11pm IST/CST)
‚îî‚îÄ Other:     5%

Peak hours overlap ‚Üí 2√ó load during 9pm-12am UTC
```

### Traffic Calculations

#### Example: Twitter-like System

**Given:**
- 200M Daily Active Users (DAU)
- 500M Monthly Active Users (MAU)
- DAU/MAU = 0.4 (40% daily engagement)

**Use-Case Breakdown:**
```
Per user per day:
‚îú‚îÄ Post tweet:        2 times
‚îú‚îÄ Read timeline:    50 times
‚îú‚îÄ Like/retweet:     10 times
‚îú‚îÄ Search:            3 times
‚îî‚îÄ View profile:      5 times

Total operations per day:
‚îú‚îÄ Writes: 200M √ó 2 = 400M tweets/day
‚îÇ         = 4,630 tweets/sec
‚îÇ         Peak (3√ó): 13,890 tweets/sec
‚îÇ
‚îú‚îÄ Reads: 200M √ó 50 = 10B timeline loads/day
‚îÇ        = 115,740 reads/sec
‚îÇ        Peak (3√ó): 347,220 reads/sec
‚îÇ
‚îî‚îÄ Interactions: 200M √ó 10 = 2B likes/day
           = 23,148 likes/sec
           Peak: 69,444 likes/sec

Read:Write Ratio = 10B:400M = 25:1 (read-heavy)
```

---

## 2. Read/Write Mix

### Understanding Read/Write Patterns

#### Read-Heavy Systems (90-99% reads)

**Examples:**
- Social media feeds (Twitter, Instagram)
- News sites
- Video streaming (YouTube, Netflix)
- Wikipedia

**Characteristics:**
```
Read:Write ratio = 100:1 or higher

Operations:
‚îú‚îÄ Writes: 1K/sec
‚îî‚îÄ Reads: 100K/sec
```

**Optimization Strategy:**
- Aggressive caching (Redis, Memcached)
- CDN for static content
- Read replicas (5-10 replicas)
- Eventual consistency OK
- Denormalized data (optimize for reads)

**Architecture Pattern:**
```
Write:
User ‚Üí Load Balancer ‚Üí App Server ‚Üí Primary DB
                                   ‚Üí Cache invalidation

Read:
User ‚Üí CDN ‚Üí Load Balancer ‚Üí App Server ‚Üí Cache (99% hit)
                                        ‚Üì (1% miss)
                                      Read Replica
```

#### Write-Heavy Systems (30-50% writes)

**Examples:**
- IoT sensor data
- Trading systems
- Logging/analytics
- Chat/messaging

**Characteristics:**
```
Read:Write ratio = 1:1 to 2:1

Operations:
‚îú‚îÄ Writes: 50K/sec
‚îî‚îÄ Reads: 50K/sec
```

**Optimization Strategy:**
- Write-optimized databases (Cassandra, time-series DB)
- Async writes (message queue)
- Batching writes
- Sharding by write key
- Append-only logs

**Architecture Pattern:**
```
Write:
User ‚Üí Load Balancer ‚Üí App Server ‚Üí Message Queue
                                           ‚Üì
                                      Queue Consumer
                                           ‚Üì
                                     Sharded DBs

Read:
User ‚Üí Load Balancer ‚Üí App Server ‚Üí Aggregation Service
                                           ‚Üì
                                      Multiple Shards
```

#### Balanced Systems (50-50)

**Examples:**
- E-commerce (browse vs purchase)
- Banking
- Collaborative editing

**Strategy:**
- Separate read/write paths (CQRS)
- Different databases for read/write
- Hybrid caching strategy

### Read/Write Mix Impact on Design

```
System Type    | Caching | DB Choice      | Consistency
---------------|---------|----------------|-------------
Read-heavy     | Heavy   | Read replicas  | Eventual
Write-heavy    | Light   | Write-optimized| Eventual
Balanced       | Medium  | Both           | Strong
Financial      | None    | ACID DB        | Strong
```

---

## 3. Latency & Throughput Budgets

### Understanding the Difference

```
Latency:    Time for ONE request (milliseconds)
Throughput: Number of requests per time unit (req/sec)

Example:
‚îú‚îÄ Latency: 100ms per request
‚îî‚îÄ Throughput: 10,000 req/sec

Can have high throughput with high latency (batching)
Can have low latency with low throughput (real-time)
```

### Latency Budgets

#### Latency Expectations by Use-Case

```
Use-Case               | P50    | P95    | P99    | Max
-----------------------|--------|--------|--------|--------
Key-value lookup       | 1ms    | 5ms    | 10ms   | 50ms
Database query         | 10ms   | 50ms   | 100ms  | 500ms
API call               | 50ms   | 100ms  | 200ms  | 1s
Page load              | 200ms  | 500ms  | 1s     | 3s
Search query           | 100ms  | 300ms  | 500ms  | 2s
Video start            | 500ms  | 1s     | 2s     | 5s
File upload            | 1s     | 5s     | 10s    | 30s
Batch job              | 1min   | 5min   | 10min  | 1hr
```

**User Perception:**
```
< 100ms:   Instant (feels like direct manipulation)
100-300ms: Slight delay (acceptable)
300-1000ms: Noticeable (need progress indicator)
> 1s:      Users get distracted
> 10s:     Users leave
```

#### Latency Budget Example

**Goal:** Page load in 1 second (P95)

**Budget Breakdown:**
```
Total budget: 1000ms

DNS lookup:        20ms  (2%)
TCP connection:    30ms  (3%)
TLS handshake:     50ms  (5%)
Request time:      10ms  (1%)
Server processing: 200ms (20%)  ‚Üê Your backend
   ‚îú‚îÄ Auth:         20ms
   ‚îú‚îÄ Cache check:  10ms
   ‚îú‚îÄ DB query:     50ms
   ‚îú‚îÄ Business logic: 30ms
   ‚îú‚îÄ Rendering:    70ms
   ‚îî‚îÄ Buffer:       20ms
Response transfer: 100ms (10%)
Browser parsing:   200ms (20%)
Asset loading:     300ms (30%)
Rendering:         100ms (10%)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL:            1010ms ‚ùå Over budget!

Optimization needed:
- Reduce server processing: 200ms ‚Üí 150ms
- Optimize assets: 300ms ‚Üí 250ms
‚úì New total: 960ms
```

### Throughput Budgets

#### Throughput Calculations

**Formula:**
```
Throughput = (Number of servers √ó Capacity per server) / Load

Example:
- Each server: 1000 req/sec
- Target load: 100K req/sec
- Servers needed: 100K / 1000 = 100 servers
- With redundancy (30%): 130 servers
```

**Capacity Planning:**
```
Expected load:        50K req/sec
Peak load (3√ó):      150K req/sec
Buffer (30%):         45K req/sec
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Design capacity:     195K req/sec

Servers needed:
- Each handles 2K req/sec
- Total: 195K / 2K = 98 servers
- Round up: 100 servers

Cost:
- $100/server/month
- 100 √ó $100 = $10K/month
```

#### Throughput by Component

**Example System:**
```
Component           | Throughput    | Bottleneck?
--------------------|---------------|-------------
Load Balancer       | 500K req/sec  | ‚úì
App Servers (√ó50)   | 100K req/sec  | ‚úì (50 √ó 2K)
Cache (Redis)       | 1M req/sec    | ‚úì
Database (Primary)  | 10K write/sec | ‚úó BOTTLENECK
DB Replicas (√ó10)   | 100K read/sec | ‚úì
Message Queue       | 50K msg/sec   | ‚úì

Bottleneck: Database writes at 10K/sec
Solution: Shard database (10 shards ‚Üí 100K write/sec)
```

### Latency vs Throughput Trade-offs

```
Scenario 1: Real-time Chat
‚îú‚îÄ Priority: LOW latency (< 100ms)
‚îú‚îÄ Throughput: Medium (10K msg/sec)
‚îî‚îÄ Design: Direct connections, minimal processing

Scenario 2: Analytics Pipeline
‚îú‚îÄ Priority: HIGH throughput (1M events/sec)
‚îú‚îÄ Latency: Can be seconds (batch processing)
‚îî‚îÄ Design: Batching, streaming, eventual consistency

Scenario 3: Payment Processing
‚îú‚îÄ Priority: BOTH low latency AND consistency
‚îú‚îÄ Throughput: Lower is OK (10K/sec)
‚îî‚îÄ Design: Strong consistency, optimized critical path
```

---

## 4. API Design

### REST vs gRPC vs GraphQL

#### REST (REpresentational State Transfer)

**Characteristics:**
- HTTP verbs: GET, POST, PUT, DELETE
- Resource-based URLs
- JSON payload (typically)
- Stateless

**Pros:**
- Simple, widely understood
- HTTP caching works
- Easy to debug (browser, curl)
- Language-agnostic

**Cons:**
- Over-fetching (get more data than needed)
- Under-fetching (multiple requests needed)
- No schema enforcement

**Use-Cases:**
- Public APIs
- CRUD operations
- Web/mobile apps
- Third-party integrations

**Example:**
```http
# Get user
GET /api/v1/users/123
Response:
{
  "id": 123,
  "name": "Alice",
  "email": "alice@example.com",
  "created_at": "2024-01-01T00:00:00Z"
}

# Create post
POST /api/v1/posts
Content-Type: application/json
{
  "user_id": 123,
  "title": "Hello World",
  "content": "This is my first post"
}
Response: 201 Created
{
  "id": 456,
  "user_id": 123,
  "title": "Hello World",
  "created_at": "2024-01-15T10:30:00Z"
}

# Update user
PUT /api/v1/users/123
{
  "name": "Alice Smith"
}

# Delete post
DELETE /api/v1/posts/456
Response: 204 No Content
```

#### gRPC (Google Remote Procedure Call)

**Characteristics:**
- Protocol Buffers (binary format)
- HTTP/2 (multiplexing, streaming)
- Strongly typed
- Code generation from .proto files

**Pros:**
- 7√ó faster than REST (binary vs JSON)
- Smaller payload (30-50% less)
- Bi-directional streaming
- Built-in load balancing, retries
- Type safety

**Cons:**
- Not browser-friendly (needs gRPC-web)
- Harder to debug (binary format)
- Less human-readable
- Steeper learning curve

**Use-Cases:**
- Microservices communication
- High-performance systems
- Real-time bidirectional streaming
- Internal APIs

**Example:**
```protobuf
// user.proto
syntax = "proto3";

service UserService {
  rpc GetUser(GetUserRequest) returns (User);
  rpc CreatePost(CreatePostRequest) returns (Post);
  rpc StreamPosts(StreamRequest) returns (stream Post);
}

message User {
  int64 id = 1;
  string name = 2;
  string email = 3;
  int64 created_at = 4;
}

message GetUserRequest {
  int64 user_id = 1;
}

message Post {
  int64 id = 1;
  int64 user_id = 2;
  string title = 3;
  string content = 4;
}
```

```cpp
// Client code (auto-generated)
UserServiceClient client(channel);
GetUserRequest request;
request.set_user_id(123);

User user;
Status status = client.GetUser(&context, request, &user);
if (status.ok()) {
  std::cout << "Name: " << user.name() << std::endl;
}
```

#### GraphQL

**Characteristics:**
- Single endpoint
- Client specifies exact data needed
- Query language for APIs
- Schema-based

**Pros:**
- No over-fetching (get only what you need)
- No under-fetching (get related data in one request)
- Strongly typed schema
- Self-documenting
- Real-time with subscriptions

**Cons:**
- Complex caching
- N+1 query problem (needs DataLoader)
- Harder rate limiting
- Learning curve

**Use-Cases:**
- Frontend-driven apps
- Mobile apps (save bandwidth)
- Complex data relationships
- Rapidly changing requirements

**Example:**
```graphql
# Schema definition
type User {
  id: ID!
  name: String!
  email: String!
  posts: [Post!]!
  followers: [User!]!
}

type Post {
  id: ID!
  title: String!
  content: String!
  author: User!
  comments: [Comment!]!
}

type Query {
  user(id: ID!): User
  post(id: ID!): Post
}

type Mutation {
  createPost(userId: ID!, title: String!, content: String!): Post!
  deletePost(id: ID!): Boolean!
}

type Subscription {
  postCreated(userId: ID!): Post!
}
```

```graphql
# Client query - get exactly what's needed
query {
  user(id: "123") {
    name
    email
    posts(limit: 5) {
      title
      comments(limit: 3) {
        text
        author {
          name
        }
      }
    }
  }
}

# Response - exactly matches query structure
{
  "data": {
    "user": {
      "name": "Alice",
      "email": "alice@example.com",
      "posts": [
        {
          "title": "Hello World",
          "comments": [
            {
              "text": "Great post!",
              "author": { "name": "Bob" }
            }
          ]
        }
      ]
    }
  }
}
```

### Comparison Table

| Feature | REST | gRPC | GraphQL |
|---------|------|------|---------|
| **Protocol** | HTTP/1.1 | HTTP/2 | HTTP/1.1+ |
| **Format** | JSON | Protobuf (binary) | JSON |
| **Schema** | Optional (OpenAPI) | Required (.proto) | Required |
| **Streaming** | No (SSE separate) | Yes (bidirectional) | Yes (subscriptions) |
| **Performance** | Baseline | 7√ó faster | Similar to REST |
| **Browser** | ‚úì Native | ‚úó Needs proxy | ‚úì Native |
| **Caching** | ‚úì HTTP caching | ‚úó Custom | ‚úó Complex |
| **Learning Curve** | Easy | Medium | Medium |
| **Best For** | Public APIs | Microservices | Frontend-heavy |

### API Versioning

#### Strategy 1: URL Versioning (Most Common)

```http
/api/v1/users/123
/api/v2/users/123

Pros:
- Clear, visible
- Easy to route
- Can run multiple versions simultaneously

Cons:
- URL changes (breaks bookmarks)
- Code duplication
```

#### Strategy 2: Header Versioning

```http
GET /api/users/123
Accept: application/vnd.myapp.v1+json

Pros:
- Clean URLs
- RESTful

Cons:
- Hidden (less discoverable)
- Harder to test in browser
```

#### Strategy 3: Query Parameter

```http
/api/users/123?version=2

Pros:
- Easy to implement
- Simple testing

Cons:
- Pollutes query string
- Not standard
```

#### Strategy 4: Content Negotiation

```http
GET /api/users/123
Accept: application/vnd.myapp.v2+json

Pros:
- True REST
- Flexible

Cons:
- Complex
- Harder to debug
```

**Best Practice:**
```
Use URL versioning for major versions:
‚îú‚îÄ /api/v1/ ‚Üí Breaking changes (old API)
‚îú‚îÄ /api/v2/ ‚Üí Complete redesign
‚îî‚îÄ /api/v3/ ‚Üí Future major changes

Within version, maintain backward compatibility:
- Add new fields (OK)
- Don't remove fields (breaking)
- Don't change types (breaking)
- Deprecate gracefully (6-12 months notice)

Version lifecycle:
v1: Released ‚Üí Supported ‚Üí Deprecated ‚Üí Sunset
                 ‚Üì          (6 months)   (12 months)
v2: Released ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Pagination

#### Offset-Based Pagination (Traditional)

```http
GET /api/posts?limit=20&offset=40

Response:
{
  "data": [...],
  "pagination": {
    "limit": 20,
    "offset": 40,
    "total": 1000
  }
}

Pros:
- Simple to implement
- Can jump to any page
- Shows total count

Cons:
- Slow for large offsets (OFFSET 1000000)
- Inconsistent if data changes (duplicate/missing items)
- Expensive COUNT(*) query
```

**SQL:**
```sql
SELECT * FROM posts 
ORDER BY created_at DESC 
LIMIT 20 OFFSET 40;

-- Problem: Slow for large offsets
-- DB must scan and skip first 1M rows
```

#### Cursor-Based Pagination (Recommended)

```http
GET /api/posts?limit=20&cursor=eyJpZCI6MTAwfQ

Response:
{
  "data": [...],
  "pagination": {
    "next_cursor": "eyJpZCI6MTIwfQ",
    "has_more": true
  }
}

Pros:
- Fast for any position
- Consistent results (no duplicates)
- Scales to millions of records

Cons:
- Can't jump to specific page
- No total count
- More complex implementation
```

**Cursor encoding:**
```javascript
// Cursor = base64({ id: 100, created_at: "2024-01-15" })
const cursor = Buffer.from(
  JSON.stringify({ id: 100, created_at: "2024-01-15" })
).toString('base64');
// "eyJpZCI6MTAwLCJjcmVhdGVkX2F0IjoiMjAyNC0wMS0xNSJ9"
```

**SQL:**
```sql
-- Decode cursor: id=100, created_at='2024-01-15'
SELECT * FROM posts 
WHERE (created_at, id) < ('2024-01-15', 100)
ORDER BY created_at DESC, id DESC 
LIMIT 20;

-- Fast! Uses composite index on (created_at, id)
```

#### Keyset Pagination (Alternative)

```http
GET /api/posts?limit=20&since_id=100

Response:
{
  "data": [...],
  "pagination": {
    "since_id": 120,
    "until_id": 100
  }
}

Used by: Twitter, Facebook
```

#### Infinite Scroll Implementation

```javascript
// Frontend (React example)
function Posts() {
  const [posts, setPosts] = useState([]);
  const [cursor, setCursor] = useState(null);
  const [loading, setLoading] = useState(false);

  const loadMore = async () => {
    setLoading(true);
    const url = cursor 
      ? `/api/posts?limit=20&cursor=${cursor}`
      : `/api/posts?limit=20`;
    
    const res = await fetch(url);
    const data = await res.json();
    
    setPosts([...posts, ...data.data]);
    setCursor(data.pagination.next_cursor);
    setLoading(false);
  };

  useEffect(() => {
    loadMore();
  }, []);

  return (
    <InfiniteScroll
      dataLength={posts.length}
      next={loadMore}
      hasMore={cursor !== null}
      loader={<Spinner />}
    >
      {posts.map(post => <Post key={post.id} {...post} />)}
    </InfiniteScroll>
  );
}
```

### Idempotency Keys

**Problem:** Network failures cause duplicate requests

```
Client sends: POST /api/payments { amount: 100 }
Network times out (uncertain if received)
Client retries: POST /api/payments { amount: 100 }
Result: User charged TWICE! ‚ùå
```

**Solution:** Idempotency keys

```http
POST /api/payments
Idempotency-Key: a1b2c3d4-e5f6-7890-abcd-ef1234567890
Content-Type: application/json

{
  "amount": 100,
  "currency": "USD",
  "user_id": 123
}

Server:
1. Check if Idempotency-Key exists in cache/DB
2. If exists: Return cached response (no duplicate charge)
3. If new: Process request, cache response with key
4. Return response
```

**Implementation:**

```python
# Backend (Python/Flask example)
import uuid
from functools import wraps

idempotency_cache = {}  # In production: use Redis

def idempotent(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        key = request.headers.get('Idempotency-Key')
        
        if not key:
            return {'error': 'Idempotency-Key required'}, 400
        
        # Check cache
        if key in idempotency_cache:
            return idempotency_cache[key], 200
        
        # Process request
        result = f(*args, **kwargs)
        
        # Cache result (expire after 24 hours)
        idempotency_cache[key] = result
        redis.setex(f"idempotency:{key}", 86400, json.dumps(result))
        
        return result, 201
    
    return decorated_function

@app.route('/api/payments', methods=['POST'])
@idempotent
def create_payment():
    data = request.json
    payment = charge_user(data['user_id'], data['amount'])
    return {'payment_id': payment.id, 'status': 'success'}
```

**Best Practices:**
```
1. Client generates UUID v4 for each unique operation
2. Same key = same operation (retry)
3. Different key = different operation (new charge)
4. Server stores key + response for 24 hours
5. After 24h, key expires (operation should be done)

Key structure:
{user_id}_{operation}_{timestamp}_{random}
123_payment_20240115_a1b2c3d4

Used by: Stripe, Shopify, AWS
```

---

## 5. Core Entities & Schemas

### Entity Identification

**Process:**
1. Extract nouns from requirements ‚Üí Entities
2. Extract verbs ‚Üí Relationships
3. Define attributes

#### Example: Twitter-like System

**Requirements:**
- Users can post tweets
- Users can follow other users
- Users can like tweets
- Tweets can have media attachments
- Users have profiles

**Entities:**
```
1. User
2. Tweet
3. Follow (relationship)
4. Like (relationship)
5. Media
```

### Schema Design

#### User Entity

```sql
CREATE TABLE users (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    display_name VARCHAR(100),
    bio TEXT,
    profile_image_url VARCHAR(500),
    banner_image_url VARCHAR(500),
    verified BOOLEAN DEFAULT FALSE,
    follower_count INT DEFAULT 0,
    following_count INT DEFAULT 0,
    tweet_count INT DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    INDEX idx_username (username),
    INDEX idx_email (email),
    INDEX idx_created_at (created_at)
);
```

#### Tweet Entity

```sql
CREATE TABLE tweets (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id BIGINT NOT NULL,
    content TEXT NOT NULL,
    reply_to_tweet_id BIGINT,  -- NULL if not a reply
    retweet_of_tweet_id BIGINT,  -- NULL if original tweet
    like_count INT DEFAULT 0,
    retweet_count INT DEFAULT 0,
    reply_count INT DEFAULT 0,
    view_count BIGINT DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE,
    FOREIGN KEY (reply_to_tweet_id) REFERENCES tweets(id) ON DELETE CASCADE,
    FOREIGN KEY (retweet_of_tweet_id) REFERENCES tweets(id) ON DELETE CASCADE,
    
    INDEX idx_user_id_created_at (user_id, created_at DESC),
    INDEX idx_created_at (created_at DESC),
    INDEX idx_reply_to (reply_to_tweet_id),
    
    -- Full-text search
    FULLTEXT INDEX idx_content (content)
);
```

#### Follow Relationship

```sql
CREATE TABLE follows (
    follower_id BIGINT NOT NULL,
    followee_id BIGINT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    PRIMARY KEY (follower_id, followee_id),
    FOREIGN KEY (follower_id) REFERENCES users(id) ON DELETE CASCADE,
    FOREIGN KEY (followee_id) REFERENCES users(id) ON DELETE CASCADE,
    
    INDEX idx_follower (follower_id),
    INDEX idx_followee (followee_id),
    INDEX idx_created_at (created_at)
);

-- Check constraint: User can't follow themselves
ALTER TABLE follows ADD CONSTRAINT check_no_self_follow 
    CHECK (follower_id != followee_id);
```

#### Like Relationship

```sql
CREATE TABLE likes (
    user_id BIGINT NOT NULL,
    tweet_id BIGINT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    PRIMARY KEY (user_id, tweet_id),
    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE,
    FOREIGN KEY (tweet_id) REFERENCES tweets(id) ON DELETE CASCADE,
    
    INDEX idx_user_id_created_at (user_id, created_at DESC),
    INDEX idx_tweet_id_created_at (tweet_id, created_at DESC)
);
```

#### Media Entity

```sql
CREATE TABLE media (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    tweet_id BIGINT NOT NULL,
    media_type ENUM('image', 'video', 'gif') NOT NULL,
    url VARCHAR(500) NOT NULL,
    thumbnail_url VARCHAR(500),
    width INT,
    height INT,
    size_bytes BIGINT,
    duration_sec INT,  -- For videos
    alt_text TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (tweet_id) REFERENCES tweets(id) ON DELETE CASCADE,
    INDEX idx_tweet_id (tweet_id)
);
```

### NoSQL Schema Design

#### MongoDB Schema (Document-based)

```javascript
// User document
{
  _id: ObjectId("507f1f77bcf86cd799439011"),
  username: "alice",
  email: "alice@example.com",
  password_hash: "$2b$12$...",
  profile: {
    display_name: "Alice Smith",
    bio: "Software engineer",
    profile_image: "https://...",
    verified: false
  },
  stats: {
    follower_count: 1234,
    following_count: 567,
    tweet_count: 890
  },
  created_at: ISODate("2024-01-01T00:00:00Z"),
  updated_at: ISODate("2024-01-15T10:30:00Z")
}

// Tweet document (embedded user info for fast reads)
{
  _id: ObjectId("507f1f77bcf86cd799439012"),
  user_id: ObjectId("507f1f77bcf86cd799439011"),
  user: {  // Denormalized for fast display
    username: "alice",
    display_name: "Alice Smith",
    profile_image: "https://..."
  },
  content: "Hello World!",
  media: [  // Embedded array
    {
      type: "image",
      url: "https://...",
      width: 1200,
      height: 800
    }
  ],
  stats: {
    like_count: 42,
    retweet_count: 10,
    reply_count: 5,
    view_count: 1000
  },
  reply_to: null,  // ObjectId if reply
  created_at: ISODate("2024-01-15T10:30:00Z")
}

// Indexes
db.users.createIndex({ username: 1 }, { unique: true })
db.users.createIndex({ email: 1 }, { unique: true })
db.tweets.createIndex({ user_id: 1, created_at: -1 })
db.tweets.createIndex({ created_at: -1 })
db.tweets.createIndex({ content: "text" })  // Full-text search
```

---

## 6. Access Patterns

### Identifying Access Patterns

**Questions to Ask:**
1. What queries will be most frequent?
2. What data is read together?
3. What are the hot paths?
4. What are the critical operations?

#### Twitter Example Access Patterns

```
1. Load user timeline (HOT PATH - 80% of reads)
   - Query: Get latest tweets from users I follow
   - Frequency: 50 times/day per user
   - Optimization: Pre-compute timelines, cache heavily

2. Post tweet (write path)
   - Action: Insert tweet, update stats, fanout to followers
   - Frequency: 2 times/day per user
   - Optimization: Async fanout, message queue

3. View user profile
   - Query: Get user info + recent tweets
   - Frequency: 5 times/day per user
   - Optimization: Cache user profiles

4. Search tweets
   - Query: Full-text search with filters
   - Frequency: 3 times/day per user
   - Optimization: Elasticsearch, separate search cluster

5. Get notifications
   - Query: Recent mentions, likes, follows
   - Frequency: 10 times/day per user
   - Optimization: Push notifications, separate service
```

### Optimizing for Access Patterns

#### Pattern 1: Timeline Query (Read-Heavy)

**Naive Approach (Slow):**
```sql
-- Get tweets from users I follow
SELECT t.* FROM tweets t
JOIN follows f ON t.user_id = f.followee_id
WHERE f.follower_id = 123
ORDER BY t.created_at DESC
LIMIT 20;

-- Problem: JOIN on every request, slow for 1000+ following
```

**Optimized Approach (Fast):**
```
1. Fan-out on write:
   When user posts tweet:
   ‚îú‚îÄ Insert tweet to tweets table
   ‚îî‚îÄ Async: Copy tweet_id to each follower's timeline cache
   
2. Read from cache:
   Timeline request:
   ‚îú‚îÄ Read from Redis list: timeline:user:123
   ‚îî‚îÄ Hydrate tweet details (batch query)

Cache structure:
timeline:user:123 = [tweet_id:999, tweet_id:998, tweet_id:997, ...]
(Redis LIST, max 1000 items)

Read query:
1. LRANGE timeline:user:123 0 19  (get 20 tweet IDs)
2. Batch query: SELECT * FROM tweets WHERE id IN (999, 998, ...)
3. Return hydrated tweets

Write query (async):
1. Insert tweet
2. Get follower IDs: SELECT follower_id FROM follows WHERE followee_id = 123
3. For each follower: LPUSH timeline:user:{follower_id} {tweet_id}
4. Trim: LTRIM timeline:user:{follower_id} 0 999  (keep max 1000)
```

**Trade-off:**
```
Pros:
- Fast reads (no JOIN)
- Scales to millions of followers per user

Cons:
- Slow writes (fanout to all followers)
- Storage overhead (duplicate tweet_ids)

Solution for celebrities (millions of followers):
- Hybrid approach
- Regular users: Fan-out on write
- Celebrities: Fan-in on read (query when needed)
```

#### Pattern 2: Counters (Like, Retweet counts)

**Naive Approach:**
```sql
-- Count likes every time
SELECT COUNT(*) FROM likes WHERE tweet_id = 456;

-- Problem: COUNT(*) is expensive, runs on every tweet view
```

**Optimized Approach:**
```sql
-- Denormalized counter in tweets table
UPDATE tweets 
SET like_count = like_count + 1 
WHERE id = 456;

-- Read is instant
SELECT like_count FROM tweets WHERE id = 456;
```

**Eventual Consistency:**
```
User clicks like:
1. Optimistic UI update (instant feedback)
2. Async write to likes table
3. Increment counter (can be batched)

Periodic reconciliation (every hour):
UPDATE tweets t
SET like_count = (SELECT COUNT(*) FROM likes WHERE tweet_id = t.id)
WHERE t.updated_at < NOW() - INTERVAL 1 HOUR;
```

#### Pattern 3: Pagination with Cursor

**Schema includes cursor fields:**
```sql
-- Composite index for cursor pagination
CREATE INDEX idx_user_created_id ON tweets(user_id, created_at DESC, id DESC);

-- Query
SELECT * FROM tweets 
WHERE user_id = 123 
  AND (created_at, id) < ('2024-01-15 10:30:00', 999)
ORDER BY created_at DESC, id DESC 
LIMIT 20;

-- Fast! Uses index, no OFFSET needed
```

### Database Sharding for Access Patterns

```
Sharding strategy based on access patterns:

User-based sharding:
‚îú‚îÄ Shard key: user_id % 64
‚îú‚îÄ Good for: User timeline, profile queries
‚îî‚îÄ Bad for: Global search, trending topics

Tweet-based sharding:
‚îú‚îÄ Shard key: tweet_id % 64
‚îú‚îÄ Good for: Tweet lookups, replies
‚îî‚îÄ Bad for: User timeline (scatter-gather)

Hybrid:
‚îú‚îÄ Users table: Shard by user_id
‚îú‚îÄ Tweets table: Shard by user_id (co-locate with user)
‚îú‚îÄ Likes table: Shard by tweet_id
‚îî‚îÄ Search: Separate cluster (Elasticsearch)
```

---

## Complete Example: Design URL Shortener API

### 1. Use-Cases
```
1. Shorten URL
2. Redirect (main use-case - 99% traffic)
3. Get analytics
4. Delete short URL
```

### 2. Traffic Shape
```
- 100M new URLs/month = 40 URLs/sec
- 10:1 read:write ratio = 400 redirects/sec
- Peak (3√ó): 120 URLs/sec, 1200 redirects/sec
```

### 3. API Design (REST)

```http
# Shorten URL
POST /api/v1/urls
Content-Type: application/json
Idempotency-Key: uuid

{
  "long_url": "https://example.com/very/long/url/...",
  "custom_alias": "my-link",  // Optional
  "expiration": "2024-12-31"  // Optional
}

Response: 201 Created
{
  "short_url": "https://short.ly/a1B2c3",
  "long_url": "https://example.com/...",
  "created_at": "2024-01-15T10:30:00Z",
  "expiration": "2024-12-31T23:59:59Z"
}

# Redirect
GET /{short_code}
Response: 301 Moved Permanently
Location: https://example.com/...

# Get analytics
GET /api/v1/urls/{short_code}/analytics
Response: 200 OK
{
  "short_code": "a1B2c3",
  "total_clicks": 1234,
  "clicks_by_date": [...],
  "clicks_by_country": {...}
}

# Delete
DELETE /api/v1/urls/{short_code}
Response: 204 No Content
```

### 4. Core Entities

```sql
CREATE TABLE urls (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    short_code VARCHAR(10) UNIQUE NOT NULL,
    long_url VARCHAR(2048) NOT NULL,
    user_id BIGINT,
    click_count BIGINT DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expiration TIMESTAMP,
    
    INDEX idx_short_code (short_code),
    INDEX idx_user_id (user_id),
    INDEX idx_expiration (expiration)
);

CREATE TABLE clicks (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    short_code VARCHAR(10) NOT NULL,
    clicked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    ip_address VARCHAR(45),
    user_agent TEXT,
    referer VARCHAR(500),
    country VARCHAR(2),
    
    INDEX idx_short_code_date (short_code, clicked_at)
);
```

### 5. Access Patterns

```
HOT PATH: Redirect (99% traffic)
‚îú‚îÄ Query: SELECT long_url FROM urls WHERE short_code = ?
‚îú‚îÄ Optimization: Cache in Redis (99.9% hit rate)
‚îî‚îÄ TTL: 24 hours

Write path: Shorten URL
‚îú‚îÄ Generate short_code (Base62)
‚îú‚îÄ Insert to DB
‚îî‚îÄ Cache in Redis

Analytics: Batch process
‚îú‚îÄ Click events ‚Üí Kafka
‚îî‚îÄ Aggregate hourly ‚Üí Separate DB
```

**Perfect foundation for designing APIs and schemas!** üöÄ

