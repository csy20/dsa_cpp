# Replication, Sharding, Consistency

## What is Replication?

**Copying data across multiple nodes for availability and performance**

```
Single node:
Client â†’ Database (SPOF - Single Point of Failure)
â”œâ”€ Node fails â†’ Data unavailable âŒ
â””â”€ Limited capacity (vertical scaling only)

Replicated:
Client â†’ Database Cluster
         â”œâ”€ Node 1 (Primary)
         â”œâ”€ Node 2 (Replica)
         â””â”€ Node 3 (Replica)
         
âœ“ High availability (failover if primary fails)
âœ“ Read scalability (distribute reads across replicas)
âœ“ Data durability (multiple copies)
```

## What is Sharding?

**Partitioning data across multiple nodes for horizontal scalability**

```
Single database (10TB data):
Client â†’ Database (too large, slow)

Sharded (4 shards Ã— 2.5TB each):
Client â†’ Router â†’ Shard 1 (users 0-25M)
                â†’ Shard 2 (users 25M-50M)
                â†’ Shard 3 (users 50M-75M)
                â†’ Shard 4 (users 75M-100M)

âœ“ Horizontal scaling (add more shards)
âœ“ Better performance (smaller datasets per shard)
âœ“ Parallel processing (queries across shards)
```

---

## 1. Replication

### 1.1 Leader-Follower (Master-Slave) Replication

**One leader (primary) accepts writes, followers (replicas) replicate data**

```
Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client                                 â”‚
â”‚   â”œâ”€ Writes â†’ Leader                   â”‚
â”‚   â””â”€ Reads â†’ Followers (or Leader)     â”‚
â”‚                                        â”‚
â”‚ Leader (Primary)                       â”‚
â”‚   â”œâ”€ Accept writes                     â”‚
â”‚   â”œâ”€ Apply to local storage            â”‚
â”‚   â””â”€ Send changes to Followers         â”‚
â”‚       â†“                                â”‚
â”‚ Follower 1 â† Replication stream        â”‚
â”‚ Follower 2 â† Replication stream        â”‚
â”‚ Follower 3 â† Replication stream        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### How It Works

**Synchronous Replication:**
```
Timeline:
t1: Client â†’ Leader: INSERT user='Alice'
t2: Leader â†’ Storage: Write 'Alice'
t3: Leader â†’ Follower 1: Replicate 'Alice'
t4: Leader â†’ Follower 2: Replicate 'Alice'
t5: Follower 1 â†’ Leader: ACK (acknowledged)
t6: Follower 2 â†’ Leader: ACK
t7: Leader â†’ Client: Success (after all ACKs)

Characteristics:
+ Strong consistency (all replicas have data before ACK)
+ No data loss (guaranteed durability)
- High latency (wait for all replicas)
- Availability impact (if follower down, writes block)

Use case: Critical data (financial transactions, inventory)
```

**Asynchronous Replication:**
```
Timeline:
t1: Client â†’ Leader: INSERT user='Alice'
t2: Leader â†’ Storage: Write 'Alice'
t3: Leader â†’ Client: Success (immediately!)
t4: Leader â†’ Follower 1: Replicate 'Alice' (background)
t5: Leader â†’ Follower 2: Replicate 'Alice' (background)
t6: Follower 1 â†’ Leader: ACK (later)
t7: Follower 2 â†’ Leader: ACK (later)

Characteristics:
+ Low latency (no wait for replicas)
+ High availability (follower failure doesn't block writes)
- Eventual consistency (replicas lag behind)
- Potential data loss (if leader fails before replication)

Use case: Social media posts, analytics, caching
```

**Semi-Synchronous Replication (Hybrid):**
```
Timeline:
t1: Client â†’ Leader: INSERT user='Alice'
t2: Leader â†’ Storage: Write 'Alice'
t3: Leader â†’ Follower 1: Replicate 'Alice'
t4: Leader â†’ Follower 2: Replicate 'Alice' (async)
t5: Follower 1 â†’ Leader: ACK (wait for at least 1 replica)
t6: Leader â†’ Client: Success
t7: Follower 2 â†’ Leader: ACK (later, async)

Characteristics:
+ Balance latency and durability
+ Moderate consistency (at least 1 replica has data)
+ Good availability (only 1 replica needed)

Use case: Most production databases (MySQL, PostgreSQL)

MySQL configuration:
rpl_semi_sync_master_enabled = 1
rpl_semi_sync_master_wait_for_slave_count = 1
```

#### Replication Methods

**Statement-Based Replication (Logical):**
```
Leader logs SQL statements:
INSERT INTO users VALUES (1, 'Alice', 'alice@example.com');
UPDATE users SET email='alice@newmail.com' WHERE id=1;
DELETE FROM users WHERE id=1;

Followers replay statements:
1. Receive statement
2. Execute same SQL
3. Apply to local storage

Pros:
+ Small log size (just SQL statements)
+ Human-readable logs

Cons:
- Non-deterministic functions (NOW(), RAND(), UUID())
  â†’ Different results on replicas
- Order-dependent (must execute in same order)
- Complex statements (triggers, stored procedures)

MySQL: binlog_format = STATEMENT
```

**Row-Based Replication (Physical):**
```
Leader logs actual data changes:
Row inserted: id=1, name='Alice', email='alice@example.com'
Row updated: id=1, old_email='alice@example.com', new_email='alice@newmail.com'
Row deleted: id=1

Followers apply row changes:
1. Receive row change
2. Apply directly to storage
3. No SQL execution needed

Pros:
+ Deterministic (exact data changes)
+ Handles non-deterministic functions
+ Consistent across replicas

Cons:
- Large log size (full row data)
- Bulk updates expensive (UPDATE 1M rows â†’ 1M log entries)

MySQL: binlog_format = ROW (recommended)
PostgreSQL: Logical replication (row-based)
```

**Write-Ahead Log (WAL) Shipping:**
```
Leader ships WAL (binary log):
1. Leader writes WAL: [Page 123, Offset 456, Data: 'Alice']
2. Ship WAL to followers
3. Followers replay WAL (physical blocks)

Pros:
+ Efficient (low-level disk blocks)
+ Exact replica (byte-for-byte copy)

Cons:
- Version-dependent (WAL format changes across versions)
- Can't have different indexes on replicas

PostgreSQL: Streaming replication (WAL shipping)
```

### 1.2 Read Replicas

**Distribute read traffic across multiple followers**

```
Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Writes â†’ Leader (single point)          â”‚
â”‚           â†“                              â”‚
â”‚      Replication                         â”‚
â”‚           â†“                              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚    â†“             â†“           â†“           â”‚
â”‚ Follower 1   Follower 2   Follower 3    â”‚
â”‚    â†‘             â†‘           â†‘           â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚              Reads (load balanced)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Read capacity: 3Ã— (3 followers)
Write capacity: 1Ã— (single leader)

Use case: Read-heavy workloads (90% reads, 10% writes)
```

**Application Code:**
```python
# Write to leader
leader_conn = connect_to_leader()
leader_conn.execute("INSERT INTO users VALUES (1, 'Alice')")

# Read from replica
replica_conn = connect_to_replica()  # Round-robin load balancing
users = replica_conn.execute("SELECT * FROM users WHERE id=1")

# Problem: Replication lag
# Write at t1 â†’ Leader has data
# Read at t2 (t2 > t1) â†’ Replica doesn't have data yet (lag!)
```

**Handling Replication Lag:**
```python
# Strategy 1: Read from leader for critical reads
def get_user(user_id, critical=False):
    if critical:
        conn = connect_to_leader()  # Read from leader (up-to-date)
    else:
        conn = connect_to_replica()  # Read from replica (may lag)
    return conn.execute("SELECT * FROM users WHERE id=?", user_id)

# Use critical=True after writes
create_user(name='Alice')  # Write to leader
user = get_user(user_id=123, critical=True)  # Read from leader

# Strategy 2: Read-your-writes consistency (session stickiness)
session = Session()
session.write(user='Alice')  # Write to leader, record timestamp
user = session.read(user_id=123)  # Read from replica with timestamp >= write

# Strategy 3: Wait for replication
def write_and_wait(data):
    leader.write(data)
    leader.wait_for_replication(timeout=1.0)  # Wait for replicas to catch up
    return "Success"
```

### 1.3 Replication Lag

**Time delay between leader write and follower apply**

```
Scenario:
t0: Leader writes: user='Alice'
t1: Follower 1 receives (lag: 100ms)
t2: Follower 2 receives (lag: 500ms)
t3: Follower 3 receives (lag: 2s)

User writes at t0, reads from Follower 3 at t0.5:
â”œâ”€ Expected: user='Alice'
â””â”€ Actual: user not found (lag: 2s, data not yet replicated)

Result: Inconsistent read (stale data)
```

#### Causes of Replication Lag

**1. Network Latency:**
```
Leader (US-East) â†’ Follower (Europe): 100ms latency
Leader (US-East) â†’ Follower (Asia): 300ms latency

Each write incurs network delay
High-traffic: 1000 writes/sec Ã— 300ms = 300s of queued lag!
```

**2. Follower Load (Slow Apply):**
```
Leader writes: 10,000 ops/sec
Follower CPU: 100% (slow disk, slow CPU)
Follower applies: 5,000 ops/sec

Lag accumulates: 5,000 ops/sec behind
After 10 seconds: 50,000 ops behind (10s lag)
```

**3. Large Transactions:**
```
Leader: Single transaction updates 1M rows (takes 10s)
Followers: Replay same transaction (takes 10s)
During replay: 10s lag for all queries

Worse: Other writes queued behind large transaction
```

**4. Schema Changes (DDL):**
```
Leader: ALTER TABLE users ADD COLUMN age INT (takes 30s)
Followers: Replay ALTER TABLE (blocks replication)
During DDL: 30s+ lag, replication paused
```

#### Monitoring Replication Lag

**PostgreSQL:**
```sql
-- On leader
SELECT 
    client_addr,
    state,
    sent_lsn,
    write_lsn,
    flush_lsn,
    replay_lsn,
    sync_state,
    EXTRACT(EPOCH FROM (NOW() - replay_timestamp)) AS lag_seconds
FROM pg_stat_replication;

-- Output:
-- client_addr   | state     | lag_seconds
-- 10.0.1.5      | streaming | 0.5
-- 10.0.1.6      | streaming | 2.3  â† High lag!
```

**MySQL:**
```sql
-- On follower
SHOW SLAVE STATUS\G

-- Output:
-- Seconds_Behind_Master: 5  â† Lag in seconds
-- Slave_IO_Running: Yes
-- Slave_SQL_Running: Yes
```

**DynamoDB:**
```
Built-in metrics (CloudWatch):
â”œâ”€ ReplicationLatency: Milliseconds behind
â”œâ”€ Global tables: Cross-region replication lag
â””â”€ Typical: <1 second lag
```

#### Reducing Replication Lag

**1. Parallel Replication:**
```
Sequential (default):
Leader writes: [Txn1, Txn2, Txn3, Txn4]
Follower applies: Txn1 (10s) â†’ Txn2 (10s) â†’ Txn3 (10s) â†’ Txn4 (10s)
Total: 40s lag

Parallel:
Leader writes: [Txn1, Txn2, Txn3, Txn4]
Follower applies in parallel:
â”œâ”€ Thread 1: Txn1 (10s)
â”œâ”€ Thread 2: Txn2 (10s)
â”œâ”€ Thread 3: Txn3 (10s)
â””â”€ Thread 4: Txn4 (10s)
Total: 10s lag (4Ã— faster)

Requirements:
â”œâ”€ Transactions must be independent (no conflicts)
â””â”€ Preserve commit order within same partition

MySQL: slave_parallel_workers = 4
PostgreSQL: max_parallel_workers = 4
```

**2. Faster Hardware (Follower):**
```
Upgrade follower:
â”œâ”€ Faster CPU (apply transactions faster)
â”œâ”€ Faster disk (SSD instead of HDD)
â””â”€ More memory (larger buffer cache)

Example:
Slow follower: 5,000 ops/sec
Fast follower: 15,000 ops/sec (3Ã— faster)
Lag reduced: 3Ã— less
```

**3. Reduce Write Load:**
```
Leader optimizations:
â”œâ”€ Batch writes (reduce transaction count)
â”œâ”€ Async commit (don't wait for disk sync)
â””â”€ Optimize queries (less work to replicate)

Example:
Before: 1,000 individual INSERTs
After: 1 batch INSERT (1,000 rows)
Replication: 1 transaction instead of 1,000
```

**4. Regional Followers (Reduce Network Lag):**
```
Multi-region setup:
Leader (US-East) â†’ Follower 1 (US-East): 1ms
                 â†’ Follower 2 (Europe): 100ms
                 â†’ Follower 3 (Asia): 300ms

Route reads to nearest follower:
â”œâ”€ US users â†’ Follower 1 (1ms lag)
â”œâ”€ EU users â†’ Follower 2 (100ms lag)
â””â”€ Asia users â†’ Follower 3 (300ms lag)
```

### 1.4 Failover (Leader Promotion)

**Promote follower to leader when leader fails**

```
Scenario:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Leader fails (hardware crash, network) â”‚
â”‚                                        â”‚
â”‚ Before:                                â”‚
â”‚ Leader (down) âŒ                       â”‚
â”‚   â†“ (replication stopped)              â”‚
â”‚ Follower 1, Follower 2, Follower 3     â”‚
â”‚                                        â”‚
â”‚ After (automatic failover):            â”‚
â”‚ Follower 1 â†’ New Leader âœ“              â”‚
â”‚   â†“                                    â”‚
â”‚ Follower 2, Follower 3                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Failover Process:**
```
1. Detect failure:
   â”œâ”€ Heartbeat timeout (no response from leader)
   â”œâ”€ Monitoring (health checks fail)
   â””â”€ Typical: 10-30 seconds detection time

2. Choose new leader:
   â”œâ”€ Most up-to-date follower (highest LSN/position)
   â”œâ”€ Lowest lag
   â””â”€ Healthy (passing health checks)

3. Promote follower:
   â”œâ”€ Stop replication from old leader
   â”œâ”€ Enable writes on new leader
   â””â”€ Update DNS/load balancer to point to new leader

4. Redirect traffic:
   â”œâ”€ Clients reconnect to new leader
   â”œâ”€ Old leader (if recovers) becomes follower
   â””â”€ Total downtime: 30-120 seconds
```

**Challenges:**

**1. Split-Brain (Two Leaders):**
```
Problem:
Network partition separates leader from followers
â”œâ”€ Leader thinks followers are down
â”œâ”€ Followers think leader is down
â””â”€ Followers elect new leader

Result: Two leaders accepting writes!

Leader 1 (old): Writes A, B, C
Leader 2 (new): Writes D, E, F

Conflict when partition heals:
â”œâ”€ Which writes to keep?
â”œâ”€ Data divergence
â””â”€ Inconsistency!

Solution: Fencing (prevent old leader from accepting writes)
â”œâ”€ STONITH (Shoot The Other Node In The Head)
â”œâ”€ Lease-based (leader lease expires, must renew)
â””â”€ Quorum (majority votes required for writes)
```

**2. Data Loss During Failover:**
```
Scenario:
t1: Leader writes: user='Alice' (not yet replicated)
t2: Leader crashes (data lost!)
t3: Follower promoted (doesn't have 'Alice')

Result: 'Alice' insert lost

Mitigation:
â”œâ”€ Synchronous replication (wait for at least 1 follower ACK)
â”œâ”€ Quorum writes (W=2, majority acknowledges)
â””â”€ Accept some data loss (async for performance)
```

**3. Client Confusion:**
```
Client connection:
t1: Client â†’ Leader (old): Write successful
t2: Leader fails, Follower promoted
t3: Client â†’ Old leader (still cached connection): Timeout
t4: Client retries â†’ New leader: Write successful (duplicate!)

Result: Duplicate write (idempotency needed)

Solution:
â”œâ”€ Connection pooling with health checks
â”œâ”€ DNS TTL (short, update quickly)
â”œâ”€ Client retry logic with idempotency keys
```

### 1.5 Quorum Reads/Writes

**Require majority agreement for consistency**

```
Configuration:
N = 3 (replication factor, total nodes)
W = 2 (write quorum, must ACK before success)
R = 2 (read quorum, must read from 2 nodes)

Consistency guarantee: R + W > N
2 + 2 = 4 > 3 âœ“ (guaranteed overlap, strong consistency)
```

**Quorum Write:**
```
Client writes: user='Alice'

Coordinator (any node):
1. Send write to all 3 nodes (parallel)
2. Wait for W=2 nodes to ACK
3. Return success to client

Timeline:
t1: Write sent to Node A, B, C
t2: Node A ACK (fast)
t3: Node B ACK (fast) â† W=2 achieved!
t4: Return success (don't wait for Node C)
t5: Node C ACK (slow, but doesn't matter)

Result: Write acknowledged when 2/3 nodes have data
If Node C fails before ACK: Still successful (2/3 is enough)
```

**Quorum Read:**
```
Client reads: user_id=123

Coordinator:
1. Send read to R=2 nodes (parallel)
2. Receive responses
3. Return latest value (highest timestamp/version)

Timeline:
t1: Read sent to Node A, B
t2: Node A responds: user='Alice', version=5
t3: Node B responds: user='Alice', version=5
t4: Compare: Both version=5, consistent
t5: Return: user='Alice'

Conflict scenario:
t2: Node A responds: user='Alice', version=5
t3: Node B responds: user='Alice-Updated', version=6
t4: Compare: Version 6 > Version 5
t5: Return: user='Alice-Updated' (latest)
t6: Background: Repair Node A (anti-entropy)

Consistency: R+W > N guarantees reading latest write
```

**Tuning Quorum (N, R, W):**
```
Scenario 1: Strong consistency, balanced
N=3, R=2, W=2
â”œâ”€ R+W = 4 > 3 âœ“ (strong consistency)
â”œâ”€ Tolerate 1 node failure
â””â”€ Use: General-purpose

Scenario 2: Read-heavy, fast reads
N=3, R=1, W=3
â”œâ”€ R+W = 4 > 3 âœ“ (strong consistency)
â”œâ”€ Fast reads (only 1 node)
â”œâ”€ Slow writes (wait for all 3)
â””â”€ Use: Read-heavy workloads (caching)

Scenario 3: Write-heavy, fast writes
N=3, R=3, W=1
â”œâ”€ R+W = 4 > 3 âœ“ (strong consistency)
â”œâ”€ Fast writes (only 1 node)
â”œâ”€ Slow reads (must read all 3)
â””â”€ Use: Write-heavy workloads (logging)

Scenario 4: Eventual consistency, maximum performance
N=3, R=1, W=1
â”œâ”€ R+W = 2 < 3 âœ— (no overlap, eventual consistency)
â”œâ”€ Very fast reads and writes
â”œâ”€ May read stale data
â””â”€ Use: Shopping cart, session data

Scenario 5: High durability
N=5, R=3, W=3
â”œâ”€ R+W = 6 > 5 âœ“ (strong consistency)
â”œâ”€ Tolerate 2 node failures
â”œâ”€ More expensive (more replicas)
â””â”€ Use: Critical data (financial)
```

**Sloppy Quorum (Hinted Handoff):**
```
Problem: Not enough nodes available for quorum

Example: N=3, W=2
Node A: Available
Node B: Available
Node C: Down âŒ

Strict quorum: Only 2/3 nodes, can't write (not enough for W=2)
Result: Write fails (availability impact)

Sloppy quorum: Accept write on substitute node
1. Write to Node A, Node B (2 nodes)
2. Write to Node D (substitute, "hint" for Node C)
3. Return success (W=2 achieved with substitute)
4. When Node C recovers: Node D sends "hinted" data to Node C

Trade-off:
+ Higher availability (can write even if nodes down)
- Weaker consistency (data may not be on expected nodes)

DynamoDB: Uses sloppy quorum by default
Cassandra: Hinted handoff configurable
```

---

## 2. Sharding (Horizontal Partitioning)

### 2.1 Shard Keys (Partition Keys)

**Attribute used to distribute data across shards**

```
Good shard key characteristics:
âœ“ High cardinality (many unique values)
âœ“ Even distribution (no hot shards)
âœ“ Predictable access patterns
âœ“ Immutable (doesn't change frequently)

Bad shard key characteristics:
âœ— Low cardinality (few unique values)
âœ— Monotonic (auto-increment, timestamp)
âœ— Skewed distribution (celebrities, popular items)
```

**Example 1: User ID (Good)**
```sql
Shard key: user_id

Distribution:
Shard 1: user_id 0-25M
Shard 2: user_id 25M-50M
Shard 3: user_id 50M-75M
Shard 4: user_id 75M-100M

Query: SELECT * FROM users WHERE user_id = 12345678
â”œâ”€ Hash(12345678) â†’ Shard 2
â””â”€ Query single shard (efficient)

Pros:
+ Even distribution (assuming random user IDs)
+ Single-shard queries (by user_id)
+ High cardinality (millions of users)

Cons:
- Range queries expensive (SELECT * WHERE user_id BETWEEN X AND Y)
- Cross-shard joins (users + orders)
```

**Example 2: Timestamp (Bad - Monotonic)**
```sql
Shard key: created_at

Distribution:
Shard 1: created_at < '2023-01-01'
Shard 2: created_at '2023-01-01' to '2024-01-01'
Shard 3: created_at '2024-01-01' to '2025-01-01'
Shard 4: created_at >= '2025-01-01'

Problem: Hot shard!
All new writes go to Shard 4 (current time)
Shard 1, 2, 3 idle (old data, no writes)

Result:
âŒ Uneven load (Shard 4 overloaded)
âŒ Wasted capacity (Shard 1, 2, 3 underutilized)

Solution: Composite key (user_id + created_at)
```

**Example 3: Country (Bad - Low Cardinality)**
```sql
Shard key: country

Distribution:
Shard 1: country = 'US'
Shard 2: country = 'India'
Shard 3: country = 'UK'
Shard 4: country = 'Others'

Problem: Skewed distribution!
US: 50% of users â†’ Shard 1 overloaded
UK: 5% of users â†’ Shard 3 underutilized

Result:
âŒ Hot shard (Shard 1)
âŒ Inefficient resource usage

Solution: Hash(country + user_id)
```

**Example 4: Composite Key (Good)**
```sql
Shard key: HASH(tenant_id, user_id)

Multi-tenant application:
Tenant 1 (100K users): Distributed across all shards
Tenant 2 (1M users): Distributed across all shards

Even distribution even with skewed tenant sizes

Query patterns:
SELECT * FROM data WHERE tenant_id=1 AND user_id=123
â”œâ”€ Hash(1, 123) â†’ Shard N
â””â”€ Single-shard query

SELECT * FROM data WHERE tenant_id=1
â”œâ”€ Scatter-gather (all shards)
â””â”€ Acceptable (tenant-specific queries)
```

### 2.2 Sharding Strategies

#### Range-Based Sharding

```
Partition by value ranges:

Shard 1: user_id 0 - 1,000,000
Shard 2: user_id 1,000,001 - 2,000,000
Shard 3: user_id 2,000,001 - 3,000,000
Shard 4: user_id 3,000,001 - 4,000,000

Routing:
user_id = 1,500,000 â†’ Shard 2 (1M < 1.5M <= 2M)

Pros:
+ Range queries efficient (SELECT * WHERE user_id BETWEEN 1M AND 1.5M â†’ Shard 2)
+ Simple routing logic
+ Ordered data (useful for time-series)

Cons:
- Hot shards (if access skewed)
- Rebalancing complex (need to move ranges)
- Monotonic keys problematic

Use case: Time-series data, analytics
```

#### Hash-Based Sharding

```
Partition by hash function:

Shard = HASH(user_id) % num_shards

Example:
HASH(user_id=123) = 456789
456789 % 4 = 1 â†’ Shard 1

HASH(user_id=456) = 123987
123987 % 4 = 3 â†’ Shard 3

Pros:
+ Even distribution (hash function randomizes)
+ No hot shards (with good hash function)
+ Simple implementation

Cons:
- Range queries inefficient (must query all shards)
- Rebalancing requires rehashing (see consistent hashing)
- No data locality

Use case: Key-value stores, user data, session storage
```

#### Directory-Based Sharding (Lookup Table)

```
Maintain mapping table:

Shard directory:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TenantID â”‚ Shard  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ tenant1  â”‚ Shard1 â”‚
â”‚ tenant2  â”‚ Shard1 â”‚
â”‚ tenant3  â”‚ Shard2 â”‚
â”‚ tenant4  â”‚ Shard3 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Routing:
1. Lookup tenant in directory
2. Route to corresponding shard

Pros:
+ Flexible (can move tenants easily)
+ Control placement (isolate large tenants)
+ No rehashing needed

Cons:
- Extra lookup (latency)
- Directory is SPOF (must be highly available)
- Scalability limit (directory size)

Use case: Multi-tenant SaaS, geographic sharding
```

#### Geo-Based Sharding

```
Partition by geographic region:

Shard 1: US users (data in US-East datacenter)
Shard 2: EU users (data in Europe datacenter)
Shard 3: Asia users (data in Asia datacenter)

Routing by user location:
user.country = 'US' â†’ Shard 1
user.country = 'Germany' â†’ Shard 2
user.country = 'Japan' â†’ Shard 3

Pros:
+ Low latency (data close to users)
+ Data residency compliance (GDPR)
+ Isolation (region failures don't affect others)

Cons:
- Skewed distribution (population imbalance)
- Cross-region queries expensive
- User migration (user moves countries)

Use case: Global applications, compliance requirements
```

### 2.3 Consistent Hashing

**Minimize data movement when adding/removing shards**

```
Problem with simple hash:
Hash = user_id % num_shards

4 shards: user_id % 4
Add 1 shard (5 total): user_id % 5

Example:
user_id = 10 â†’ Old: 10 % 4 = 2 (Shard 2)
              â†’ New: 10 % 5 = 0 (Shard 0) â† Moved!

Result: ~80% of data needs to move (expensive!)
```

**Consistent Hashing Solution:**

```
Hash ring (0 to 2^160):

Place nodes on ring:
Node A: HASH('NodeA') = 40
Node B: HASH('NodeB') = 120
Node C: HASH('NodeC') = 200
Node D: HASH('NodeD') = 280

Ring visualization:
         0
    280  â†“  40
    D â† â— â†’ A
        |
    C â† â— â†’ B
   200     120

Place keys on ring:
Key1: HASH('Key1') = 50
Key2: HASH('Key2') = 150
Key3: HASH('Key3') = 250

Routing (clockwise to next node):
Key1 (50) â†’ Next node clockwise: B (120)
Key2 (150) â†’ Next node clockwise: C (200)
Key3 (250) â†’ Next node clockwise: D (280)

Add Node E (hash = 180):
Ring:
    D(280) â†’ A(40) â†’ B(120) â†’ E(180) â†’ C(200) â†’ D(280)

Affected keys:
Key2 (150) â†’ Old: C(200), New: E(180) â† Only this key moves!
Key1 (50) â†’ Still: B(120) (unchanged)
Key3 (250) â†’ Still: D(280) (unchanged)

Result: Only ~25% of keys move (1 out of 4)
Simple hash: ~80% move
Consistent hash: ~25% move (1/N)
```

**Virtual Nodes (Vnodes):**

```
Problem: Nodes unevenly distributed on ring

Solution: Each physical node â†’ many virtual nodes

Physical: 3 nodes (A, B, C)
Virtual: 256 vnodes per physical node

Ring: [A1, B1, C1, A2, B2, C2, ..., A256, B256, C256]

Benefits:
+ Better load distribution
+ Granular data movement (move vnodes, not entire nodes)
+ Flexible weighting (powerful nodes get more vnodes)

Example:
Node A (powerful): 300 vnodes
Node B (standard): 200 vnodes
Node C (weak): 100 vnodes

Total: 600 vnodes
Node A handles: 300/600 = 50% of data
Node B handles: 200/600 = 33% of data
Node C handles: 100/600 = 17% of data
```

**Implementation:**

```python
import hashlib

class ConsistentHash:
    def __init__(self, nodes, virtual_nodes=256):
        self.virtual_nodes = virtual_nodes
        self.ring = {}
        self.sorted_keys = []
        
        for node in nodes:
            self.add_node(node)
    
    def _hash(self, key):
        return int(hashlib.md5(key.encode()).hexdigest(), 16)
    
    def add_node(self, node):
        for i in range(self.virtual_nodes):
            vnode_key = f"{node}:{i}"
            hash_val = self._hash(vnode_key)
            self.ring[hash_val] = node
            self.sorted_keys.append(hash_val)
        self.sorted_keys.sort()
    
    def remove_node(self, node):
        for i in range(self.virtual_nodes):
            vnode_key = f"{node}:{i}"
            hash_val = self._hash(vnode_key)
            del self.ring[hash_val]
            self.sorted_keys.remove(hash_val)
    
    def get_node(self, key):
        if not self.ring:
            return None
        
        hash_val = self._hash(key)
        
        # Find first node clockwise
        for ring_key in self.sorted_keys:
            if hash_val <= ring_key:
                return self.ring[ring_key]
        
        # Wrap around to first node
        return self.ring[self.sorted_keys[0]]

# Usage
ch = ConsistentHash(['NodeA', 'NodeB', 'NodeC'])

print(ch.get_node('user:123'))  # NodeB
print(ch.get_node('user:456'))  # NodeC
print(ch.get_node('user:789'))  # NodeA

# Add node
ch.add_node('NodeD')
print(ch.get_node('user:123'))  # May move to NodeD (only affected keys)
```

### 2.4 Rebalancing (Resharding)

**Redistribute data when adding/removing shards**

#### Triggers for Rebalancing

```
1. Capacity (shard full):
   Shard 1: 1TB (80% full)
   Shard 2: 1TB (80% full)
   Shard 3: 1TB (95% full) â† Need rebalancing!
   
2. Performance (hot shard):
   Shard 1: 1K requests/sec
   Shard 2: 1K requests/sec
   Shard 3: 10K requests/sec â† Hot shard!
   
3. Scaling (add capacity):
   4 shards â†’ 8 shards (2Ã— capacity)
   
4. Node failure:
   Shard 3 down â†’ Redistribute to Shard 1, 2, 4
```

#### Rebalancing Strategies

**1. Stop-the-World (Offline Rebalancing):**
```
Process:
1. Stop writes (read-only mode)
2. Redistribute data
3. Update routing
4. Resume writes

Timeline:
t0: Stop writes (downtime starts)
t1-t10: Copy data from old shards to new shards
t11: Update routing table
t12: Resume writes (downtime ends)

Downtime: 10-60 minutes (depending on data size)

Pros:
+ Simple implementation
+ No data inconsistency

Cons:
- Downtime (unacceptable for 24/7 services)

Use case: Small databases, maintenance windows
```

**2. Live Rebalancing (Zero-Downtime):**
```
Process:
1. Add new shards (empty)
2. Start copying data (background)
3. Dual writes (write to old and new shards)
4. Cutover (switch reads to new shards)
5. Remove old shards

Timeline:
t0: Add Shard 5, 6, 7, 8 (new shards, empty)
t1: Start copying data from Shard 1-4 to Shard 1-8 (background)
t2: Enable dual writes (write to both old and new shards)
t3: Copy complete (95%)
t4: Cutover reads (read from new shards)
t5: Verify (monitor errors, rollback if needed)
t6: Disable dual writes (write only to new shards)
t7: Remove old shards (decommission)

No downtime! Service remains available

Pros:
+ Zero downtime
+ Gradual migration (can rollback)

Cons:
- Complex implementation
- Dual writes overhead (2Ã— write traffic)
- Data inconsistency risk (synchronization)

Use case: Production databases, 24/7 services
```

**3. Consistent Hashing Rebalancing:**
```
Add Node D:
Before:
Ring: A(40) â†’ B(120) â†’ C(200) â†’ A(40)
Keys: Key1(50)â†’B, Key2(150)â†’C, Key3(250)â†’A

After (D at 180):
Ring: A(40) â†’ B(120) â†’ D(180) â†’ C(200) â†’ A(40)
Keys: Key1(50)â†’B, Key2(150)â†’D, Key3(250)â†’A

Data movement:
Only keys in range (120, 180) move from C to D
~25% of keys move (1 out of 4 nodes)

Process:
1. Add Node D to ring
2. Identify affected key range: (120, 180)
3. Copy keys from Node C to Node D
4. Update routing
5. Delete keys from Node C

Minimal data movement (1/N of total data)
```

**4. Chunk-Based Rebalancing (MongoDB):**
```
MongoDB uses chunks (64MB by default):

Shard 1: [Chunk1, Chunk2, Chunk3]
Shard 2: [Chunk4, Chunk5, Chunk6]
Shard 3: [Chunk7, Chunk8, Chunk9]

Rebalancing:
Shard 1: 300GB (6 chunks)
Shard 2: 200GB (4 chunks)
Shard 3: 100GB (2 chunks) â† Underutilized

Action: Move chunks from Shard 1 to Shard 3
Shard 1: [Chunk1, Chunk2] (move Chunk3)
Shard 2: [Chunk4, Chunk5, Chunk6]
Shard 3: [Chunk7, Chunk8, Chunk9, Chunk3] â† Received chunk

Granular movement (chunk at a time)
Background balancer (automatic)
```

#### Rebalancing Challenges

**1. Data Consistency During Migration:**
```
Problem: Data changing while copying

Timeline:
t1: Start copying Key1 (value='old')
t2: User updates Key1 (value='new')
t3: Finish copying Key1 (value='old') â† Stale data!

Solution: Dual writes
â”œâ”€ Write to both old and new shards
â”œâ”€ Copy captures final state
â””â”€ Cutover when copy complete
```

**2. Performance Impact:**
```
Copying data:
â”œâ”€ Network bandwidth (GB/s)
â”œâ”€ Disk I/O (read old, write new)
â”œâ”€ CPU (serialization, compression)

Impact:
â”œâ”€ Slower queries (resource contention)
â”œâ”€ Higher latency
â””â”€ Potential timeouts

Mitigation:
â”œâ”€ Rate limiting (throttle copy)
â”œâ”€ Copy during off-peak hours
â””â”€ Monitor resource usage
```

**3. Routing Complexity:**
```
During migration:
â”œâ”€ Old routing: Key1 â†’ Shard 1
â”œâ”€ New routing: Key1 â†’ Shard 5

Client request for Key1:
â”œâ”€ Check new shard (Shard 5): Not found
â”œâ”€ Fallback to old shard (Shard 1): Found
â””â”€ Return value (with migration flag)

After migration:
â”œâ”€ Update routing table
â”œâ”€ All requests â†’ New shards
â””â”€ Remove old shards
```

---

## 3. Consistency Models

### 3.1 Strong Consistency (Linearizability)

**All clients see the same data at the same time**

```
Timeline:
t1: Client A writes: x=1
t2: Write completes (acknowledged)
t3: Client B reads: x â†’ Returns 1 (latest value)
t4: Client C reads: x â†’ Returns 1 (same value)

Guarantee:
After write completes, all subsequent reads return new value
No client can read stale data
```

**How It Works:**

```
Quorum with R+W > N:
N=3, R=2, W=2

Write x=1:
1. Coordinator sends to all 3 nodes
2. Wait for W=2 ACKs
3. Return success

Read x:
1. Coordinator reads from R=2 nodes
2. Nodes return: (x=1, version=5), (x=1, version=5)
3. Return x=1 (latest version)

Overlap guaranteed: R=2, W=2 â†’ At least 1 node has latest value
```

**Single-Leader Replication:**
```
All reads/writes go through leader:

Write:
Client â†’ Leader (apply write)
Leader â†’ Followers (replicate)
Return success

Read:
Client â†’ Leader (read latest)
Return value

Guarantee: Leader always has latest data
Drawback: Bottleneck (single point for all traffic)
```

**Pros:**
```
+ Simplest to reason about (no stale reads)
+ ACID transactions
+ Consistency guarantees (linearizability)
```

**Cons:**
```
- Higher latency (wait for quorum/replication)
- Lower availability (can't serve if quorum unavailable)
- Limited scalability (bottleneck on leader/quorum)
```

**Use Cases:**
```
âœ“ Financial transactions (no stale balances)
âœ“ Inventory management (accurate stock counts)
âœ“ User authentication (latest credentials)
âœ“ Distributed locks (coordination)
```

### 3.2 Eventual Consistency

**All replicas converge to same value eventually, but may temporarily differ**

```
Timeline:
t1: Client A writes: x=1 (to Node 1)
t2: Write completes (async replication)
t3: Client B reads from Node 2: x â†’ Returns 0 (old value, not yet replicated)
t4: Replication completes (Node 2 receives x=1)
t5: Client C reads from Node 2: x â†’ Returns 1 (new value)

Guarantee:
If no new writes, all replicas eventually agree
No guarantee on "when" (seconds, minutes, hours)
```

**How It Works:**

```
Async replication:

Write:
1. Client â†’ Node 1: Write x=1
2. Node 1 â†’ Storage: Apply write
3. Node 1 â†’ Client: Success (immediately!)
4. Node 1 â†’ Node 2, 3: Replicate (background)

Read:
1. Client â†’ Any node (load balanced)
2. Node returns current value (may be stale)

Example:
t1: Write x=1 to Node 1
t2: Read from Node 2 â†’ Returns 0 (lag: 500ms)
t3: Read from Node 3 â†’ Returns 0 (lag: 1s)
t4: Read from Node 2 â†’ Returns 1 (replicated)
t5: Read from Node 3 â†’ Returns 1 (replicated)
```

**Conflict Resolution (Last-Write-Wins):**
```
Scenario: Concurrent writes

t1: Client A â†’ Node 1: Write x=1 (timestamp: t1)
t1: Client B â†’ Node 2: Write x=2 (timestamp: t1)

Nodes replicate to each other:
Node 1 sees: x=1 (local), x=2 (from Node 2)
Node 2 sees: x=2 (local), x=1 (from Node 1)

Conflict! Which value to keep?

Last-Write-Wins (LWW):
Compare timestamps: t1 == t1 (tie)
Compare node IDs: Node 2 > Node 1
Winner: x=2 (from Node 2)

Both nodes converge: x=2

Drawback: Lost update (x=1 discarded)
```

**Pros:**
```
+ High availability (can write even if replicas down)
+ Low latency (no wait for replicas)
+ High scalability (independent replicas)
```

**Cons:**
```
- Stale reads (read old data)
- Conflict resolution needed
- Harder to reason about (no guarantees on "when")
```

**Use Cases:**
```
âœ“ Social media (posts, likes, comments)
âœ“ Shopping cart (add/remove items)
âœ“ User profiles (non-critical updates)
âœ“ Analytics (counters, metrics)
âœ“ Caching (acceptable staleness)
```

### 3.3 Read-Your-Writes Consistency

**User sees their own writes immediately, but not others' writes**

```
Timeline:
t1: User A writes: profile_pic='new.jpg'
t2: Write completes
t3: User A reads: profile_pic â†’ Returns 'new.jpg' (own write visible)
t4: User B reads: profile_pic â†’ Returns 'old.jpg' (may not see A's write yet)
t5: Eventually, User B reads: profile_pic â†’ Returns 'new.jpg'

Guarantee:
User always sees their own writes
No guarantee about other users' writes
```

**How It Works:**

```
Session stickiness (route user to same replica):

Write:
1. User A â†’ Replica 1: Write profile_pic='new.jpg'
2. Replica 1 â†’ Storage: Apply write
3. Replica 1 â†’ Client: Success
4. Store session: User A â†’ Replica 1

Read:
1. User A â†’ Load balancer: Read profile_pic
2. Load balancer checks session: User A â†’ Replica 1
3. Route to Replica 1 (has latest write)
4. Return: profile_pic='new.jpg'

User B (no session):
1. User B â†’ Load balancer: Read profile_pic
2. Route to any replica (e.g., Replica 2)
3. Replica 2 may have old value: 'old.jpg'
```

**Read-After-Write with Timestamps:**
```
Write:
1. User writes: x=1 at timestamp t=100
2. Store write timestamp: User â†’ last_write=100

Read:
1. User reads from any replica
2. Include last_write timestamp: "Give me x, but only if version >= 100"
3. Replica checks: Local version=95 < 100 (stale)
4. Replica waits or fetches latest from leader
5. Return: x=1 (version >= 100)

Ensures user always sees writes >= their last write timestamp
```

**Pros:**
```
+ Better UX (users see their own changes)
+ Low latency (read from nearby replica)
+ Good availability (eventual consistency for others)
```

**Cons:**
```
- Session stickiness required (complex routing)
- Cross-device challenges (different sessions)
- Not truly strong consistency
```

**Use Cases:**
```
âœ“ User profile updates (user sees own changes)
âœ“ Post creation (author sees post immediately)
âœ“ Settings changes (user sees new settings)
```

### 3.4 Monotonic Reads Consistency

**User never sees older data after seeing newer data (time doesn't go backward)**

```
Timeline:
t1: User reads from Replica 1: x=5 (version 5)
t2: User reads from Replica 2: x=3 (version 3) âŒ Violation! (went backward)

Monotonic reads guarantee:
t1: User reads from Replica 1: x=5 (version 5)
t2: User reads from Replica 2: x=5 or newer (never x=3)

Guarantee:
Time doesn't go backward for user
Once user sees x=5, will never see x<5
```

**How It Works:**

```
Session stickiness (same replica):

Implementation 1: Always read from same replica
1. User's first read â†’ Replica 1
2. Store session: User â†’ Replica 1
3. All subsequent reads â†’ Replica 1

Guarantee: Same replica, monotonically increasing versions

Implementation 2: Track max version seen
1. User reads: x=5 (version 5)
2. Store session: User â†’ max_version=5
3. Next read from any replica: "Give me x, version >= 5"
4. Replica returns: x=5 or newer (never older)
```

**Example: Facebook Feed:**
```
Without monotonic reads:
t1: User scrolls feed â†’ Replica 1 â†’ Sees post from 10:30am
t2: User refreshes â†’ Replica 2 (lagging) â†’ Sees posts only up to 10:00am
Result: Confusion! Posts disappeared (time went backward)

With monotonic reads:
t1: User scrolls feed â†’ Replica 1 â†’ Sees posts up to 10:30am
t2: User refreshes â†’ Replica 2 â†’ Sees posts up to 10:30am or later
Result: Consistent experience (time moves forward)
```

**Pros:**
```
+ Better UX (no "time travel" confusion)
+ Relatively easy to implement (session stickiness)
+ Good performance (read from any replica with version check)
```

**Cons:**
```
- Session stickiness required
- Cross-device challenges
- Not full strong consistency
```

**Use Cases:**
```
âœ“ Social media feeds (no disappearing posts)
âœ“ Comment threads (no comments disappearing)
âœ“ Timelines (monotonic time)
```

### 3.5 Consistency Model Comparison

| Model | Guarantee | Latency | Availability | Complexity | Use Case |
|-------|-----------|---------|--------------|------------|----------|
| **Strong** | All see same data immediately | High (wait for quorum) | Lower (need quorum) | Low | Banking, inventory |
| **Eventual** | All converge eventually | Low (no wait) | High (any replica) | High (conflicts) | Social media, caching |
| **Read-Your-Writes** | User sees own writes | Medium | Medium | Medium | User profiles, settings |
| **Monotonic Reads** | Time doesn't go backward | Low | High | Low | Feeds, timelines |
| **Causal** | Causally related events ordered | Medium | Medium | High | Chat, comments |

**CAP Theorem Trade-off:**
```
CAP: Choose 2 out of 3:
â”œâ”€ Consistency (C)
â”œâ”€ Availability (A)
â””â”€ Partition tolerance (P)

In practice: Partition tolerance (P) is mandatory (networks fail)
Choose between:
â”œâ”€ CP: Consistency + Partition tolerance (sacrifice availability)
â”‚   â””â”€ Example: Strong consistency, reject requests if can't reach quorum
â””â”€ AP: Availability + Partition tolerance (sacrifice consistency)
    â””â”€ Example: Eventual consistency, always accept requests

Strong consistency (CP):
â”œâ”€ PostgreSQL with synchronous replication
â”œâ”€ Spanner, CockroachDB
â””â”€ ZooKeeper, etcd

Eventual consistency (AP):
â”œâ”€ Cassandra, DynamoDB
â”œâ”€ MongoDB (default config)
â””â”€ Riak, Voldemort
```

---

## Best Practices Summary

```
Replication:
âœ“ Use async replication for performance (accept eventual consistency)
âœ“ Use sync/semi-sync for critical data (accept higher latency)
âœ“ Monitor replication lag (alert if > threshold)
âœ“ Use read replicas for read-heavy workloads
âœ“ Implement automatic failover (reduce downtime)
âœ“ Test failover regularly (chaos engineering)
âœ— Don't ignore replication lag (can cause data inconsistency)
âœ— Don't forget about split-brain (use fencing/quorum)

Sharding:
âœ“ Choose shard key carefully (high cardinality, even distribution)
âœ“ Use consistent hashing (minimize data movement)
âœ“ Monitor shard metrics (disk, CPU, hot shards)
âœ“ Plan for rebalancing (capacity, growth)
âœ“ Use directory service for routing (flexible)
âœ— Don't use monotonic keys (timestamp, auto-increment)
âœ— Don't forget about cross-shard queries (expensive)
âœ— Don't over-shard (overhead, complexity)

Consistency:
âœ“ Choose consistency model based on use case
âœ“ Use strong consistency for critical data (financial)
âœ“ Use eventual consistency for performance (social)
âœ“ Implement read-your-writes for better UX
âœ“ Document consistency guarantees (SLA)
âœ“ Use quorum (R+W>N) for strong consistency
âœ— Don't assume strong consistency by default (check database config)
âœ— Don't ignore CAP theorem (understand trade-offs)

General:
âœ“ Monitor metrics (lag, throughput, latency, errors)
âœ“ Test failure scenarios (node down, network partition)
âœ“ Have rollback plan (rebalancing, migrations)
âœ“ Document architecture (shard key, replication topology)
âœ“ Automate operations (failover, monitoring, alerting)
```

Complete replication, sharding, and consistency foundation! ğŸ”„ğŸ“Š
