# Classic System Design Exercises

## 1. URL Shortener

### How It Works
**Problem**: Convert long URLs into short, memorable links (e.g., `bit.ly/abc123`)

**Key Components**:
- **Hashing**: Generate unique short codes from long URLs
  - Base62 encoding (a-z, A-Z, 0-9) for compact IDs
  - Hash collision resolution (check if short code exists)
  
- **ID Generation**: 
  - Auto-incrementing counter (simple but predictable)
  - Random ID generation with collision check
  - Snowflake/UUID for distributed systems
  - Counter + Base62 encoding (ID 12345 → "dnh")
  
- **Hot Keys**: Popular URLs get massive traffic
  - Cache frequently accessed mappings (Redis/Memcached)
  - CDN for redirect responses
  - Read replicas for popular URLs
  
- **Cache/CDN**:
  - Cache layer: short_url → long_url mapping
  - TTL-based expiration for temporary links
  - Geographic CDN nodes for low-latency redirects

**Flow**:
```
Shorten: POST long_url → generate ID → encode to short_url → store mapping
Redirect: GET short_url → cache check → DB lookup → 301/302 redirect
```

---

## 2. Rate Limiter

### How It Works
**Problem**: Prevent abuse by limiting requests per user/IP/API key

**Key Algorithms**:

- **Token Bucket**: 
  - Bucket holds N tokens, refilled at rate R tokens/sec
  - Each request consumes 1 token
  - Allows bursts up to bucket capacity
  - Smooth for variable traffic patterns
  
- **Leaky Bucket**:
  - Requests enter queue, processed at fixed rate
  - Overflow requests rejected
  - Enforces strict constant output rate
  
- **Fixed Window Counter**:
  - Count requests in fixed time windows (e.g., per minute)
  - Simple but has boundary issues
  
- **Sliding Window Log**:
  - Track timestamps of requests
  - Remove old entries outside window
  - Accurate but memory-intensive

**Quotas**: Different limits per tier (free: 100/hr, pro: 10k/hr)

**Distributed Counters**:
- Redis with atomic INCR operations
- Sliding window in Redis sorted sets (ZADD with timestamps)
- Rate limit rules in centralized cache
- Sticky sessions or consistent hashing to same rate limiter

**Flow**:
```
Request → Extract identifier (user_id/IP) → Check counter → 
  If under limit: increment counter, allow
  If over limit: return 429 Too Many Requests
```

---

## 3. News Feed / Timeline

### How It Works
**Problem**: Show personalized, ranked feed of posts from friends/followers

**Fan-out Strategies**:

- **Fan-out on Write (Push Model)**:
  - When user posts → push to all followers' feeds immediately
  - Pre-compute feeds at write time
  - Fast reads (just fetch pre-built feed)
  - Slow writes for users with millions of followers
  - Used for regular users
  
- **Fan-out on Read (Pull Model)**:
  - When user requests feed → fetch posts from all followees
  - Compute feed on-demand
  - Slow reads (aggregation needed)
  - Fast writes
  - Used for celebrities with huge follower counts
  
- **Hybrid**: 
  - Push for normal users
  - Pull for celebrities
  - Merge at read time

**Ranking**:
- Score posts by: recency, engagement (likes/comments), relationship strength
- Machine learning models for personalization
- Edge rank algorithm (affinity × weight × time decay)

**Denormalization**:
- Store redundant data to avoid joins
- Embed author info in post objects
- Cache aggregated counts (likes, comments)
- Trade consistency for performance

**Flow**:
```
Post creation → Fan-out to followers' feed caches → 
Feed request → Fetch from cache → Rank → Return top N posts
```

---

## 4. Chat / Messenger

### How It Works
**Problem**: Real-time bidirectional messaging between users

**Real-time Delivery**:
- **WebSockets**: Persistent TCP connections for instant message push
- **Long Polling**: HTTP fallback for older browsers
- **Server-Sent Events**: One-way server → client stream
- Message queue (Kafka) for async processing

**Presence**:
- Track online/offline/typing status
- Heartbeat mechanism (ping every 30s)
- Last seen timestamp
- Publish presence updates to Redis pub/sub
- Optimistic UI updates

**Partitioning**:
- **By User ID**: Hash user_id to assign chat server
- **By Conversation ID**: Keep all messages in thread together
- Consistent hashing for load distribution
- Sticky sessions (user always connects to same server)

**Storage**:
- Recent messages in cache (Redis)
- Full history in DB (Cassandra for write-heavy workload)
- Media in object storage (S3)

**Flow**:
```
Send: User A → WebSocket → Chat server → Message queue → 
      Save to DB → Push to User B's WebSocket → Deliver
```

---

## 5. File Storage (Dropbox/Drive)

### How It Works
**Problem**: Store, sync, and share files across devices

**Metadata DB**:
- Separate metadata (filename, size, permissions, version) from content
- MySQL/PostgreSQL for file hierarchy
- Store file_id, owner, path, chunk_ids, modified_time
- Fast lookups without reading file content

**Chunking**:
- Split files into fixed-size blocks (4MB chunks)
- Upload/download in parallel
- Resume interrupted transfers
- Only sync changed chunks (delta sync)

**Deduplication**:
- Content-addressable storage (hash of chunk = chunk_id)
- If chunk exists (same hash), create pointer instead of storing duplicate
- Saves massive storage (OS images, shared libraries)
- Global deduplication across users (privacy concerns)

**Sync**:
- Client polls for changes (long polling or WebSocket)
- Version vectors/timestamps for conflict detection
- Operational transforms for concurrent edits
- Local cache + remote storage synchronization

**Flow**:
```
Upload: File → Split into chunks → Hash each chunk → 
        Check if exists → Upload new chunks → Update metadata
Download: Request file → Fetch metadata → Download chunks in parallel → Reassemble
```

---

## 6. Ride Hailing (Uber/Lyft)

### How It Works
**Problem**: Match riders with nearby drivers in real-time

**Geo-Indexing**:
- **Geohash**: Encode lat/long into string (nearby points share prefix)
- **Quadtree**: Recursively divide map into 4 quadrants
- **S2 Geometry**: Google's library for spherical geometry
- Index drivers by current location for fast range queries

**Dispatch**:
- Request arrives → Find drivers in radius (geo query)
- Rank by: distance, rating, acceptance rate, direction
- Send request to top N drivers (first to accept wins)
- Timeout and retry if no acceptance
- State machine for trip lifecycle

**State Machines**:
```
IDLE → REQUESTED → ACCEPTED → ARRIVED → PICKED_UP → 
DROPPED_OFF → COMPLETED → PAID
```
- Track state transitions for consistency
- Handle failures (driver cancels, rider no-show)

**Hot Regions**:
- High demand areas (airports, stadiums during events)
- Surge pricing algorithm
- Pre-positioning drivers via predictive models
- Sharding by geographic region
- Cache popular routes

**Flow**:
```
Rider requests → Geo query for nearby drivers → 
Dispatch algorithm ranks → Push to drivers → 
First accept wins → Real-time tracking via GPS updates
```

---

## 7. Search Autocomplete

### How It Works
**Problem**: Suggest completions as user types (Google search box)

**Prefix Indexes**:
- **Trie (Prefix Tree)**: Store all search terms
  - Each node represents character
  - Traverse from root following input prefix
  - Return top K suggestions from subtree
  
- **Inverted Index**: Term → frequency mapping
  - Store popular queries with scores
  - Prefix match on indexed terms
  
- **Precomputed Suggestions**: 
  - For each prefix, pre-calculate top N results
  - Store in cache (Redis)

**Debouncing**:
- Don't query on every keystroke
- Wait 100-200ms after user stops typing
- Cancel previous requests if new input arrives
- Reduces server load by 10x

**Caching**:
- Cache popular prefixes ("goog", "face")
- LRU/LFU eviction policies
- Personalized caches per user
- CDN for static/popular suggestions
- Update cache periodically with trending searches

**Ranking**:
- By popularity (search volume)
- By recency (trending topics)
- Personalized (user history)
- Typo tolerance (fuzzy matching)

**Flow**:
```
User types "goo" → Debounce → Check cache → 
Trie lookup for "goo*" → Return ["google", "good morning", "google maps"] → Display
```

---

## 8. Ad Serving

### How It Works
**Problem**: Serve relevant ads with low latency (<100ms) at massive scale

**Low-Latency Joins**:
- Pre-join ad data with user segments
- Denormalize ad metadata (avoid joins at serve time)
- In-memory databases (Redis, Aerospike)
- Feature vectors pre-computed and cached

**Pacing**:
- Budget control (spend $1000 over 24 hours evenly)
- Throttle ad impressions to avoid budget exhaustion
- Real-time spend tracking per campaign
- Predictive pacing algorithms

**Targeting Caches**:
- User segments cached (age: 25-34, interests: tech)
- Ad eligibility rules pre-evaluated
- Bloom filters for negative targeting (exclude iOS users)
- Geographic, demographic, behavioral indexes

**Architecture**:
```
Ad Request → User profile lookup (cache) → 
Match eligible ads → Auction/Ranking → 
Select winner → Record impression → Serve ad
```

**Challenges**:
- Real-time bidding (RTB) auctions in <50ms
- Click-through rate (CTR) prediction models
- Fraud detection
- A/B testing different ad creatives

---

## 9. Video Streaming (YouTube/Netflix)

### How It Works
**Problem**: Deliver high-quality video to millions of concurrent viewers

**Chunking**:
- Split video into small segments (2-10 second chunks)
- Each chunk encoded at multiple bitrates (360p, 720p, 1080p, 4K)
- HTTP-based delivery (not streaming protocols like RTMP)
- Enables adaptive bitrate streaming

**ABR (Adaptive Bitrate)**:
- Client measures bandwidth continuously
- Switch to lower quality if network degrades
- Switch to higher quality if bandwidth improves
- Algorithms: buffer-based, throughput-based, hybrid
- Smooth playback without buffering

**CDN**:
- Cache video chunks at edge locations globally
- 90%+ of traffic served from CDN (not origin)
- Popular videos cached, long-tail on-demand
- Origin fetch only on cache miss

**Prefetching**:
- Download next chunks before playback reaches them
- Maintain 10-30 second buffer
- Predict user behavior (skip intro, watch next episode)
- Preload recommendations

**Encoding Pipeline**:
```
Upload → Transcode to multiple resolutions → 
Chunk into segments → Upload to CDN → 
Generate manifest file (playlist) → Ready to stream
```

**Flow**:
```
User hits play → Fetch manifest → 
Download first chunk (low quality for fast start) → 
Measure bandwidth → Request appropriate quality → 
Continuous ABR adjustments → Prefetch ahead
```

---

## Key Patterns Across Systems

1. **Caching**: Nearly every system uses multi-layer caching (client, CDN, server, DB)
2. **Sharding**: Partition data by user_id, geo, or entity_id for horizontal scaling
3. **Async Processing**: Decouple writes via message queues (Kafka, RabbitMQ)
4. **Denormalization**: Trade consistency for read performance
5. **CDN**: Push static/cacheable content to edge for low latency
6. **Rate Limiting**: Protect backend from abuse and overload
7. **Monitoring**: Logs, metrics, alerts for observability
8. **Gradual Rollouts**: Feature flags, A/B testing, canary deployments