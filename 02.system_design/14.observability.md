# Observability

## What Is Observability?

**Understand system behavior through logs, metrics, and traces**

```
Monitoring vs Observability:

Monitoring (traditional):
â”œâ”€ Predefined metrics (CPU, memory, request count)
â”œâ”€ Known failure modes (disk full, service down)
â”œâ”€ Dashboards with fixed queries
â””â”€ "Is the system up?"

Observability (modern):
â”œâ”€ High-cardinality data (user_id, session_id, region, etc.)
â”œâ”€ Unknown failure modes (debug new issues)
â”œâ”€ Ad-hoc queries (slice data any way)
â””â”€ "Why is the system slow for user X in region Y?"

Three pillars:
1. Logs: What happened (events, errors)
2. Metrics: How much, how fast (aggregates over time)
3. Traces: Request path through distributed system
```

---

## 1. Structured Logging

### 1.1 Unstructured vs Structured

**Machine-parsable logs with context**

```
Unstructured (plain text):
2024-01-15 10:23:45 User 12345 logged in from 192.168.1.1
2024-01-15 10:24:01 Order 67890 failed: payment declined

Problems:
â”œâ”€ Hard to parse (regex nightmare)
â”œâ”€ No consistent format
â”œâ”€ Difficult to search/filter
â””â”€ No indexing

Structured (JSON):
{
  "timestamp": "2024-01-15T10:23:45Z",
  "level": "info",
  "event": "user_login",
  "user_id": 12345,
  "ip": "192.168.1.1",
  "service": "auth"
}

{
  "timestamp": "2024-01-15T10:24:01Z",
  "level": "error",
  "event": "order_failed",
  "order_id": 67890,
  "error": "payment_declined",
  "user_id": 12345,
  "service": "payments"
}

Benefits:
+ Easy to parse (JSON)
+ Consistent schema
+ Searchable fields (user_id, order_id)
+ Aggregatable (count by event, user_id)
```

### 1.2 Log Levels

**Severity classification**

```
TRACE: Very detailed (function calls, variable values)
â”œâ”€ Use: Development, debugging
â””â”€ Volume: Extremely high

DEBUG: Detailed diagnostic (algorithm steps, state changes)
â”œâ”€ Use: Development, troubleshooting
â””â”€ Volume: High

INFO: Normal operations (user logged in, order placed)
â”œâ”€ Use: Business events, audit trail
â””â”€ Volume: Medium

WARN: Unexpected but recoverable (retry, fallback)
â”œâ”€ Use: Degraded performance, potential issues
â””â”€ Volume: Low

ERROR: Failures requiring attention (request failed, exception)
â”œâ”€ Use: Errors affecting users
â””â”€ Volume: Very low (should be rare)

FATAL/CRITICAL: System-wide failures (service crash, data loss)
â”œâ”€ Use: Catastrophic failures
â””â”€ Volume: Extremely rare

Production settings:
â”œâ”€ Default: INFO (capture business events)
â”œâ”€ Debugging: DEBUG or TRACE (temporarily)
â””â”€ Storage cost: Higher levels = less data
```

### 1.3 Structured Logging Implementation

**Context-rich logging with correlation**

```python
import logging
import json
import uuid
from datetime import datetime

# Structured logger
class StructuredLogger:
    def __init__(self, service_name):
        self.service_name = service_name
        self.logger = logging.getLogger(service_name)
    
    def _log(self, level, event, **kwargs):
        log_entry = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "level": level,
            "service": self.service_name,
            "event": event,
            **kwargs
        }
        
        # Add trace context if available
        if hasattr(self, 'trace_id'):
            log_entry['trace_id'] = self.trace_id
        if hasattr(self, 'span_id'):
            log_entry['span_id'] = self.span_id
        
        self.logger.log(
            getattr(logging, level.upper()),
            json.dumps(log_entry)
        )
    
    def info(self, event, **kwargs):
        self._log("info", event, **kwargs)
    
    def error(self, event, error=None, **kwargs):
        if error:
            kwargs['error'] = str(error)
            kwargs['error_type'] = type(error).__name__
        self._log("error", event, **kwargs)
    
    def warn(self, event, **kwargs):
        self._log("warn", event, **kwargs)

# Usage
logger = StructuredLogger("order-service")

# Business event
logger.info(
    "order_placed",
    order_id=12345,
    user_id=67890,
    amount=99.99,
    currency="USD",
    items=3
)

# Error
try:
    process_payment(order_id)
except PaymentError as e:
    logger.error(
        "payment_failed",
        error=e,
        order_id=12345,
        user_id=67890,
        payment_method="credit_card"
    )

# Output:
# {
#   "timestamp": "2024-01-15T10:23:45Z",
#   "level": "info",
#   "service": "order-service",
#   "event": "order_placed",
#   "order_id": 12345,
#   "user_id": 67890,
#   "amount": 99.99,
#   "currency": "USD",
#   "items": 3
# }
```

### 1.4 Correlation IDs (Trace Context)

**Track requests across services**

```
Problem: Distributed system, single user request
User â†’ API Gateway â†’ Auth Service â†’ Order Service â†’ Payment Service
â””â”€ How to correlate logs from all services for this request?

Solution: Correlation ID (Trace ID)
1. API Gateway generates trace_id: "abc-123-xyz"
2. Passes trace_id to all downstream services (HTTP header)
3. Each service logs with trace_id

Logs:
API Gateway:    {"trace_id": "abc-123-xyz", "event": "request_received"}
Auth Service:   {"trace_id": "abc-123-xyz", "event": "auth_success", "user_id": 123}
Order Service:  {"trace_id": "abc-123-xyz", "event": "order_created", "order_id": 456}
Payment Service:{"trace_id": "abc-123-xyz", "event": "payment_processed"}

Query: trace_id="abc-123-xyz" â†’ See all logs for this request
```

**Implementation:**

```python
from flask import Flask, request, g
import uuid

app = Flask(__name__)

# Middleware: Extract or generate trace ID
@app.before_request
def before_request():
    # Extract from header or generate new
    g.trace_id = request.headers.get('X-Trace-ID', str(uuid.uuid4()))
    g.span_id = str(uuid.uuid4())
    
    # Add to logger
    logger.trace_id = g.trace_id
    logger.span_id = g.span_id

@app.after_request
def after_request(response):
    # Return trace ID to client (for debugging)
    response.headers['X-Trace-ID'] = g.trace_id
    return response

# Service calls: Propagate trace ID
import requests

def call_downstream_service(url, data):
    headers = {
        'X-Trace-ID': g.trace_id,
        'X-Parent-Span-ID': g.span_id
    }
    
    response = requests.post(url, json=data, headers=headers)
    return response

# Endpoint
@app.route('/orders', methods=['POST'])
def create_order():
    logger.info("order_request_received", user_id=request.json['user_id'])
    
    # Call payment service (trace ID propagated)
    payment_response = call_downstream_service(
        'http://payment-service/charge',
        {'amount': 99.99}
    )
    
    logger.info("order_created", order_id=12345)
    
    return {"order_id": 12345}
```

### 1.5 Log Aggregation

**Centralized log storage and search**

```
Architecture:
Service 1 â†’ Log shipper (Filebeat, Fluentd)
Service 2 â†’ Log shipper                      â†’ Log storage (Elasticsearch)
Service 3 â†’ Log shipper                      â†’ Query UI (Kibana)

Flow:
1. Services write logs to stdout/file
2. Log shipper reads logs, parses JSON
3. Shipper sends to Elasticsearch (bulk API)
4. Elasticsearch indexes logs (searchable)
5. Kibana provides UI for search, dashboards

Search examples:
â”œâ”€ Find all errors: level:error
â”œâ”€ Find logs for user: user_id:12345
â”œâ”€ Find slow requests: response_time:>1000
â””â”€ Find traces: trace_id:"abc-123-xyz"
```

**Fluentd Configuration:**

```xml
<!-- fluentd.conf -->
<source>
  @type tail
  path /var/log/app/*.log
  pos_file /var/log/td-agent/app.pos
  tag app.logs
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%SZ
  </parse>
</source>

<filter app.logs>
  @type record_transformer
  <record>
    hostname ${hostname}
    environment production
  </record>
</filter>

<match app.logs>
  @type elasticsearch
  host elasticsearch.example.com
  port 9200
  index_name app-logs-%Y.%m.%d
  type_name _doc
  <buffer>
    @type memory
    flush_interval 5s
  </buffer>
</match>
```

### 1.6 Log Sampling

**Reduce volume for high-traffic services**

```
Problem: 10,000 requests/sec
â”œâ”€ 10,000 INFO logs/sec
â”œâ”€ Storage: 100 GB/day
â””â”€ Cost: $3,000/month

Solution: Sampling
â”œâ”€ Log 100% of errors (all ERROR, WARN)
â”œâ”€ Log 1% of successful requests (sample INFO)
â””â”€ Storage: 10 GB/day, $300/month (90% savings)

Sampling strategies:
1. Random sampling: Log 1% randomly (statistically representative)
2. Head-based sampling: Decide at request start (consistent per trace)
3. Tail-based sampling: Decide at request end (log if error or slow)
4. Adaptive sampling: Increase sampling for rare errors

Tail-based sampling (best):
1. Buffer logs in memory during request
2. At request end, decide: error or slow? â†’ flush logs
3. Otherwise: discard logs (save storage)
```

**Implementation:**

```python
class SamplingLogger:
    def __init__(self, service_name, sample_rate=0.01):
        self.service_name = service_name
        self.sample_rate = sample_rate
        self.buffer = []
        self.should_sample = False
    
    def start_request(self, trace_id):
        self.trace_id = trace_id
        self.buffer = []
        self.should_sample = False
    
    def log(self, level, event, **kwargs):
        log_entry = {
            "level": level,
            "event": event,
            "trace_id": self.trace_id,
            **kwargs
        }
        
        # Always log errors immediately
        if level in ["error", "warn"]:
            self.should_sample = True
            self._flush()
        else:
            # Buffer other logs
            self.buffer.append(log_entry)
    
    def end_request(self, status_code, response_time):
        # Tail-based sampling decision
        if status_code >= 500:  # Server error
            self.should_sample = True
        elif response_time > 1000:  # Slow request (>1s)
            self.should_sample = True
        elif random.random() < self.sample_rate:  # Random sample
            self.should_sample = True
        
        if self.should_sample:
            self._flush()
        else:
            self.buffer.clear()
    
    def _flush(self):
        for log_entry in self.buffer:
            print(json.dumps(log_entry))
        self.buffer.clear()

# Usage
logger = SamplingLogger("api-service", sample_rate=0.01)

@app.before_request
def before():
    logger.start_request(g.trace_id)

@app.after_request
def after(response):
    logger.end_request(response.status_code, g.response_time)
    return response
```

---

## 2. Metrics

### 2.1 RED Method (Request-oriented)

**Request Rate, Error Rate, Duration**

```
RED metrics (for services handling requests):

1. Rate: Requests per second
   â”œâ”€ Metric: request_count (counter)
   â”œâ”€ Labels: method, endpoint, status
   â””â”€ Query: rate(request_count[1m])
   
2. Errors: Error rate
   â”œâ”€ Metric: request_count{status=~"5.."}
   â”œâ”€ Query: rate(request_count{status=~"5.."}[1m]) / rate(request_count[1m])
   â””â”€ Alert: Error rate > 1%
   
3. Duration: Response time distribution
   â”œâ”€ Metric: request_duration_seconds (histogram)
   â”œâ”€ Percentiles: p50, p95, p99
   â””â”€ Query: histogram_quantile(0.95, request_duration_seconds)

Example dashboard:
â”œâ”€ Rate: 1,000 req/sec
â”œâ”€ Errors: 0.5% (5 req/sec failing)
â””â”€ Duration: p50=50ms, p95=200ms, p99=500ms
```

**Prometheus Metrics:**

```python
from prometheus_client import Counter, Histogram
import time

# Define metrics
request_count = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1, 5]  # 10ms, 50ms, 100ms, 500ms, 1s, 5s
)

# Instrument endpoint
@app.route('/api/orders', methods=['POST'])
def create_order():
    start = time.time()
    
    try:
        # Process order
        result = process_order(request.json)
        
        # Record success
        request_count.labels(
            method='POST',
            endpoint='/api/orders',
            status='200'
        ).inc()
        
        return result, 200
        
    except Exception as e:
        # Record error
        request_count.labels(
            method='POST',
            endpoint='/api/orders',
            status='500'
        ).inc()
        
        raise
    
    finally:
        # Record duration
        duration = time.time() - start
        request_duration.labels(
            method='POST',
            endpoint='/api/orders'
        ).observe(duration)

# Prometheus scrapes /metrics endpoint
from prometheus_client import generate_latest

@app.route('/metrics')
def metrics():
    return generate_latest()
```

**Prometheus Queries (PromQL):**

```promql
# Rate: Requests per second (last 1 minute)
rate(http_requests_total[1m])

# Rate by endpoint
sum by (endpoint) (rate(http_requests_total[1m]))

# Error rate
sum(rate(http_requests_total{status=~"5.."}[1m])) 
/ 
sum(rate(http_requests_total[1m]))

# p95 latency
histogram_quantile(0.95, 
  sum by (le) (rate(http_request_duration_seconds_bucket[1m]))
)

# p99 latency by endpoint
histogram_quantile(0.99,
  sum by (endpoint, le) (rate(http_request_duration_seconds_bucket[1m]))
)

# Slow endpoints (p95 > 1 second)
histogram_quantile(0.95,
  sum by (endpoint, le) (rate(http_request_duration_seconds_bucket[1m]))
) > 1
```

### 2.2 USE Method (Resource-oriented)

**Utilization, Saturation, Errors**

```
USE metrics (for resources: CPU, memory, disk, network):

1. Utilization: % resource in use
   â”œâ”€ CPU: 70% busy
   â”œâ”€ Memory: 8 GB / 16 GB = 50%
   â”œâ”€ Disk: 500 GB / 1 TB = 50%
   â””â”€ Network: 500 Mbps / 1 Gbps = 50%
   
2. Saturation: Degree of queuing/waiting
   â”œâ”€ CPU: Run queue length (processes waiting)
   â”œâ”€ Memory: Swap usage, page faults
   â”œâ”€ Disk: I/O queue depth, await time
   â””â”€ Network: TCP retransmits, queue drops
   
3. Errors: Error count
   â”œâ”€ CPU: (rare)
   â”œâ”€ Memory: OOM kills, allocation failures
   â”œâ”€ Disk: Read/write errors
   â””â”€ Network: Packet loss, CRC errors

Resource bottleneck detection:
â”œâ”€ Utilization > 80% â†’ Investigate
â”œâ”€ Saturation > 0 â†’ Resource contention
â””â”€ Errors > 0 â†’ Hardware/config issue
```

**System Metrics Collection:**

```python
import psutil
from prometheus_client import Gauge

# CPU metrics
cpu_utilization = Gauge('cpu_utilization_percent', 'CPU utilization')
cpu_queue_length = Gauge('cpu_queue_length', 'CPU run queue length')

# Memory metrics
memory_utilization = Gauge('memory_utilization_percent', 'Memory utilization')
memory_available_bytes = Gauge('memory_available_bytes', 'Available memory')

# Disk metrics
disk_utilization = Gauge('disk_utilization_percent', 'Disk utilization', ['device'])
disk_io_queue = Gauge('disk_io_queue_depth', 'Disk I/O queue', ['device'])

# Network metrics
network_bytes_sent = Counter('network_bytes_sent_total', 'Network bytes sent')
network_bytes_recv = Counter('network_bytes_recv_total', 'Network bytes received')
network_errors = Counter('network_errors_total', 'Network errors')

# Collect metrics
def collect_system_metrics():
    # CPU
    cpu_utilization.set(psutil.cpu_percent(interval=1))
    cpu_queue_length.set(len(psutil.Process().threads()))
    
    # Memory
    mem = psutil.virtual_memory()
    memory_utilization.set(mem.percent)
    memory_available_bytes.set(mem.available)
    
    # Disk
    for partition in psutil.disk_partitions():
        usage = psutil.disk_usage(partition.mountpoint)
        disk_utilization.labels(device=partition.device).set(usage.percent)
    
    # Network
    net = psutil.net_io_counters()
    network_bytes_sent.inc(net.bytes_sent)
    network_bytes_recv.inc(net.bytes_recv)
    network_errors.inc(net.errin + net.errout)

# Run periodically
import threading
import time

def metrics_collector():
    while True:
        collect_system_metrics()
        time.sleep(15)  # Collect every 15 seconds

threading.Thread(target=metrics_collector, daemon=True).start()
```

### 2.3 Metric Types

**Counters, Gauges, Histograms, Summaries**

```
1. Counter (monotonically increasing):
   Use: Count events (requests, errors, bytes sent)
   Example: http_requests_total = 15,234 (always increasing)
   Query: rate(http_requests_total[1m]) â†’ requests per second
   Reset: On process restart (counter goes to 0)

2. Gauge (value that goes up and down):
   Use: Current state (CPU%, memory, queue length)
   Example: cpu_utilization = 75.5
   Query: cpu_utilization â†’ current value
   No rate() needed (already a snapshot)

3. Histogram (distribution of values):
   Use: Request durations, response sizes
   Buckets: [0.01, 0.05, 0.1, 0.5, 1, 5] (predefined)
   
   Example: Request durations
   â”œâ”€ 0-10ms: 100 requests
   â”œâ”€ 10-50ms: 500 requests
   â”œâ”€ 50-100ms: 300 requests
   â”œâ”€ 100-500ms: 80 requests
   â”œâ”€ 500ms-1s: 15 requests
   â””â”€ 1s-5s: 5 requests
   
   Query: histogram_quantile(0.95, ...) â†’ p95 latency
   Aggregatable: Can aggregate across instances

4. Summary (similar to histogram, client-side quantiles):
   Use: Same as histogram
   Quantiles: Computed on client (0.5, 0.9, 0.99)
   
   Example:
   â”œâ”€ p50: 50ms
   â”œâ”€ p90: 200ms
   â””â”€ p99: 500ms
   
   Not aggregatable: Can't combine summaries from multiple instances
   Prefer: Histogram (aggregatable)
```

### 2.4 Cardinality

**Dimension explosion problem**

```
Problem: High-cardinality labels
metric_name{user_id="123", session_id="abc", ip="1.2.3.4", ...}

Example: 1 million users
â”œâ”€ user_id label: 1 million unique values
â”œâ”€ Time series: 1 million (one per user)
â””â”€ Storage explosion, slow queries

Solution: Low-cardinality labels only
Good labels (bounded values):
â”œâ”€ endpoint: /api/orders, /api/users (10-100 values)
â”œâ”€ status: 200, 404, 500 (5-10 values)
â”œâ”€ method: GET, POST, PUT, DELETE (4 values)
â””â”€ region: us-east, us-west, eu-west (5-10 values)

Bad labels (unbounded values):
â”œâ”€ user_id: 1,000,000+ values âœ—
â”œâ”€ session_id: Millions âœ—
â”œâ”€ ip_address: Millions âœ—
â””â”€ trace_id: Billions âœ—

Use logs for high-cardinality data (user_id, trace_id)
Use metrics for aggregates (count by endpoint, region)

Rule: Keep labels < 100 unique values (max 1000)
```

---

## 3. Distributed Tracing

### 3.1 Tracing Concepts

**Track requests across microservices**

```
Trace: End-to-end request path
â”œâ”€ Trace ID: Unique identifier for request
â””â”€ Spans: Individual operations within trace

Example trace:
Trace ID: abc-123-xyz
â”œâ”€ Span 1: API Gateway (100ms)
â”‚  â”œâ”€ Span 2: Auth Service (20ms)
â”‚  â””â”€ Span 3: Order Service (70ms)
â”‚     â”œâ”€ Span 4: Database query (30ms)
â”‚     â””â”€ Span 5: Payment Service (35ms)
â”‚        â””â”€ Span 6: External API (25ms)

Visualization (waterfall):
API Gateway    [====================] 100ms
  Auth         [====] 20ms
  Order              [==============] 70ms
    Database           [======] 30ms
    Payment                  [=======] 35ms
      Ext API                  [===] 25ms

Insights:
â”œâ”€ Total time: 100ms
â”œâ”€ Slowest span: Order Service (70ms)
â””â”€ External API adds 25ms latency
```

### 3.2 OpenTelemetry

**Standard for tracing, metrics, logs**

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.instrumentation.flask import FlaskInstrumentor

# Initialize tracer
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

# Export to Jaeger
jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=6831,
)
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(jaeger_exporter)
)

# Auto-instrument Flask
app = Flask(__name__)
FlaskInstrumentor().instrument_app(app)

# Manual instrumentation
@app.route('/api/orders', methods=['POST'])
def create_order():
    # Span automatically created by FlaskInstrumentor
    # Add custom span for database operation
    with tracer.start_as_current_span("database_query") as span:
        span.set_attribute("db.system", "postgresql")
        span.set_attribute("db.statement", "INSERT INTO orders ...")
        
        order_id = db.execute("INSERT INTO orders ...")
        
        span.set_attribute("order_id", order_id)
    
    # Add custom span for payment service call
    with tracer.start_as_current_span("payment_service") as span:
        span.set_attribute("http.method", "POST")
        span.set_attribute("http.url", "http://payment-service/charge")
        
        try:
            response = requests.post(
                "http://payment-service/charge",
                json={"amount": 99.99}
            )
            
            span.set_attribute("http.status_code", response.status_code)
            
        except Exception as e:
            span.set_status(Status(StatusCode.ERROR, str(e)))
            span.record_exception(e)
            raise
    
    return {"order_id": order_id}
```

**Trace Context Propagation:**

```python
# Automatic propagation (HTTP headers)
# W3C Trace Context standard:
# traceparent: 00-{trace_id}-{span_id}-01
# tracestate: key1=value1,key2=value2

# Example headers:
# traceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01
#              â””â”€ version
#                 â””â”€ trace_id (16 bytes hex)
#                                           â””â”€ span_id (8 bytes hex)
#                                                         â””â”€ flags

# Client propagates trace context
import requests
from opentelemetry import trace
from opentelemetry.propagate import inject

def call_downstream_service(url, data):
    headers = {}
    
    # Inject trace context into headers
    inject(headers)
    
    # Headers now contain: traceparent, tracestate
    response = requests.post(url, json=data, headers=headers)
    
    return response

# Downstream service extracts context
from opentelemetry.propagate import extract

@app.before_request
def before_request():
    # Extract trace context from headers
    ctx = extract(request.headers)
    
    # Subsequent spans will be children of extracted context
```

### 3.3 Sampling

**Reduce tracing overhead**

```
Problem: Tracing every request
â”œâ”€ 10,000 req/sec Ã— 5 spans/req = 50,000 spans/sec
â”œâ”€ Storage: 100 GB/day
â””â”€ Network overhead: Sending spans to collector

Solution: Sampling
â”œâ”€ Sample 1% of traces (100 req/sec)
â”œâ”€ Storage: 1 GB/day
â””â”€ Still representative (statistically)

Sampling strategies:

1. Head-based (decided at trace start):
   â”œâ”€ Random: Sample 1% of all traces
   â”œâ”€ Rate-limiting: Max 100 traces/sec
   â””â”€ Consistent: Same decision across all services

2. Tail-based (decided at trace end):
   â”œâ”€ Sample all errors (status code 500)
   â”œâ”€ Sample slow requests (>1 second)
   â”œâ”€ Sample random 1% of successful requests
   â””â”€ Better: Captures interesting traces

3. Adaptive:
   â”œâ”€ Increase sampling for rare endpoints
   â”œâ”€ Decrease sampling for high-traffic endpoints
   â””â”€ Balance: Coverage vs volume
```

**Tail-based Sampling:**

```python
from opentelemetry.sdk.trace import SpanProcessor
from opentelemetry.sdk.trace.export import SpanExporter

class TailBasedSamplingProcessor(SpanProcessor):
    def __init__(self, exporter, sample_rate=0.01):
        self.exporter = exporter
        self.sample_rate = sample_rate
        self.trace_buffers = {}  # {trace_id: [spans]}
    
    def on_start(self, span, parent_context=None):
        pass
    
    def on_end(self, span):
        trace_id = span.get_span_context().trace_id
        
        # Buffer span
        if trace_id not in self.trace_buffers:
            self.trace_buffers[trace_id] = []
        self.trace_buffers[trace_id].append(span)
        
        # Root span (end of trace)
        if span.parent is None:
            self._decide_sampling(trace_id)
    
    def _decide_sampling(self, trace_id):
        spans = self.trace_buffers[trace_id]
        
        # Sample if error
        has_error = any(span.status.status_code == StatusCode.ERROR for span in spans)
        
        # Sample if slow (root span > 1s)
        root_span = [s for s in spans if s.parent is None][0]
        is_slow = (root_span.end_time - root_span.start_time) > 1_000_000_000  # 1 second
        
        # Random sample
        random_sample = random.random() < self.sample_rate
        
        if has_error or is_slow or random_sample:
            # Export spans
            for span in spans:
                self.exporter.export([span])
        
        # Clear buffer
        del self.trace_buffers[trace_id]
```

### 3.4 Flamegraphs

**Visualize performance bottlenecks**

```
Flamegraph: Stack traces aggregated over time
â”œâ”€ X-axis: % of time spent
â”œâ”€ Y-axis: Call stack depth
â””â”€ Width: Proportion of time

Example:
main() [========================================] 100%
â”œâ”€ process_request() [========================] 60%
â”‚  â”œâ”€ database_query() [=============] 33%
â”‚  â””â”€ external_api() [===========] 28%
â””â”€ render_response() [================] 40%
   â””â”€ template_render() [===========] 25%

Insights:
â”œâ”€ database_query: 33% of time (optimize queries)
â”œâ”€ external_api: 28% of time (cache, parallelize)
â””â”€ template_render: 25% of time (cache templates)

Tools:
â”œâ”€ py-spy (Python profiler, generates flamegraphs)
â”œâ”€ pprof (Go profiler)
â””â”€ async-profiler (Java)
```

**Generate Flamegraph:**

```bash
# Install py-spy
pip install py-spy

# Profile running process
py-spy record -o flamegraph.svg --pid 12345

# Profile new process
py-spy record -o flamegraph.svg -- python app.py

# Open flamegraph.svg in browser
# Interactive: Click to zoom, search for functions
```

**Continuous Profiling:**

```python
# Pyroscope: Continuous profiling
import pyroscope

pyroscope.configure(
    application_name="my-app",
    server_address="http://pyroscope-server:4040",
)

# Automatically profiles application
# Uploads flamegraphs to Pyroscope server
# Query historical flamegraphs (last week, month)

# Compare flamegraphs (before/after deployment)
# Detect performance regressions
```

---

## 4. Alerting & On-Call

### 4.1 SLO & Error Budgets

**Service Level Objectives and error budgets**

```
SLI (Service Level Indicator): Metric measuring service quality
â”œâ”€ Availability: % of successful requests
â”œâ”€ Latency: % of requests < 200ms
â””â”€ Throughput: Requests per second

SLO (Service Level Objective): Target for SLI
â”œâ”€ Availability: 99.9% (3 nines)
â”œâ”€ Latency: 95% of requests < 200ms
â””â”€ Throughput: Support 10,000 req/sec

SLA (Service Level Agreement): Contract with customers
â”œâ”€ Availability: 99.9% (or refund)
â””â”€ Usually SLA < SLO (buffer for incidents)

Error Budget: Allowed downtime
â”œâ”€ SLO: 99.9% availability
â”œâ”€ Downtime allowed: 0.1% = 43 minutes/month
â””â”€ Error budget: 43 minutes

Error budget consumption:
â”œâ”€ Incident: 10 minutes downtime
â”œâ”€ Remaining budget: 33 minutes
â””â”€ If budget exhausted: Freeze releases, focus on reliability
```

**Calculate Error Budget:**

```python
# SLO: 99.9% availability (3 nines)
SLO = 0.999

# Time period: 30 days
PERIOD_DAYS = 30
PERIOD_SECONDS = PERIOD_DAYS * 24 * 60 * 60  # 2,592,000 seconds

# Error budget: Allowed downtime
error_budget_seconds = PERIOD_SECONDS * (1 - SLO)
# 2,592,000 * 0.001 = 2,592 seconds = 43.2 minutes

# Current month metrics
total_requests = 10_000_000
failed_requests = 5_000  # 500 errors

availability = (total_requests - failed_requests) / total_requests
# (10,000,000 - 5,000) / 10,000,000 = 0.9995 = 99.95%

# Error budget consumption
error_budget_consumed = (1 - availability) / (1 - SLO)
# (1 - 0.9995) / (1 - 0.999) = 0.5 = 50%

# Remaining budget
remaining_budget_seconds = error_budget_seconds * (1 - error_budget_consumed)
# 2,592 * 0.5 = 1,296 seconds = 21.6 minutes

print(f"Error budget consumed: {error_budget_consumed * 100:.1f}%")
print(f"Remaining budget: {remaining_budget_seconds / 60:.1f} minutes")

# Alert if error budget consumption > 80%
if error_budget_consumed > 0.8:
    alert("Error budget nearly exhausted!")
```

### 4.2 Alerting Rules

**When to alert, what to alert on**

```
Good alerts:
â”œâ”€ Actionable: Clear action needed (restart service, scale up)
â”œâ”€ User-impacting: Affects users (not internal metrics)
â”œâ”€ Urgent: Requires immediate response
â””â”€ Novel: Not duplicate of existing alert

Bad alerts:
â”œâ”€ Flapping: Alerts firing/resolving repeatedly
â”œâ”€ Noisy: Too many alerts (alert fatigue)
â”œâ”€ Not actionable: "Disk will be full in 6 months"
â””â”€ Symptom-only: Alert on symptom, not cause

Symptom-based alerting:
âœ“ Alert: Error rate > 1% (user-facing symptom)
âœ— Alert: Database CPU > 80% (may not affect users)

Cause-based alerting:
âœ— Alert: Service down (cause, but what's the impact?)
âœ“ Alert: Error rate > 1% + Service down (symptom + cause)
```

**Prometheus Alerting Rules:**

```yaml
# prometheus-alerts.yml
groups:
  - name: SLO
    interval: 1m
    rules:
      # Error budget burn rate alert
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total[5m]))
          ) > 0.01
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Error rate {{ $value | humanizePercentage }} exceeds 1%"
          description: "Service {{ $labels.service }} error rate is high, burning error budget"
      
      # Latency SLO breach
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, 
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service)
          ) > 0.2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "p95 latency {{ $value }}s exceeds 200ms"
          description: "Service {{ $labels.service }} is slow"
      
      # Availability (uptime)
      - alert: ServiceDown
        expr: up{job="my-service"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.instance }} is down"
          description: "No metrics received for 1 minute"
      
      # Saturation (resource exhaustion)
      - alert: HighMemoryUsage
        expr: |
          (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) 
          / node_memory_MemTotal_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Memory usage {{ $value | humanizePercentage }} on {{ $labels.instance }}"
          description: "Memory critically low, consider scaling"
```

**Alert Routing (Alertmanager):**

```yaml
# alertmanager.yml
global:
  slack_api_url: 'https://hooks.slack.com/services/...'
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

route:
  group_by: ['alertname', 'service']
  group_wait: 30s        # Wait 30s before sending (batch alerts)
  group_interval: 5m     # Send updates every 5 minutes
  repeat_interval: 4h    # Re-send every 4 hours if unresolved
  
  receiver: 'slack-notifications'
  
  routes:
    # Critical alerts â†’ PagerDuty (on-call)
    - match:
        severity: critical
      receiver: 'pagerduty'
      continue: true      # Also send to Slack
    
    # Warning alerts â†’ Slack only
    - match:
        severity: warning
      receiver: 'slack-notifications'

receivers:
  - name: 'slack-notifications'
    slack_configs:
      - channel: '#alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ end }}'
  
  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'YOUR_SERVICE_KEY'

# Inhibition rules (suppress alerts)
inhibit_rules:
  # If ServiceDown fires, suppress HighLatency (redundant)
  - source_match:
      alertname: 'ServiceDown'
    target_match:
      alertname: 'HighLatency'
    equal: ['service']
```

### 4.3 Dashboards

**Visualize system health**

```
Dashboard structure:

1. Overview (high-level health):
   â”œâ”€ SLO compliance (99.95% availability this month)
   â”œâ”€ Error budget remaining (21 minutes)
   â”œâ”€ Request rate (1,000 req/sec)
   â””â”€ Error rate (0.5%)

2. RED metrics (per service):
   â”œâ”€ Rate: Requests per second (timeseries)
   â”œâ”€ Errors: Error rate % (timeseries)
   â””â”€ Duration: p50/p95/p99 latency (timeseries)

3. USE metrics (resources):
   â”œâ”€ CPU: Utilization % (timeseries)
   â”œâ”€ Memory: Utilization % (timeseries)
   â”œâ”€ Disk: Utilization %, I/O (timeseries)
   â””â”€ Network: Throughput, errors (timeseries)

4. Dependencies (downstream services):
   â”œâ”€ Database: Query latency, connections
   â”œâ”€ Cache: Hit rate, evictions
   â””â”€ External APIs: Response time, errors

5. Business metrics:
   â”œâ”€ Orders per minute
   â”œâ”€ Revenue per hour
   â””â”€ Active users
```

**Grafana Dashboard (JSON):**

```json
{
  "dashboard": {
    "title": "Service Overview",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[1m])) by (service)",
            "legendFormat": "{{ service }}"
          }
        ],
        "yaxes": [{"label": "req/sec"}]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{status=~\"5..\"}[1m])) / sum(rate(http_requests_total[1m]))",
            "legendFormat": "Error rate"
          }
        ],
        "yaxes": [{"label": "%", "format": "percentunit"}],
        "alert": {
          "conditions": [
            {
              "type": "query",
              "query": {"params": ["A", "5m", "now"]},
              "reducer": {"type": "avg"},
              "evaluator": {"type": "gt", "params": [0.01]}
            }
          ]
        }
      },
      {
        "title": "Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket[1m])) by (le))",
            "legendFormat": "p50"
          },
          {
            "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[1m])) by (le))",
            "legendFormat": "p95"
          },
          {
            "expr": "histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[1m])) by (le))",
            "legendFormat": "p99"
          }
        ],
        "yaxes": [{"label": "seconds"}]
      }
    ]
  }
}
```

### 4.4 Runbooks

**Documented procedures for incidents**

```markdown
# Runbook: High Error Rate Alert

## Alert Details
- **Alert Name:** HighErrorRate
- **Severity:** Critical
- **Threshold:** Error rate > 1% for 5 minutes
- **Impact:** Users experiencing failures

## Triage Steps
1. **Verify alert is firing:**
   - Check Grafana dashboard: [link]
   - Confirm error rate > 1%

2. **Identify affected endpoints:**
   ```promql
   topk(5, sum by (endpoint) (rate(http_requests_total{status=~"5.."}[5m])))
   ```

3. **Check recent deployments:**
   - Rollout dashboard: [link]
   - Recent deploys (last 1 hour)

4. **Check dependencies:**
   - Database: Is it slow? Down?
   - Redis: Cache hit rate dropped?
   - External APIs: Returning errors?

## Investigation
1. **View error logs:**
   ```
   level:error AND service:my-service | tail -100
   ```

2. **Sample error traces:**
   - Jaeger: [link]
   - Filter: error=true, last 15 minutes
   - Identify slow spans, failed calls

3. **Check resource utilization:**
   - CPU > 80%? (scale horizontally)
   - Memory > 90%? (memory leak? restart?)
   - Disk full? (clear logs, increase size)

## Mitigation
1. **Rollback recent deployment:**
   ```bash
   kubectl rollout undo deployment/my-service
   ```

2. **Scale up (if resource constrained):**
   ```bash
   kubectl scale deployment/my-service --replicas=10
   ```

3. **Circuit breaker (if dependency failing):**
   - Enable circuit breaker for failing dependency
   - Return degraded response (cached data, default value)

4. **Rate limiting (if abusive traffic):**
   - Identify abusive IPs/users (top offenders)
   - Block at API gateway

## Recovery
1. **Monitor error rate:**
   - Should drop below 1% within 5 minutes

2. **Verify user impact:**
   - Check support tickets
   - User-facing dashboards

3. **Postmortem:**
   - Document incident: [template]
   - Root cause analysis
   - Action items (prevent recurrence)

## Contacts
- **On-call engineer:** PagerDuty rotation
- **Escalation:** Engineering manager
- **Stakeholders:** Product team, customer support
```

### 4.5 On-Call Best Practices

```
On-call rotation:
â”œâ”€ Primary on-call (first responder)
â”œâ”€ Secondary on-call (escalation)
â””â”€ Rotation: Weekly (sustainable, fresh perspective)

Alert fatigue prevention:
â”œâ”€ Tune alerts (reduce false positives)
â”œâ”€ Aggregate alerts (batch similar alerts)
â”œâ”€ Suppress during maintenance (planned downtime)
â””â”€ Runbooks (reduce MTTR, time to resolve)

Incident response:
1. Acknowledge alert (PagerDuty, Slack)
2. Assess severity (user-impacting? widespread?)
3. Mitigate (rollback, scale, circuit breaker)
4. Communicate (status page, Slack, stakeholders)
5. Investigate root cause (logs, traces, metrics)
6. Resolve (fix, deploy, verify)
7. Postmortem (blameless, action items)

Postmortem template:
â”œâ”€ Incident summary (what happened)
â”œâ”€ Timeline (events, actions taken)
â”œâ”€ Root cause (why it happened)
â”œâ”€ Impact (users affected, downtime)
â”œâ”€ Mitigation (what fixed it)
â”œâ”€ Action items (prevent recurrence)
â””â”€ Lessons learned (what we learned)

Blameless culture:
âœ“ Focus on systems, not people
âœ“ Human error is symptom, not cause
âœ“ Ask "why" 5 times (5 Whys technique)
âœ“ Action items: Improve systems, processes, monitoring
```

---

## Best Practices Summary

```
Structured Logging:
âœ“ Use JSON format (machine-parsable)
âœ“ Include trace_id, span_id (correlation)
âœ“ Log at appropriate levels (INFO for business events, ERROR for failures)
âœ“ Sample high-volume logs (reduce costs)
âœ“ Include context (user_id, order_id, request_id)
âœ“ Centralize logs (Elasticsearch, Loki)
âœ— Don't log sensitive data (PII, passwords, tokens)
âœ— Don't use unstructured text (hard to parse)
âœ— Don't log at DEBUG in production (too verbose)

Metrics:
âœ“ Use RED method for services (Rate, Errors, Duration)
âœ“ Use USE method for resources (Utilization, Saturation, Errors)
âœ“ Keep labels low-cardinality (<100 unique values)
âœ“ Use histograms for latency (aggregatable percentiles)
âœ“ Scrape metrics every 15-60 seconds (balance freshness vs overhead)
âœ“ Store metrics for 30-90 days (long-term trends)
âœ— Don't use high-cardinality labels (user_id, trace_id)
âœ— Don't over-instrument (too many metrics = noise)

Tracing:
âœ“ Use OpenTelemetry (vendor-neutral standard)
âœ“ Propagate trace context (W3C traceparent header)
âœ“ Sample traces (1-10%, reduce overhead)
âœ“ Use tail-based sampling (capture errors, slow requests)
âœ“ Instrument critical paths (database, external APIs)
âœ“ Add custom attributes (order_id, user_id for context)
âœ— Don't trace 100% (storage explosion)
âœ— Don't add too many spans (overhead, noise)

Alerting:
âœ“ Alert on SLO breach (error budget consumption)
âœ“ Symptom-based alerts (user-facing issues)
âœ“ Actionable alerts (clear remediation steps)
âœ“ Group/batch alerts (reduce noise)
âœ“ Runbooks for common alerts (reduce MTTR)
âœ“ Route by severity (critical â†’ PagerDuty, warning â†’ Slack)
âœ— Don't alert on symptoms without impact (disk 80% but service fine)
âœ— Don't alert on everything (alert fatigue)
âœ— Don't forget to document (runbooks essential)

Dashboards:
âœ“ SLO dashboard (error budget, compliance)
âœ“ Service dashboard (RED metrics per service)
âœ“ Resource dashboard (USE metrics)
âœ“ Business metrics (orders, revenue, users)
âœ“ Update dashboards with deployments (annotate)
âœ— Don't create dashboards without purpose (unused = clutter)
âœ— Don't overcrowd (focus on key metrics)

On-Call:
âœ“ Sustainable rotation (weekly, primary + secondary)
âœ“ Runbooks for common incidents (reduce MTTR)
âœ“ Blameless postmortems (learn, improve)
âœ“ Error budget-driven releases (freeze if budget exhausted)
âœ“ Test alerts (fire alerts during business hours, verify)
âœ“ Escalation path (clear chain of command)
âœ— Don't ignore alert fatigue (tune, suppress, aggregate)
âœ— Don't skip postmortems (learning opportunity)
âœ— Don't blame individuals (focus on systems)
```

Complete observability foundation! ðŸ“ŠðŸ”ðŸš¨