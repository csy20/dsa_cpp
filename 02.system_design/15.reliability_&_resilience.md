# Reliability & Resilience

## What Is Resilience?

**System's ability to handle and recover from failures**

```
Failure is inevitable:
‚îú‚îÄ Network partitions (temporary disconnects)
‚îú‚îÄ Service crashes (bugs, OOM, segfaults)
‚îú‚îÄ Overload (traffic spikes)
‚îú‚îÄ Dependencies down (database, API)
‚îî‚îÄ Hardware failures (disk, memory)

Fragile system (no resilience):
Request ‚Üí Service A ‚Üí Service B (down) ‚Üí ‚úó Fail
‚îî‚îÄ User sees error, request lost

Resilient system:
Request ‚Üí Service A ‚Üí Service B (down) ‚Üí Retry ‚Üí Circuit breaker ‚Üí Fallback
‚îî‚îÄ User gets degraded response (cached data, default value)

Resilience patterns:
1. Timeouts (don't wait forever)
2. Retries (try again on transient failures)
3. Circuit breakers (stop calling failing service)
4. Bulkheads (isolate failures)
5. Graceful degradation (provide reduced functionality)
6. Load shedding (reject requests when overloaded)
```

---

## 1. Timeouts

### 1.1 Why Timeouts?

**Prevent cascading failures from slow services**

```
Without timeout:
Client ‚Üí Server ‚Üí Database (hung, not responding)
         ‚îî‚îÄ Waits forever (thread blocked)
         ‚îî‚îÄ More requests ‚Üí More blocked threads
         ‚îî‚îÄ Server runs out of threads ‚Üí Can't accept new requests
         ‚îî‚îÄ Cascading failure (entire service down)

With timeout:
Client ‚Üí Server ‚Üí Database (hung)
         ‚îî‚îÄ Wait 1 second (timeout)
         ‚îî‚îÄ Cancel request, free thread
         ‚îî‚îÄ Return error to client (fast fail)
         ‚îî‚îÄ Server remains responsive

Timeout types:
1. Connection timeout (time to establish connection)
2. Read timeout (time to receive response after sending)
3. Request timeout (total time for entire request)
```

### 1.2 Timeout Configuration

**Set timeouts at every layer**

```
Request flow with timeouts:

Browser (30s timeout)
  ‚Üí Load Balancer (25s timeout)
    ‚Üí API Server (20s timeout)
      ‚Üí Database (5s timeout)
      ‚Üí External API (10s timeout)

Rule: Parent timeout > Child timeout (with buffer)
‚îú‚îÄ Database timeout: 5s
‚îú‚îÄ API server timeout: 20s (5s DB + 10s external + 5s buffer)
‚îú‚îÄ Load balancer timeout: 25s (20s + 5s buffer)
‚îî‚îÄ Browser timeout: 30s (25s + 5s buffer)

Why buffer? Prevent timeout race conditions
‚îú‚îÄ DB times out at 5.0s
‚îú‚îÄ API cancels at 5.1s (detected timeout)
‚îú‚îÄ API processes response, sends to client (5.2s)
‚îî‚îÄ Load balancer still waiting (25s timeout)
```

**Code Examples:**

```python
import requests
from requests.exceptions import Timeout

# HTTP client timeout
try:
    response = requests.get(
        'http://api.example.com/data',
        timeout=(3.0, 10.0)  # (connect timeout, read timeout)
    )
    # Connect timeout: 3 seconds to establish connection
    # Read timeout: 10 seconds to receive response
except Timeout:
    # Handle timeout
    return {"error": "Service temporarily unavailable"}

# Database timeout
import psycopg2

conn = psycopg2.connect(
    host="localhost",
    database="mydb",
    user="user",
    password="password",
    connect_timeout=3,  # Connection timeout: 3 seconds
    options="-c statement_timeout=5000"  # Query timeout: 5 seconds
)

cursor = conn.cursor()
try:
    cursor.execute("SELECT * FROM large_table")
except psycopg2.extensions.QueryCanceledError:
    # Query exceeded 5 seconds
    return {"error": "Query timeout"}

# Redis timeout
import redis

r = redis.Redis(
    host='localhost',
    port=6379,
    socket_connect_timeout=1,  # Connect timeout
    socket_timeout=2  # Read/write timeout
)

try:
    value = r.get('key')
except redis.TimeoutError:
    # Timeout exceeded
    pass
```

**Context-based Timeouts:**

```python
import asyncio
from contextlib import asynccontextmanager

@asynccontextmanager
async def timeout_context(seconds):
    """Context manager for timeouts"""
    try:
        async with asyncio.timeout(seconds):
            yield
    except asyncio.TimeoutError:
        raise TimeoutError(f"Operation exceeded {seconds}s")

# Usage
async def fetch_data():
    async with timeout_context(5.0):
        # All operations within this block must complete in 5s
        user = await db.fetch_user(user_id)
        orders = await db.fetch_orders(user_id)
        return {"user": user, "orders": orders}

# Go example (context with timeout)
import (
    "context"
    "time"
)

func fetchData(ctx context.Context) error {
    // Create context with 5 second timeout
    ctx, cancel := context.WithTimeout(ctx, 5*time.Second)
    defer cancel()
    
    // Pass context to all operations
    user, err := db.FetchUser(ctx, userID)
    if err != nil {
        return err  // May be context.DeadlineExceeded
    }
    
    orders, err := db.FetchOrders(ctx, userID)
    if err != nil {
        return err
    }
    
    return nil
}
```

---

## 2. Retries

### 2.1 Transient vs Permanent Failures

**Retry transient failures, fail fast on permanent failures**

```
Transient failures (retry):
‚îú‚îÄ Network timeout (temporary network issue)
‚îú‚îÄ Connection refused (service restarting)
‚îú‚îÄ 503 Service Unavailable (temporary overload)
‚îú‚îÄ 429 Too Many Requests (rate limit, backoff)
‚îî‚îÄ Database deadlock (retry usually succeeds)

Permanent failures (don't retry):
‚îú‚îÄ 400 Bad Request (invalid input, won't succeed)
‚îú‚îÄ 401 Unauthorized (missing/invalid credentials)
‚îú‚îÄ 404 Not Found (resource doesn't exist)
‚îú‚îÄ 422 Unprocessable Entity (validation error)
‚îî‚îÄ 5xx Server Error (if known to be permanent)

Idempotency requirement:
‚îú‚îÄ Safe to retry: GET, PUT, DELETE (idempotent)
‚îú‚îÄ Unsafe to retry: POST (may create duplicates)
‚îî‚îÄ Solution: Idempotency keys (detect duplicates)
```

### 2.2 Exponential Backoff

**Increase delay between retries exponentially**

```
Linear backoff (bad):
Retry 1: Wait 1 second
Retry 2: Wait 1 second
Retry 3: Wait 1 second
Problem: Thundering herd (all clients retry simultaneously)

Exponential backoff (better):
Retry 1: Wait 1 second (2^0 = 1)
Retry 2: Wait 2 seconds (2^1 = 2)
Retry 3: Wait 4 seconds (2^2 = 4)
Retry 4: Wait 8 seconds (2^3 = 8)
Retry 5: Wait 16 seconds (2^4 = 16)

Benefits:
+ Gradual backoff (give service time to recover)
+ Reduced load (fewer retries as delay increases)
+ Bounded retries (max attempts or max delay)

Formula:
delay = min(max_delay, base_delay * 2^attempt)

Example:
base_delay = 1s
max_delay = 60s
attempt 0: min(60, 1 * 2^0) = 1s
attempt 1: min(60, 1 * 2^1) = 2s
attempt 2: min(60, 1 * 2^2) = 4s
attempt 5: min(60, 1 * 2^5) = 32s
attempt 10: min(60, 1 * 2^10) = 60s (capped)
```

### 2.3 Jitter

**Add randomness to prevent synchronized retries**

```
Problem: Exponential backoff alone
Service crashes at 10:00:00
‚îú‚îÄ 1000 clients all retry at 10:00:01 (1s delay)
‚îú‚îÄ Service gets 1000 simultaneous requests (overload)
‚îî‚îÄ Service crashes again (thundering herd)

Solution: Jitter (randomness)
Retry 1: Wait 0.5-1.5 seconds (random)
Retry 2: Wait 1.0-3.0 seconds (random)
Retry 3: Wait 2.0-6.0 seconds (random)

Clients retry at different times (spread out load)

Jitter types:

1. Full jitter (AWS recommendation):
   delay = random(0, min(max_delay, base_delay * 2^attempt))
   
   Example (attempt 3):
   max = min(60, 1 * 2^3) = 8s
   delay = random(0, 8) = random 0-8s

2. Equal jitter:
   temp = min(max_delay, base_delay * 2^attempt)
   delay = temp/2 + random(0, temp/2)
   
   Example (attempt 3):
   temp = 8s
   delay = 4s + random(0, 4s) = 4-8s

3. Decorrelated jitter:
   delay = random(base_delay, previous_delay * 3)
   (prevents correlation between retries)

Prefer: Full jitter (best distribution)
```

**Implementation:**

```python
import random
import time
from typing import Callable, Any

def retry_with_backoff(
    func: Callable,
    max_attempts: int = 5,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    jitter: bool = True,
    retryable_exceptions: tuple = (Exception,)
) -> Any:
    """
    Retry function with exponential backoff and jitter
    """
    for attempt in range(max_attempts):
        try:
            return func()
        except retryable_exceptions as e:
            if attempt == max_attempts - 1:
                # Last attempt, re-raise exception
                raise
            
            # Calculate delay with exponential backoff
            delay = min(max_delay, base_delay * (2 ** attempt))
            
            # Add jitter (full jitter)
            if jitter:
                delay = random.uniform(0, delay)
            
            print(f"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.2f}s")
            time.sleep(delay)
    
    raise Exception("Max retries exceeded")

# Usage
def unstable_api_call():
    response = requests.get('http://api.example.com/data', timeout=5)
    response.raise_for_status()
    return response.json()

data = retry_with_backoff(
    unstable_api_call,
    max_attempts=5,
    base_delay=1.0,
    max_delay=60.0,
    retryable_exceptions=(requests.exceptions.RequestException,)
)
```

**Decorator Pattern:**

```python
import functools

def retry(max_attempts=3, base_delay=1.0, max_delay=60.0, jitter=True):
    """Retry decorator with exponential backoff and jitter"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts - 1:
                        raise
                    
                    delay = min(max_delay, base_delay * (2 ** attempt))
                    if jitter:
                        delay = random.uniform(0, delay)
                    
                    print(f"Retry {attempt + 1}/{max_attempts} after {delay:.2f}s")
                    time.sleep(delay)
        
        return wrapper
    return decorator

# Usage
@retry(max_attempts=5, base_delay=1.0)
def fetch_user_data(user_id):
    return requests.get(f'http://api.example.com/users/{user_id}').json()

# Automatic retry on failure
user = fetch_user_data(123)
```

### 2.4 Idempotency

**Safe retries without side effects**

```
Non-idempotent operation (unsafe retry):
POST /orders
{
  "user_id": 123,
  "amount": 99.99
}

Problem: Retry creates duplicate orders
‚îú‚îÄ Request 1: Create order (network timeout, uncertain)
‚îú‚îÄ Retry: Create order again (duplicate!)
‚îî‚îÄ User charged twice

Idempotent operation (safe retry):
POST /orders
{
  "idempotency_key": "order-123-20240101",
  "user_id": 123,
  "amount": 99.99
}

Server checks idempotency key:
1. First request: Create order, store key
2. Retry: Key exists, return original response (no duplicate)

Idempotency key strategies:
‚îú‚îÄ Client-generated: UUID, hash(request body)
‚îú‚îÄ User-provided: order_id, transaction_id
‚îî‚îÄ Time-based: user_id + date (one order per day)
```

**Server-side Implementation:**

```python
from flask import Flask, request, jsonify
import hashlib
import json

app = Flask(__name__)

# In-memory cache (use Redis in production)
processed_requests = {}

@app.route('/orders', methods=['POST'])
def create_order():
    # Extract idempotency key
    idempotency_key = request.headers.get('Idempotency-Key')
    
    if not idempotency_key:
        # Generate from request body (content-based)
        idempotency_key = hashlib.sha256(
            json.dumps(request.json, sort_keys=True).encode()
        ).hexdigest()
    
    # Check if already processed
    if idempotency_key in processed_requests:
        # Return cached response (idempotent)
        return jsonify(processed_requests[idempotency_key])
    
    # Process request
    order_id = create_order_in_database(request.json)
    
    response = {
        "order_id": order_id,
        "status": "created"
    }
    
    # Cache response (TTL: 24 hours)
    processed_requests[idempotency_key] = response
    
    return jsonify(response), 201

# Redis-based implementation (production)
import redis

r = redis.Redis(host='localhost', port=6379)

@app.route('/payments', methods=['POST'])
def process_payment():
    idempotency_key = request.headers.get('Idempotency-Key')
    
    # Check Redis cache
    cached = r.get(f"idempotency:{idempotency_key}")
    if cached:
        return json.loads(cached)
    
    # Process payment
    payment_id = charge_credit_card(request.json)
    
    response = {"payment_id": payment_id, "status": "success"}
    
    # Cache in Redis (TTL: 24 hours)
    r.setex(
        f"idempotency:{idempotency_key}",
        86400,  # 24 hours
        json.dumps(response)
    )
    
    return jsonify(response)
```

---

## 3. Circuit Breakers

### 3.1 Circuit Breaker States

**Stop calling failing services to prevent cascading failures**

```
States:

1. CLOSED (normal operation):
   ‚îú‚îÄ Requests pass through
   ‚îú‚îÄ Track failures
   ‚îî‚îÄ If failure rate > threshold ‚Üí OPEN

2. OPEN (failing, reject requests):
   ‚îú‚îÄ Reject requests immediately (fast fail)
   ‚îú‚îÄ Return cached data or error
   ‚îú‚îÄ After timeout ‚Üí HALF_OPEN

3. HALF_OPEN (testing recovery):
   ‚îú‚îÄ Allow limited requests (test if service recovered)
   ‚îú‚îÄ If success ‚Üí CLOSED (resume normal)
   ‚îî‚îÄ If failure ‚Üí OPEN (still failing)

State transitions:
CLOSED ‚Üí (failure rate > threshold) ‚Üí OPEN
OPEN ‚Üí (timeout expires) ‚Üí HALF_OPEN
HALF_OPEN ‚Üí (success) ‚Üí CLOSED
HALF_OPEN ‚Üí (failure) ‚Üí OPEN

Example flow:
10:00 - CLOSED: 100 requests, 2 failures (2% error rate) ‚úì
10:01 - CLOSED: 100 requests, 15 failures (15% error rate, threshold: 10%) ‚Üí OPEN
10:01-10:06 - OPEN: Reject all requests (fast fail, timeout: 5 minutes)
10:06 - HALF_OPEN: Allow 3 test requests
10:06 - HALF_OPEN: 3 requests succeed ‚Üí CLOSED
10:07 - CLOSED: Resume normal operation
```

**Circuit Breaker Visualization:**

```
CLOSED state:
Request ‚Üí Circuit Breaker (closed) ‚Üí Service ‚Üí Response
          ‚îú‚îÄ Track: 90 success, 10 failures (10%)
          ‚îî‚îÄ Threshold: 20% failures

Request ‚Üí Circuit Breaker (closed) ‚Üí Service (down) ‚Üí Failure
          ‚îú‚îÄ Track: 75 success, 25 failures (25%)
          ‚îî‚îÄ 25% > 20% ‚Üí OPEN

OPEN state:
Request ‚Üí Circuit Breaker (open) ‚Üí ‚úó Fast fail (no call to service)
          Return cached data or error

After 5 minutes ‚Üí HALF_OPEN

HALF_OPEN state:
Request ‚Üí Circuit Breaker (half-open) ‚Üí Service ‚Üí Success
          ‚îî‚îÄ 1/1 success ‚Üí CLOSED

Or:
Request ‚Üí Circuit Breaker (half-open) ‚Üí Service ‚Üí Failure
          ‚îî‚îÄ 0/1 success ‚Üí OPEN (back to open, wait another 5 min)
```

### 3.2 Circuit Breaker Implementation

```python
import time
from enum import Enum
from threading import Lock

class CircuitState(Enum):
    CLOSED = 1
    OPEN = 2
    HALF_OPEN = 3

class CircuitBreaker:
    def __init__(
        self,
        failure_threshold: int = 5,      # Open after 5 failures
        recovery_timeout: float = 60.0,  # Try recovery after 60s
        expected_exception: type = Exception,
        name: str = "CircuitBreaker"
    ):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.expected_exception = expected_exception
        self.name = name
        
        self.state = CircuitState.CLOSED
        self.failure_count = 0
        self.last_failure_time = None
        self.lock = Lock()
    
    def call(self, func, *args, **kwargs):
        with self.lock:
            if self.state == CircuitState.OPEN:
                # Check if recovery timeout elapsed
                if time.time() - self.last_failure_time >= self.recovery_timeout:
                    print(f"{self.name}: OPEN ‚Üí HALF_OPEN (testing recovery)")
                    self.state = CircuitState.HALF_OPEN
                else:
                    # Still open, reject request
                    raise Exception(f"{self.name}: Circuit breaker is OPEN")
        
        try:
            # Attempt request
            result = func(*args, **kwargs)
            
            # Success
            with self.lock:
                if self.state == CircuitState.HALF_OPEN:
                    print(f"{self.name}: HALF_OPEN ‚Üí CLOSED (service recovered)")
                    self.state = CircuitState.CLOSED
                
                self.failure_count = 0  # Reset failures
            
            return result
        
        except self.expected_exception as e:
            # Failure
            with self.lock:
                self.failure_count += 1
                self.last_failure_time = time.time()
                
                if self.state == CircuitState.HALF_OPEN:
                    # Half-open test failed
                    print(f"{self.name}: HALF_OPEN ‚Üí OPEN (service still failing)")
                    self.state = CircuitState.OPEN
                elif self.failure_count >= self.failure_threshold:
                    # Threshold exceeded
                    print(f"{self.name}: CLOSED ‚Üí OPEN (failure threshold exceeded)")
                    self.state = CircuitState.OPEN
            
            raise

# Usage
breaker = CircuitBreaker(
    failure_threshold=3,
    recovery_timeout=30.0,
    expected_exception=requests.exceptions.RequestException,
    name="PaymentService"
)

def call_payment_service(amount):
    return breaker.call(
        requests.post,
        'http://payment-service/charge',
        json={'amount': amount},
        timeout=5
    )

# Decorator version
def circuit_breaker(failure_threshold=5, recovery_timeout=60.0):
    breakers = {}
    
    def decorator(func):
        breaker_name = func.__name__
        breakers[breaker_name] = CircuitBreaker(
            failure_threshold=failure_threshold,
            recovery_timeout=recovery_timeout,
            name=breaker_name
        )
        
        def wrapper(*args, **kwargs):
            return breakers[breaker_name].call(func, *args, **kwargs)
        
        return wrapper
    return decorator

@circuit_breaker(failure_threshold=3, recovery_timeout=30.0)
def fetch_user_profile(user_id):
    response = requests.get(f'http://user-service/users/{user_id}', timeout=5)
    response.raise_for_status()
    return response.json()
```

### 3.3 Circuit Breaker + Fallback

**Provide degraded functionality when circuit is open**

```python
class CircuitBreakerWithFallback:
    def __init__(self, breaker, fallback_func=None):
        self.breaker = breaker
        self.fallback_func = fallback_func
    
    def call(self, func, *args, **kwargs):
        try:
            return self.breaker.call(func, *args, **kwargs)
        except Exception as e:
            if self.fallback_func:
                print(f"Circuit open, using fallback")
                return self.fallback_func(*args, **kwargs)
            raise

# Fallback functions
def get_user_from_cache(user_id):
    """Return cached user data"""
    cached = redis_client.get(f"user:{user_id}")
    if cached:
        return json.loads(cached)
    return {"user_id": user_id, "name": "Unknown", "cached": True}

def get_default_recommendations():
    """Return default recommendations"""
    return ["Popular Item 1", "Popular Item 2", "Popular Item 3"]

# Usage
breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=30.0)
breaker_with_fallback = CircuitBreakerWithFallback(
    breaker,
    fallback_func=get_user_from_cache
)

user = breaker_with_fallback.call(fetch_user_profile, user_id=123)
# If circuit is open, returns cached data instead of failing
```

---

## 4. Bulkheads

### 4.1 Isolation Pattern

**Isolate failures to prevent total system failure**

```
No bulkhead (shared thread pool):
Thread pool (10 threads):
‚îú‚îÄ 5 threads: Slow database queries (stuck)
‚îú‚îÄ 5 threads: Slow external API calls (stuck)
‚îî‚îÄ 0 threads: Available for new requests ‚úó

System completely unresponsive (cascade failure)

With bulkhead (isolated thread pools):
Database pool (5 threads):
‚îú‚îÄ 5 threads: Slow queries (stuck)
‚îî‚îÄ Isolated (doesn't affect other operations)

External API pool (5 threads):
‚îú‚îÄ 5 threads: Slow API calls (stuck)
‚îî‚îÄ Isolated

Fast operations pool (10 threads):
‚îú‚îÄ 10 threads: Available ‚úì
‚îî‚îÄ System still responsive for fast operations

Analogy: Ship bulkheads
‚îú‚îÄ Compartments separated by watertight walls
‚îú‚îÄ Water in one compartment doesn't sink entire ship
‚îî‚îÄ Same for service failures
```

### 4.2 Thread Pool Bulkheads

```python
from concurrent.futures import ThreadPoolExecutor
import threading

class BulkheadExecutor:
    """Isolated thread pools for different operations"""
    
    def __init__(self):
        # Separate pools for different operations
        self.database_pool = ThreadPoolExecutor(max_workers=5, thread_name_prefix="db")
        self.external_api_pool = ThreadPoolExecutor(max_workers=5, thread_name_prefix="api")
        self.fast_ops_pool = ThreadPoolExecutor(max_workers=10, thread_name_prefix="fast")
    
    def execute_database_query(self, func, *args, **kwargs):
        """Execute in database pool (isolated)"""
        return self.database_pool.submit(func, *args, **kwargs)
    
    def execute_external_api_call(self, func, *args, **kwargs):
        """Execute in external API pool (isolated)"""
        return self.external_api_pool.submit(func, *args, **kwargs)
    
    def execute_fast_operation(self, func, *args, **kwargs):
        """Execute in fast operations pool"""
        return self.fast_ops_pool.submit(func, *args, **kwargs)

bulkhead = BulkheadExecutor()

# Database queries use dedicated pool
def slow_database_query():
    # Even if this hangs, only affects 5 threads (database pool)
    return db.execute("SELECT * FROM large_table")

future = bulkhead.execute_database_query(slow_database_query)
result = future.result(timeout=10)

# External API calls use separate pool
def external_api_call():
    return requests.get('http://slow-api.example.com/data', timeout=30)

future = bulkhead.execute_external_api_call(external_api_call)

# Fast operations unaffected by slow database/API
def fast_cache_lookup():
    return redis_client.get('key')

future = bulkhead.execute_fast_operation(fast_cache_lookup)
```

### 4.3 Connection Pool Bulkheads

```python
import psycopg2.pool

class DatabaseBulkheads:
    """Separate connection pools for different query types"""
    
    def __init__(self):
        # OLTP queries (fast, small)
        self.oltp_pool = psycopg2.pool.ThreadedConnectionPool(
            minconn=2,
            maxconn=10,
            host='localhost',
            database='mydb',
            user='user',
            password='password'
        )
        
        # Analytics queries (slow, large)
        self.analytics_pool = psycopg2.pool.ThreadedConnectionPool(
            minconn=1,
            maxconn=3,  # Limit analytics connections
            host='localhost',
            database='mydb',
            user='analytics_user',
            password='password'
        )
        
        # Read replicas
        self.replica_pool = psycopg2.pool.ThreadedConnectionPool(
            minconn=5,
            maxconn=20,
            host='replica.localhost',
            database='mydb',
            user='readonly_user',
            password='password'
        )
    
    def execute_oltp(self, query, params=None):
        """Fast transactional queries"""
        conn = self.oltp_pool.getconn()
        try:
            cursor = conn.cursor()
            cursor.execute(query, params)
            return cursor.fetchall()
        finally:
            self.oltp_pool.putconn(conn)
    
    def execute_analytics(self, query, params=None):
        """Slow analytical queries (isolated)"""
        conn = self.analytics_pool.getconn()
        try:
            cursor = conn.cursor()
            cursor.execute(query, params)
            return cursor.fetchall()
        finally:
            self.analytics_pool.putconn(conn)

db = DatabaseBulkheads()

# OLTP: Fast, won't be blocked by slow analytics
users = db.execute_oltp("SELECT * FROM users WHERE user_id = %s", (123,))

# Analytics: Slow, but isolated (only uses 3 connections max)
report = db.execute_analytics("SELECT * FROM orders GROUP BY region")
```

### 4.4 Semaphore Bulkheads

```python
import asyncio
from asyncio import Semaphore

class AsyncBulkhead:
    """Rate limiting with semaphores"""
    
    def __init__(self):
        # Limit concurrent operations
        self.database_semaphore = Semaphore(10)  # Max 10 concurrent DB queries
        self.api_semaphore = Semaphore(5)        # Max 5 concurrent API calls
    
    async def database_query(self, query):
        async with self.database_semaphore:
            # Only 10 queries can run concurrently
            # 11th query waits until one completes
            return await db.execute(query)
    
    async def external_api_call(self, url):
        async with self.api_semaphore:
            # Only 5 API calls concurrently
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    return await response.json()

bulkhead = AsyncBulkhead()

# 100 concurrent requests
tasks = [bulkhead.database_query("SELECT * FROM users") for _ in range(100)]
results = await asyncio.gather(*tasks)
# Only 10 execute at a time (others queued)
```

---

## 5. Graceful Degradation

### 5.1 Fallback Strategies

**Provide reduced functionality instead of complete failure**

```
Full functionality:
User ‚Üí Personalized recommendations (ML model) ‚Üí Custom list

ML service down (no degradation):
User ‚Üí ‚úó Error: "Recommendations unavailable"

Graceful degradation:
User ‚Üí ML service (down) ‚Üí Fallback: Popular items
     ‚Üí Still functional (reduced quality)

Degradation levels:
1. Primary: ML-based recommendations (personalized)
2. Fallback 1: Cached recommendations (recent)
3. Fallback 2: Category-based recommendations (generic)
4. Fallback 3: Popular items (static)
5. Last resort: Empty list (still functional)
```

**Implementation:**

```python
class RecommendationService:
    def get_recommendations(self, user_id):
        # Try primary (ML-based)
        try:
            return self._get_ml_recommendations(user_id)
        except MLServiceUnavailable:
            print("ML service down, using cached recommendations")
        
        # Fallback 1: Cached recommendations
        try:
            return self._get_cached_recommendations(user_id)
        except CacheError:
            print("Cache unavailable, using category-based")
        
        # Fallback 2: Category-based
        try:
            user_categories = self._get_user_categories(user_id)
            return self._get_category_recommendations(user_categories)
        except DatabaseError:
            print("Database unavailable, using popular items")
        
        # Fallback 3: Popular items (static)
        return self._get_popular_items()
    
    def _get_ml_recommendations(self, user_id):
        response = requests.get(
            f'http://ml-service/recommend/{user_id}',
            timeout=1.0  # Fail fast
        )
        response.raise_for_status()
        return response.json()['items']
    
    def _get_cached_recommendations(self, user_id):
        cached = redis_client.get(f"rec:{user_id}")
        if cached:
            return json.loads(cached)
        raise CacheError("No cached recommendations")
    
    def _get_category_recommendations(self, categories):
        items = []
        for category in categories:
            items.extend(
                db.query("SELECT * FROM items WHERE category = ?", category)
            )
        return items[:10]
    
    def _get_popular_items(self):
        # Static list (always available)
        return [
            "Popular Item 1",
            "Popular Item 2",
            "Popular Item 3"
        ]
```

### 5.2 Feature Flags

**Disable non-critical features during incidents**

```python
class FeatureFlags:
    """Runtime feature toggles"""
    
    def __init__(self):
        # Stored in Redis (dynamic, no deployment)
        self.redis = redis.Redis(host='localhost', port=6379)
    
    def is_enabled(self, feature_name, default=True):
        """Check if feature is enabled"""
        flag = self.redis.get(f"feature:{feature_name}")
        if flag is None:
            return default
        return flag.decode() == 'true'
    
    def enable(self, feature_name):
        self.redis.set(f"feature:{feature_name}", 'true')
    
    def disable(self, feature_name):
        self.redis.set(f"feature:{feature_name}", 'false')

flags = FeatureFlags()

@app.route('/product/<product_id>')
def get_product(product_id):
    product = db.get_product(product_id)
    
    # Optional: Personalized recommendations
    if flags.is_enabled('personalized_recommendations'):
        try:
            product['recommendations'] = get_ml_recommendations(product_id)
        except Exception as e:
            print(f"Recommendations failed: {e}")
            # Continue without recommendations
    
    # Optional: User reviews
    if flags.is_enabled('user_reviews'):
        try:
            product['reviews'] = get_user_reviews(product_id)
        except Exception as e:
            print(f"Reviews failed: {e}")
    
    # Core functionality always works
    return jsonify(product)

# During incident: Disable non-critical features
flags.disable('personalized_recommendations')
flags.disable('user_reviews')
# Reduced load, core functionality preserved
```

---

## 6. Load Shedding

### 6.1 Reject Non-Critical Requests

**Drop requests when overloaded to protect system**

```
Overload scenario:
‚îú‚îÄ System capacity: 1000 req/sec
‚îú‚îÄ Current load: 2000 req/sec
‚îî‚îÄ Without load shedding: System crashes (OOM, CPU 100%)

Load shedding:
‚îú‚îÄ Accept: 1000 req/sec (capacity)
‚îú‚îÄ Reject: 1000 req/sec (503 Service Unavailable)
‚îî‚îÄ System stable, serves 1000 req/sec (better than 0)

Better: Prioritize critical requests
‚îú‚îÄ Accept: Login, checkout (critical)
‚îú‚îÄ Reject: Analytics, recommendations (non-critical)
‚îî‚îÄ Core functionality preserved
```

### 6.2 Priority-Based Load Shedding

```python
from enum import Enum
import time

class RequestPriority(Enum):
    CRITICAL = 1   # Login, payments (never shed)
    HIGH = 2       # Core API (shed only if critical)
    NORMAL = 3     # Regular requests
    LOW = 4        # Analytics, background jobs (shed first)

class LoadShedder:
    def __init__(self, max_concurrent_requests=1000):
        self.max_concurrent = max_concurrent_requests
        self.current_requests = 0
        self.rejected_count = 0
    
    def should_accept(self, priority: RequestPriority) -> bool:
        """Decide whether to accept request based on priority"""
        
        # Always accept critical requests
        if priority == RequestPriority.CRITICAL:
            return True
        
        # Calculate current load percentage
        load = self.current_requests / self.max_concurrent
        
        # Shed based on priority and load
        if load < 0.7:
            # < 70% load: Accept all
            return True
        elif load < 0.85:
            # 70-85% load: Shed low priority
            if priority == RequestPriority.LOW:
                self.rejected_count += 1
                return False
            return True
        elif load < 0.95:
            # 85-95% load: Shed low + normal
            if priority in [RequestPriority.LOW, RequestPriority.NORMAL]:
                self.rejected_count += 1
                return False
            return True
        else:
            # > 95% load: Shed all except critical
            if priority != RequestPriority.CRITICAL:
                self.rejected_count += 1
                return False
            return True
    
    def track_request(self):
        """Increment concurrent request count"""
        self.current_requests += 1
    
    def complete_request(self):
        """Decrement concurrent request count"""
        self.current_requests -= 1

shedder = LoadShedder(max_concurrent_requests=1000)

@app.before_request
def load_shedding():
    # Determine request priority
    if request.path.startswith('/auth/'):
        priority = RequestPriority.CRITICAL
    elif request.path.startswith('/checkout/'):
        priority = RequestPriority.HIGH
    elif request.path.startswith('/analytics/'):
        priority = RequestPriority.LOW
    else:
        priority = RequestPriority.NORMAL
    
    # Check if should accept
    if not shedder.should_accept(priority):
        return jsonify({
            "error": "Service overloaded, try again later"
        }), 503  # Service Unavailable
    
    shedder.track_request()

@app.after_request
def after_request(response):
    shedder.complete_request()
    return response
```

### 6.3 Queue Overflow Policies

**Handle full queues gracefully**

```
Queue overflow policies:

1. Reject new (default):
   ‚îú‚îÄ Queue full ‚Üí Reject new requests
   ‚îú‚îÄ Process existing (FIFO)
   ‚îî‚îÄ Problem: New requests fail immediately

2. Drop oldest:
   ‚îú‚îÄ Queue full ‚Üí Drop oldest request
   ‚îú‚îÄ Add new request
   ‚îî‚îÄ Problem: Long-waiting requests never complete

3. Drop newest:
   ‚îú‚îÄ Queue full ‚Üí Drop newest request
   ‚îú‚îÄ Keep existing (prioritize old)
   ‚îî‚îÄ Problem: Same as reject new

4. Priority queue:
   ‚îú‚îÄ Queue full ‚Üí Drop lowest priority
   ‚îú‚îÄ Add high priority
   ‚îî‚îÄ Better: Preserve critical requests

5. Timeout-based:
   ‚îú‚îÄ Queue full ‚Üí Drop requests older than timeout
   ‚îú‚îÄ Add new request
   ‚îî‚îÄ Best: Remove stale requests
```

**Implementation:**

```python
import queue
import threading
import time
from typing import Any, Callable

class PriorityWorkQueue:
    """Priority queue with overflow policy"""
    
    def __init__(self, max_size=100, workers=4):
        self.queue = queue.PriorityQueue(maxsize=max_size)
        self.max_size = max_size
        self.dropped_count = 0
        
        # Start worker threads
        self.workers = []
        for _ in range(workers):
            worker = threading.Thread(target=self._worker, daemon=True)
            worker.start()
            self.workers.append(worker)
    
    def submit(self, func: Callable, priority: int, timeout: float = 30.0) -> bool:
        """
        Submit task with priority (lower number = higher priority)
        Returns: True if accepted, False if dropped
        """
        task = (priority, time.time(), func)
        
        try:
            # Try to add without blocking
            self.queue.put_nowait(task)
            return True
        except queue.Full:
            # Queue full, apply overflow policy
            return self._handle_overflow(task)
    
    def _handle_overflow(self, new_task):
        """Drop oldest low-priority task, add new high-priority"""
        new_priority, new_timestamp, new_func = new_task
        
        # If new task is low priority, just drop it
        if new_priority >= 3:  # NORMAL or LOW
            self.dropped_count += 1
            return False
        
        # New task is high priority, drop lowest priority from queue
        # (Not directly possible with PriorityQueue, would need custom implementation)
        # For now, reject high-priority when full (simpler)
        self.dropped_count += 1
        return False
    
    def _worker(self):
        """Worker thread processes tasks"""
        while True:
            try:
                priority, timestamp, func = self.queue.get(timeout=1)
                
                # Check if task too old (timeout)
                if time.time() - timestamp > 30:
                    print(f"Task timed out (waited {time.time() - timestamp:.1f}s)")
                    continue
                
                # Execute task
                func()
                
                self.queue.task_done()
            except queue.Empty:
                continue
            except Exception as e:
                print(f"Task failed: {e}")

# Usage
work_queue = PriorityWorkQueue(max_size=100, workers=4)

def process_critical_request():
    print("Processing critical request")
    time.sleep(0.1)

def process_normal_request():
    print("Processing normal request")
    time.sleep(0.5)

# Submit tasks with priority
work_queue.submit(process_critical_request, priority=1)  # High priority
work_queue.submit(process_normal_request, priority=3)    # Normal priority

# When queue full, normal requests dropped first
```

### 6.4 Adaptive Load Shedding

**Dynamically adjust shedding based on system health**

```python
import psutil

class AdaptiveLoadShedder:
    """Adjust load shedding based on resource utilization"""
    
    def __init__(self):
        self.cpu_threshold = 0.8     # 80% CPU
        self.memory_threshold = 0.85  # 85% memory
        self.latency_threshold = 1.0  # 1 second p95
        
        self.current_latency_p95 = 0
    
    def should_shed(self) -> float:
        """
        Returns: Probability of shedding (0.0 - 1.0)
        0.0 = No shedding
        1.0 = Shed all non-critical
        """
        # Check CPU
        cpu_usage = psutil.cpu_percent(interval=0.1) / 100
        cpu_pressure = max(0, (cpu_usage - self.cpu_threshold) / (1 - self.cpu_threshold))
        
        # Check memory
        memory_usage = psutil.virtual_memory().percent / 100
        memory_pressure = max(0, (memory_usage - self.memory_threshold) / (1 - self.memory_threshold))
        
        # Check latency
        latency_pressure = max(0, (self.current_latency_p95 - self.latency_threshold) / self.latency_threshold)
        
        # Overall pressure (worst of all)
        pressure = max(cpu_pressure, memory_pressure, latency_pressure)
        
        return min(1.0, pressure)
    
    def update_latency(self, latency_p95):
        """Update current p95 latency"""
        self.current_latency_p95 = latency_p95

shedder = AdaptiveLoadShedder()

@app.before_request
def adaptive_shedding():
    # Get shedding probability
    shed_probability = shedder.should_shed()
    
    # Determine if should shed this request
    if random.random() < shed_probability:
        # Shed request
        return jsonify({
            "error": "Service overloaded",
            "retry_after": 5  # Seconds
        }), 503
    
    # Accept request
    g.request_start = time.time()

@app.after_request
def update_metrics(response):
    # Track latency
    latency = time.time() - g.request_start
    # Update p95 (simplified, use proper percentile tracking in production)
    shedder.update_latency(latency)
    
    return response
```

---

## Best Practices Summary

```
Timeouts:
‚úì Set timeouts at every layer (database, HTTP, RPC)
‚úì Parent timeout > child timeout (with buffer)
‚úì Fail fast (don't wait forever)
‚úì Use context/cancellation for propagation
‚úì Monitor timeout frequency (tune values)
‚úó Don't use infinite timeouts (blocks threads)
‚úó Don't set same timeout everywhere (hierarchy needed)

Retries:
‚úì Retry transient failures only (5xx, timeout, connection)
‚úì Don't retry permanent failures (4xx, validation errors)
‚úì Use exponential backoff (1s, 2s, 4s, 8s, ...)
‚úì Add jitter (randomness, prevent thundering herd)
‚úì Implement idempotency (safe retries, no duplicates)
‚úì Limit max retries (3-5 attempts typical)
‚úó Don't retry without backoff (hammers failing service)
‚úó Don't retry non-idempotent operations (POST without key)
‚úó Don't retry forever (eventual failure acceptable)

Circuit Breakers:
‚úì Use for external dependencies (APIs, databases)
‚úì Set appropriate thresholds (10-20% error rate)
‚úì Implement fallbacks (cached data, defaults)
‚úì Monitor state transitions (alert on open circuit)
‚úì Test recovery (half-open state)
‚úó Don't use for internal calls (prefer retries)
‚úó Don't set threshold too low (flapping)
‚úó Don't forget fallback strategy (fail gracefully)

Bulkheads:
‚úì Isolate resource pools (database, API, fast ops)
‚úì Prevent cascade failures (one failure doesn't sink all)
‚úì Size pools appropriately (balance isolation vs utilization)
‚úì Monitor pool exhaustion (alert when full)
‚úì Use semaphores for async operations
‚úó Don't over-isolate (too many pools = waste)
‚úó Don't share pools for critical/non-critical (prioritize)

Graceful Degradation:
‚úì Implement fallback strategies (primary ‚Üí cache ‚Üí default)
‚úì Use feature flags (disable non-critical features)
‚úì Prioritize critical functionality (login, checkout)
‚úì Cache aggressively (serve stale if necessary)
‚úì Communicate degradation (status page, headers)
‚úó Don't fail completely (reduced > nothing)
‚úó Don't hide degradation (users should know)

Load Shedding:
‚úì Shed non-critical requests first (analytics, background)
‚úì Monitor system resources (CPU, memory, latency)
‚úì Adaptive shedding (dynamic based on load)
‚úì Return 503 Service Unavailable (not 500)
‚úì Include Retry-After header (help clients backoff)
‚úì Priority-based (preserve critical requests)
‚úó Don't shed critical requests (login, payments)
‚úó Don't shed blindly (prioritize based on criticality)
‚úó Don't ignore queue overflow (implement policy)
```

Complete reliability and resilience foundation! üõ°Ô∏èüí™üîß