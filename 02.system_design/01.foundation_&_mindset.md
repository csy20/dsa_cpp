# Foundation & Mindset

## 1. Functional vs Non-Functional Requirements

### Functional Requirements (FRs)
**What the system DOES** - the business logic and features

**Examples:**
- Users can post tweets (Twitter)
- Users can upload and watch videos (YouTube)
- Users can search for products (Amazon)
- System should send notifications when mentioned
- Users can like/comment on posts

**Characteristics:**
- Defines system behavior
- Directly visible to users
- Easy to test (works or doesn't work)
- Usually captured in user stories

### Non-Functional Requirements (NFRs)
**How WELL the system does it** - quality attributes

**Key Categories:**

#### Performance
- **Latency**: Response time (e.g., API responds in < 100ms)
- **Throughput**: Requests/second the system can handle

#### Scalability
- Handle 100M daily active users
- Support 10K writes/sec
- Store 10 PB of data

#### Availability
- System uptime (99.9%, 99.99%, etc.)
- Disaster recovery time

#### Reliability
- Mean Time Between Failures (MTBF)
- Data durability (no data loss)

#### Consistency
- Strong vs eventual consistency
- Data synchronization across regions

#### Security
- Authentication & Authorization
- Encryption (at rest, in transit)
- Compliance (GDPR, HIPAA)

#### Maintainability
- Code quality, testability
- Monitoring, debugging capability

**Trade-off Example:**
- **Strong consistency** (all nodes see same data) vs **High availability** (system always responsive)
- Can't have both in network partition â†’ CAP theorem

---

## 2. SLA / SLO / SLI

### SLI (Service Level Indicator)
**Quantitative measure** of service performance

**Examples:**
- Request latency: 95th percentile response time
- Error rate: % of requests returning 5xx
- Availability: % of successful probes
- Throughput: Requests per second
- Durability: % of data retained

```
SLI = (Good Events / Total Events) Ã— 100%

Example:
- Total requests: 10,000
- Successful: 9,950
- SLI = 99.5% success rate
```

### SLO (Service Level Objective)
**Target value** or range for an SLI

**Examples:**
- 95th percentile latency < 200ms
- 99.9% of requests succeed
- 99.95% uptime per month
- Search results returned in < 500ms for 99% of queries

**Format:** `SLO = SLI â‰¥ Target`

**Error Budget:**
```
Error Budget = 100% - SLO

If SLO = 99.9%:
- Error budget = 0.1%
- Per month (30 days): 43.2 minutes of downtime allowed
- Per year: 8.76 hours allowed

If SLO = 99.99%:
- Error budget = 0.01%
- Per month: 4.32 minutes
- Per year: 52.56 minutes
```

### SLA (Service Level Agreement)
**Contract** with consequences if SLOs aren't met

**Components:**
1. **Service commitment**: What's promised (based on SLOs)
2. **Consequences**: Refunds, credits, penalties
3. **Exclusions**: Planned maintenance, user errors

**Example (AWS S3):**
- **SLA**: 99.9% uptime commitment
- **Penalty**: 
  - 99.0-99.9% â†’ 10% service credit
  - 95.0-99.0% â†’ 25% service credit
  - < 95.0% â†’ 100% service credit

**Relationship:**
```
SLI (measurement) â†’ SLO (internal goal) â†’ SLA (customer promise)

Example:
SLI: Actual uptime measured = 99.95%
SLO: Internal target = 99.95% (stricter than SLA)
SLA: Customer promise = 99.9% (buffer for error budget)
```

**Why SLO > SLA?**
- SLA violations cost money/reputation
- SLO gives internal buffer
- Allows catching issues before SLA breach

---

## 3. Constraints & Trade-offs

### Common Constraints

#### Technical Constraints
- Budget: Limited servers, bandwidth
- Latency: Speed of light (300ms RTT across globe)
- Storage: Cost per GB
- Network: Bandwidth limits

#### Business Constraints
- Time to market: 3 months to launch
- Team size: 5 engineers
- Regulatory: Must store EU data in EU (GDPR)

#### Physical Constraints
- CAP theorem: Can't have all three (Consistency, Availability, Partition tolerance)
- Disk I/O limits
- Memory constraints

### Fundamental Trade-offs

#### 1. **CAP Theorem**
Choose 2 of 3:
- **Consistency**: All nodes see same data
- **Availability**: Every request gets response
- **Partition tolerance**: Works despite network failures

```
Partition happens:
â”œâ”€ Choose CP: MongoDB, HBase, Redis
â”‚   â†’ Reject requests to maintain consistency
â”‚
â””â”€ Choose AP: Cassandra, DynamoDB, Riak
    â†’ Serve possibly stale data to stay available

(CA not realistic - networks always partition)
```

#### 2. **Consistency vs Performance**
- **Strong consistency**: Slower (coordination overhead)
  - Banking: Account balance must be exact
- **Eventual consistency**: Faster (no coordination)
  - Social media: OK if likes update in few seconds

#### 3. **Normalization vs Denormalization**
- **Normalized**: 
  - Pros: No redundancy, easier updates
  - Cons: Complex queries, JOINs slow at scale
- **Denormalized**: 
  - Pros: Fast reads, simple queries
  - Cons: Data duplication, update complexity

#### 4. **Horizontal vs Vertical Scaling**
- **Vertical (Scale-up)**: Bigger machine
  - Pros: Simple, no code changes
  - Cons: Hardware limits, expensive, single point of failure
- **Horizontal (Scale-out)**: More machines
  - Pros: No limits, cost-effective, resilient
  - Cons: Complex coordination, eventual consistency

#### 5. **Latency vs Throughput**
- Can't optimize both simultaneously
- Batching: Increases throughput, adds latency
- Real-time: Low latency, lower throughput

#### 6. **Build vs Buy**
- **Build**: Custom solution, full control, high cost
- **Buy**: Fast deployment, vendor lock-in, limited customization

---

## 4. Scale Targets

### Calculating Scale

#### Example: Twitter-like System

**User Metrics:**
```
Daily Active Users (DAU): 200M
Each user posts: 2 tweets/day
Each user reads: 50 tweets/day

Write ops: 200M Ã— 2 = 400M tweets/day
           = 400M / 86400 = ~4,630 tweets/sec
           = ~5K writes/sec (peak: 15K writes/sec, 3Ã— average)

Read ops: 200M Ã— 50 = 10B tweets/day
         = 10B / 86400 = ~115K reads/sec
         = (peak: 345K reads/sec)

Read:Write ratio = 100:1 (read-heavy)
```

#### Storage Calculation
```
Average tweet size: 280 chars Ã— 2 bytes = 560 bytes
+ metadata (user_id, timestamp, likes): 200 bytes
+ media URL: 100 bytes
Total: ~1 KB per tweet

Daily storage: 400M tweets Ã— 1 KB = 400 GB/day
Yearly storage: 400 GB Ã— 365 = 146 TB/year
5-year storage: 730 TB â‰ˆ 1 PB

With replication (3Ã—): 3 PB
```

#### Bandwidth Calculation
```
Upload: 4,630 tweets/sec Ã— 1 KB = 4.6 MB/sec
Download: 115K tweets/sec Ã— 1 KB = 115 MB/sec = 920 Mbps

Add media (images, videos):
- 10% of tweets have images (avg 200 KB)
- Upload: 463 tweets/sec Ã— 200 KB = 92.6 MB/sec
- Download: 11.5K Ã— 200 KB = 2.3 GB/sec

Total bandwidth: ~20 Gbps
```

### Key Metrics to Calculate

1. **QPS** (Queries Per Second): Read + Write
2. **Storage**: Size Ã— Volume Ã— Retention Ã— Replication
3. **Bandwidth**: Data Ã— Frequency
4. **Memory (Cache)**: Hot data Ã— Hit ratio
5. **Servers**: Load / (Capacity per server)

---

## 5. Bottlenecks

### Common Bottlenecks

#### 1. **Database Bottleneck**
**Symptoms:**
- Slow query performance
- High CPU on DB server
- Connection pool exhaustion

**Solutions:**
- Indexing
- Read replicas
- Caching (Redis, Memcached)
- Sharding/Partitioning
- Query optimization

#### 2. **Network Bottleneck**
**Symptoms:**
- High latency
- Packet loss
- Bandwidth saturation

**Solutions:**
- CDN for static content
- Compression (gzip, Brotli)
- Protocol optimization (HTTP/2, gRPC)
- Data center closer to users

#### 3. **CPU Bottleneck**
**Symptoms:**
- High CPU utilization
- Request queuing
- Increased response time

**Solutions:**
- Horizontal scaling
- Optimize algorithms (O(nÂ²) â†’ O(n log n))
- Asynchronous processing
- Load balancing

#### 4. **Memory Bottleneck**
**Symptoms:**
- Out of Memory errors
- Excessive swapping
- Garbage collection pauses

**Solutions:**
- Increase RAM
- Memory profiling (find leaks)
- Use streaming for large data
- Pagination

#### 5. **I/O Bottleneck**
**Symptoms:**
- Disk queue length high
- Slow read/write operations

**Solutions:**
- SSD instead of HDD
- Optimize file access patterns
- Asynchronous I/O
- In-memory data structures

### Identifying Bottlenecks

**Little's Law:**
```
L = Î» Ã— W

L = Average number of requests in system
Î» = Average arrival rate (req/sec)
W = Average time spent in system (sec)

Example:
- 1000 req/sec arrive
- Each request takes 2 sec
- System has 2000 concurrent requests

If response time increases â†’ bottleneck!
```

---

## 6. Amdahl's Law

### The Formula
**Limits of parallelization**

```
Speedup = 1 / ((1 - P) + (P / N))

P = Portion of program that can be parallelized (0 to 1)
N = Number of processors
(1 - P) = Serial portion (can't be parallelized)
```

### Example 1: 95% Parallelizable
```
Task: 95% can be parallel, 5% must be serial

With 1 processor:  Time = 100 units
With 2 processors: Speedup = 1 / (0.05 + 0.95/2) = 1.90Ã—
With 4 processors: Speedup = 1 / (0.05 + 0.95/4) = 3.48Ã—
With 8 processors: Speedup = 1 / (0.05 + 0.95/8) = 5.93Ã—
With âˆž processors: Speedup = 1 / 0.05 = 20Ã— (MAX)
```

**Key Insight:** Even if 95% is parallel, max speedup is only 20Ã—!

### Example 2: Database Queries
```
Query pipeline:
1. Parse SQL (5% - serial)
2. Fetch data (90% - parallelizable)
3. Aggregate results (5% - serial)

With 100 servers:
Speedup = 1 / (0.10 + 0.90/100) = 9.17Ã—
(NOT 100Ã—!)
```

### Practical Implications

#### System Design:
```
Microservices:
â”œâ”€ Each service can scale independently
â”œâ”€ BUT: Synchronous calls create serial bottleneck
â””â”€ Solution: Async messaging, event-driven

Example:
- Order service calls: Payment â†’ Inventory â†’ Shipping
- Serial chain: Limited by slowest service
- Async: Publish OrderCreated event, services process independently
```

#### Diminishing Returns:
```
Adding servers:
  2 servers: 1.9Ã— faster âœ“ Cost-effective
  4 servers: 3.5Ã— faster âœ“ Still good
 16 servers: 10Ã— faster  ~ Marginal gains
100 servers: 18Ã— faster  âœ— Expensive, little gain

Focus on reducing serial portion first!
```

### How to Improve

1. **Reduce serial portion** (bigger impact than adding processors)
   - Remove synchronous dependencies
   - Use eventual consistency
   - Parallel algorithms

2. **Identify serial bottlenecks**
   - Distributed tracing
   - Critical path analysis

3. **Design for parallelism**
   - Stateless services
   - Idempotent operations
   - Partitionable data

---

## 7. Fallacies of Distributed Computing

**8 False Assumptions** developers make (Peter Deutsch & James Gosling, Sun Microsystems)

### 1. The Network is Reliable
**Fallacy:** Packets always arrive

**Reality:**
- Network partitions happen
- Routers fail, cables cut
- DNS failures, packet loss

**Solutions:**
- Retry logic with exponential backoff
- Timeouts on all network calls
- Circuit breakers
- Idempotent operations (safe to retry)

```cpp
// Don't do this
response = httpClient.get(url);

// Do this
int maxRetries = 3;
for (int i = 0; i < maxRetries; i++) {
    try {
        response = httpClient.get(url, timeout=5s);
        break;
    } catch (NetworkException e) {
        if (i == maxRetries - 1) throw;
        sleep(pow(2, i) * 1000); // Exponential backoff
    }
}
```

### 2. Latency is Zero
**Fallacy:** Network calls are instant

**Reality:**
```
Within datacenter: 0.5-1ms
Cross-region (US East to West): 60-80ms
Cross-continent (US to Europe): 100-150ms
Cross-globe (US to Australia): 200-300ms

Speed of light limit: ~300ms around Earth
```

**Solutions:**
- Minimize network hops
- Cache aggressively
- Asynchronous communication
- Data locality (geo-distribution)
- Batch requests

```python
# Bad: N+1 query problem
for user_id in user_ids:  # 1000 users
    user = db.get_user(user_id)  # 1000 network calls!
    # 1000 calls Ã— 1ms = 1 second

# Good: Batch
users = db.get_users(user_ids)  # 1 network call
# 1ms total
```

### 3. Bandwidth is Infinite
**Fallacy:** Can send unlimited data

**Reality:**
- 1 Gbps link: ~125 MB/sec max
- Shared with other apps
- Costs money (cloud egress fees)

**Solutions:**
- Compression (gzip, protobuf)
- Send only necessary data (pagination)
- GraphQL (client specifies fields)
- CDN for static assets

```json
// Bad: Return everything
{
  "user": {
    "id": 123,
    "name": "Alice",
    "email": "alice@example.com",
    "avatar_url": "...",
    "bio": "...",
    "full_profile": { /* 50 KB of data */ }
  }
}

// Good: Return only needed fields
{
  "user": {
    "id": 123,
    "name": "Alice"
  }
}
```

### 4. The Network is Secure
**Fallacy:** Data can't be intercepted

**Reality:**
- Man-in-the-middle attacks
- Packet sniffing
- DDoS attacks

**Solutions:**
- TLS/SSL encryption (HTTPS)
- Authentication tokens (JWT, OAuth)
- API keys, rate limiting
- Zero-trust architecture
- Network segmentation, firewalls

```
HTTP  â†’ HTTPS (TLS)
TCP   â†’ mTLS (mutual authentication)
VPN   â†’ Encrypt all traffic
```

### 5. Topology Doesn't Change
**Fallacy:** Network structure is static

**Reality:**
- Servers added/removed (auto-scaling)
- Load balancers route differently
- DNS changes
- Containers/pods restart frequently

**Solutions:**
- Service discovery (Consul, etcd, ZooKeeper)
- Load balancers (dynamic routing)
- Health checks
- Circuit breakers
- Don't hardcode IPs/hostnames

```yaml
# Bad: Hardcoded
database_host = "192.168.1.100"

# Good: Service discovery
database_host = service_discovery.get("postgres-primary")
```

### 6. There is One Administrator
**Fallacy:** One person controls everything

**Reality:**
- Multiple teams, multiple organizations
- Third-party services (AWS, payment gateway)
- Microservices owned by different teams

**Solutions:**
- Service Level Agreements (SLAs)
- Monitoring & alerting
- Graceful degradation
- Fallback mechanisms

```
Your app depends on:
â”œâ”€ Your database (your team)
â”œâ”€ Authentication service (another team)
â”œâ”€ Payment gateway (Stripe)
â”œâ”€ Email service (SendGrid)
â””â”€ Cloud provider (AWS)

Any can fail independently!
```

### 7. Transport Cost is Zero
**Fallacy:** Sending data is free

**Reality:**
- Serialization/deserialization CPU cost
- Network bandwidth costs
- Latency (time = money)

**Solutions:**
- Efficient protocols (gRPC, MessagePack)
- Binary formats over JSON
- Reduce chatty APIs
- Batch operations

```
JSON serialization: ~1 Âµs per small object
Protobuf: ~0.1 Âµs (10Ã— faster)

1M requests/sec:
- JSON: 1 sec CPU time
- Protobuf: 0.1 sec CPU time
```

### 8. The Network is Homogeneous
**Fallacy:** All parts use same technology

**Reality:**
- Mix of old and new systems
- Different languages, protocols
- Various OS, hardware

**Solutions:**
- Standard protocols (HTTP, gRPC)
- API versioning
- Backward compatibility
- Service mesh (standardize communication)

```
System might have:
â”œâ”€ Java microservices
â”œâ”€ Python ML service
â”œâ”€ Legacy .NET monolith
â”œâ”€ Third-party REST APIs
â””â”€ Internal gRPC services

Need interoperability layer!
```

---

## Summary Table

### Quick Reference

| Concept | Key Takeaway |
|---------|-------------|
| **Functional Reqs** | What the system does (features) |
| **Non-Functional Reqs** | How well it does it (performance, scale) |
| **SLI** | Measurement (99.5% success rate) |
| **SLO** | Internal goal (99.9% uptime) |
| **SLA** | Customer promise + penalties (99% uptime) |
| **CAP Theorem** | Choose 2: Consistency, Availability, Partition tolerance |
| **Amdahl's Law** | Speedup limited by serial portion |
| **Bottlenecks** | DB, network, CPU, memory, I/O |
| **Fallacies** | Network fails, has latency, costs money, changes |

### Design Mindset Checklist

When designing a system:

- [ ] What are the functional requirements?
- [ ] What are the non-functional requirements (scale, latency, availability)?
- [ ] What are my SLOs? What's my error budget?
- [ ] What are the constraints (budget, time, technical)?
- [ ] What trade-offs am I making? (consistency vs availability, latency vs throughput)
- [ ] Where are the bottlenecks? (DB, network, CPU, memory)
- [ ] Can this be parallelized? (Amdahl's Law)
- [ ] How do I handle network failures? (Fallacies)
- [ ] How do I monitor and measure? (SLIs, alerts)

---

## Real-World Example: Design a URL Shortener

### 1. Requirements
**Functional:**
- Shorten long URLs
- Redirect short URL to original
- (Optional) Custom aliases, analytics

**Non-Functional:**
- 100M new URLs per month
- 10:1 read:write ratio
- Low latency (< 100ms)
- 99.9% availability

### 2. Scale Calculation
```
Writes: 100M/month = ~40 URLs/sec (peak: 200/sec)
Reads: 400 URLs/sec (peak: 2K/sec)
Storage: 100M Ã— 500 bytes = 50 GB/month = 600 GB/year
```

### 3. Constraints & Trade-offs
- **Consistency vs Availability**: Choose AP (eventual consistency OK)
- **Storage**: URL + metadata, needs fast reads â†’ Cache + DB
- **URL generation**: Base62 encoding vs random vs hash

### 4. Bottlenecks
- **Database reads**: Cache with Redis (99% hit rate)
- **URL generation**: Pre-generate IDs, use counter service

### 5. Handling Fallacies
- **Network unreliable**: Retry on redirect
- **Latency**: Cache at edge (CDN)
- **Topology changes**: Use DNS, load balancer

### 6. SLOs
- P99 latency < 100ms
- 99.9% uptime
- 99.99% data durability

Perfect foundation for system design! ðŸš€