# Distributed System Primitives

## What Are Distributed System Primitives?

**Building blocks for coordinating distributed systems**

```
Challenges in distributed systems:
â”œâ”€ Time synchronization (clocks drift)
â”œâ”€ Unique ID generation (no central authority)
â”œâ”€ Leader election (who's in charge?)
â”œâ”€ Consensus (agree on shared state)
â””â”€ Failure detection (who's alive?)

Primitives solve these fundamental problems
```

---

## 1. Time & Clocks

### 1.1 The Problem: Clock Synchronization

**Clocks drift in distributed systems**

```
Problem:
Server A: 10:00:00.000
Server B: 10:00:00.500  (500ms ahead)
Server C: 09:59:59.800  (200ms behind)

Event ordering:
Server A writes: x=1 at 10:00:00.100
Server B writes: x=2 at 10:00:00.200 (but B's clock is 500ms ahead!)

Question: Which write happened first?
A's timestamp: 10:00:00.100
B's timestamp: 10:00:00.200

Looks like B is later, but B's clock is fast!
Actual time: A at 10:00:00.100, B at 09:59:59.700 (A is actually later!)

Result: Wrong ordering! âŒ
```

**Clock Drift:**
```
Typical drift: 17 seconds/day per server
â”œâ”€ Quartz crystal oscillator imperfections
â”œâ”€ Temperature changes
â””â”€ Hardware aging

1000 servers:
â”œâ”€ After 1 day: Drift up to Â±17 seconds
â”œâ”€ After 1 week: Drift up to Â±119 seconds
â””â”€ Need synchronization!
```

### 1.2 Physical Clocks (NTP - Network Time Protocol)

**Synchronize clocks with time servers**

```
NTP Hierarchy:

Stratum 0: Atomic clock, GPS (reference time)
    â†“
Stratum 1: NTP servers directly connected to Stratum 0
    â†“
Stratum 2: NTP servers sync from Stratum 1
    â†“
Stratum 3: Your servers sync from Stratum 2

Example:
Your server â†’ pool.ntp.org (Stratum 2)
           â†’ 0.pool.ntp.org, 1.pool.ntp.org, 2.pool.ntp.org
```

**How NTP Works:**

```
Client â†” NTP Server synchronization:

1. Client sends request at time t0 (client clock)
2. Server receives at time t1 (server clock)
3. Server replies at time t2 (server clock)
4. Client receives at time t3 (client clock)

Timeline:
Client          Server
  t0 â”€â”€â”€â”€â”€â”€â”€â”€â†’
             t1 (receive)
             t2 (reply)
  t3 â†â”€â”€â”€â”€â”€â”€â”€â”€

Calculations:
Round-trip delay = (t3 - t0) - (t2 - t1)
Network delay = round-trip / 2
Offset = ((t1 - t0) + (t2 - t3)) / 2

Example:
t0 = 10:00:00.000 (client sends)
t1 = 10:00:00.100 (server receives) - Server is 100ms ahead
t2 = 10:00:00.105 (server replies) - 5ms processing
t3 = 10:00:00.200 (client receives)

Round-trip = (200 - 0) - (105 - 100) = 195ms
Offset = ((100 - 0) + (105 - 200)) / 2 = 2.5ms
Client adjusts clock: +2.5ms
```

**NTP Accuracy:**
```
Internet: Â±10-100ms accuracy
LAN: Â±1-10ms accuracy
GPS-based: Â±1Î¼s accuracy

Limitations:
â”œâ”€ Network jitter (variable latency)
â”œâ”€ Asymmetric paths (send â‰  receive latency)
â””â”€ Can't guarantee perfect sync

Good enough for: Logging, monitoring, TTL
Not good for: Strict ordering (use logical clocks)
```

**Clock Adjustments:**
```
Small drift (<128ms):
â”œâ”€ Gradual adjustment (slew)
â”œâ”€ Speed up or slow down clock slightly
â””â”€ Example: +10ms over 10 seconds = +1ms/sec

Large drift (>128ms):
â”œâ”€ Step adjustment (jump)
â”œâ”€ Immediately set to correct time
â””â”€ Risk: Time goes backward! (breaks monotonicity)

Leap smearing (Google):
â”œâ”€ Spread leap second over 24 hours
â”œâ”€ Avoid sudden jumps
â””â”€ Gradual adjustment
```

### 1.3 Logical Clocks (Lamport Timestamps)

**Order events without physical time**

```
Problem: Physical clocks unreliable, but we need ordering

Solution: Logical clock (counter)
â”œâ”€ Each process has counter: L
â”œâ”€ Increment on each event: L++
â””â”€ Send counter with messages
```

**Lamport Clock Algorithm:**

```
Rules:
1. Before event: L = L + 1
2. Send message: Include current L in message
3. Receive message: L = max(L, message.L) + 1

Example:

Process A:        Process B:        Process C:
L=0               L=0               L=0
                                    
L=1: event a      L=1: event b      
L=2: event c      
L=3: send(m1) â”€â”€â†’ L=4: recv(m1)     
                  (max(1, 3) + 1)
                  L=5: event d
                  L=6: send(m2) â”€â”€â†’ L=7: recv(m2)
                                    (max(0, 6) + 1)
L=4: send(m3) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’   L=8: recv(m3)
                                    (max(7, 4) + 1)

Ordering:
event a (A:1) < event b (B:1) â† Can't determine (concurrent)
event c (A:2) < send m1 (A:3) â† A's local order
recv m1 (B:4) happens-after send m1 (A:3) â† Causally related
```

**Happens-Before Relationship:**
```
a â†’ b (a happens-before b) if:
1. Same process: a before b locally
2. Message: a = send, b = receive
3. Transitive: a â†’ b and b â†’ c, then a â†’ c

Lamport guarantee:
If a â†’ b, then L(a) < L(b)

BUT: L(a) < L(b) doesn't mean a â†’ b (may be concurrent)
```

**Limitations:**
```
Concurrent events:
Process A: event a (L=5)
Process B: event b (L=5)

Are they concurrent or ordered?
Lamport clocks can't tell (both have L=5)

Solution: Vector clocks (more complex)
```

### 1.4 Vector Clocks

**Track causality precisely**

```
Each process maintains vector: V[N] (N = number of processes)
V[i] = number of events at process i

Example (3 processes):
Process A: V = [0, 0, 0]
Process B: V = [0, 0, 0]
Process C: V = [0, 0, 0]

Rules:
1. Before event: V[i]++ (increment own counter)
2. Send message: Include current V
3. Receive message: V[j] = max(V[j], message.V[j]) for all j, then V[i]++

Timeline:
Process A:              Process B:              Process C:
V=[0,0,0]              V=[0,0,0]              V=[0,0,0]
                                              
V=[1,0,0]: event a     V=[0,1,0]: event b     
V=[2,0,0]: send(m1) â”€â†’ V=[2,2,0]: recv(m1)   
                       (max([0,1,0], [2,0,0]) + self)
                       V=[2,3,0]: send(m2) â”€â”€â†’ V=[2,3,1]: recv(m2)
V=[3,0,0]: event c     

Comparison (happens-before):
V1 â‰¤ V2 if V1[i] â‰¤ V2[i] for all i

event a: [1,0,0]
event b: [0,1,0]
Neither [1,0,0] â‰¤ [0,1,0] nor [0,1,0] â‰¤ [1,0,0]
â†’ Concurrent! (can't determine order)

event a: [1,0,0]
recv(m1): [2,2,0]
[1,0,0] â‰¤ [2,2,0] â†’ event a happens-before recv(m1) âœ“
```

**Use Cases:**
```
Lamport clocks:
âœ“ Simple ordering
âœ“ Low overhead (single counter)
âœ— Can't detect concurrency

Vector clocks:
âœ“ Precise causality
âœ“ Detect concurrent events
âœ— Higher overhead (N counters, N = processes)

Applications:
â”œâ”€ Lamport: Distributed logs, event ordering
â””â”€ Vector: Conflict detection (Dynamo, Riak), version control
```

### 1.5 Hybrid Logical Clocks (HLC)

**Combine physical and logical clocks**

```
Problem:
Physical clocks: Unreliable, but human-readable
Logical clocks: Reliable ordering, but no wall-clock time

HLC: Best of both worlds

Structure: (pt, l)
â”œâ”€ pt: Physical time (wall clock)
â””â”€ l: Logical counter (for same physical time)

Algorithm:
1. Event happens:
   pt' = max(pt, physical_clock)
   if pt' == pt:
       l' = l + 1  (same physical time, increment logical)
   else:
       l' = 0      (new physical time, reset logical)

2. Send message: Include (pt, l)

3. Receive message with (pt_m, l_m):
   pt' = max(pt, pt_m, physical_clock)
   if pt' == pt == pt_m:
       l' = max(l, l_m) + 1
   elif pt' == pt:
       l' = l + 1
   elif pt' == pt_m:
       l' = l_m + 1
   else:
       l' = 0
```

**Example:**

```
Process A:                    Process B:
Physical: 100ms              Physical: 105ms (5ms ahead)
HLC: (100, 0)               HLC: (105, 0)

Event at A:
HLC: (100, 1)

Send message A â†’ B:
Message: (100, 1)

Receive at B:
Physical: 105ms
max(105, 100, 105) = 105
Same as B's pt, so: l = max(0, 1) + 1 = 2
HLC: (105, 2)

Properties:
1. HLC â‰ˆ Physical time (within clock skew)
2. Causality preserved (like Lamport)
3. Human-readable timestamps
```

**Benefits:**
```
+ Physical time approximation (debugging, logging)
+ Causality tracking (happens-before)
+ Bounded clock skew (adjusts for drift)
+ Space efficient (physical + small counter)

Use case: CockroachDB, MongoDB
```

### 1.6 TrueTime (Google Spanner)

**Bounded uncertainty intervals**

```
Instead of single timestamp, use interval: [earliest, latest]

TrueTime API:
TT.now() â†’ [earliest, latest]

Example:
TT.now() â†’ [100ms, 107ms]
â”œâ”€ Actual time is somewhere in this 7ms window
â”œâ”€ Uncertainty = 7ms (Îµ)
â””â”€ Guaranteed: actual_time âˆˆ [100ms, 107ms]

How it works:
1. Multiple time sources (GPS + atomic clocks)
2. Measure uncertainty from multiple sources
3. Return interval, not single time

Commit Wait:
Transaction commits at time T
Wait until: TT.now().earliest > T
Ensures all future reads see committed data

Example:
t1: Transaction commits at T=100ms
t2: TT.now() = [98ms, 105ms] (still overlaps with 100ms)
    â†’ Wait (don't return commit yet)
t3: TT.now() = [101ms, 108ms] (no overlap with 100ms)
    â†’ Safe to return commit (all future reads > 100ms)

Enables: External consistency (linearizability across datacenters)
Cost: High (specialized hardware, commit wait latency)
```

---

## 2. ID Generation

### 2.1 The Problem: Unique IDs in Distributed Systems

```
Requirements:
âœ“ Globally unique (no collisions)
âœ“ Scalable (generate millions/sec)
âœ“ Sortable (roughly time-ordered)
âœ“ No coordination (no SPOF)
âœ“ Short (fit in 64 bits)

Naive solutions:
âŒ Auto-increment: Single database bottleneck
âŒ UUID v4: 128 bits, not sortable, random collisions
âŒ Central ID server: Single point of failure
```

### 2.2 Snowflake ID (Twitter)

**64-bit time-based unique ID**

```
Structure (64 bits total):

|1 bit |41 bits          |10 bits    |12 bits   |
|unused|timestamp (ms)   |machine ID |sequence  |

Bit breakdown:
â”œâ”€ 1 bit: Unused (sign bit, always 0)
â”œâ”€ 41 bits: Timestamp (milliseconds since epoch)
â”‚   â””â”€ 2^41 ms â‰ˆ 69 years
â”œâ”€ 10 bits: Machine/datacenter ID
â”‚   â””â”€ 2^10 = 1024 unique machines
â””â”€ 12 bits: Sequence number (per machine per ms)
    â””â”€ 2^12 = 4096 IDs per ms per machine

Capacity:
â”œâ”€ Per machine: 4096 IDs/ms = 4M IDs/sec
â”œâ”€ Total (1024 machines): 4B IDs/sec
â””â”€ Lifespan: 69 years
```

**Algorithm:**

```python
class SnowflakeIDGenerator:
    def __init__(self, machine_id, epoch=1609459200000):  # 2021-01-01
        self.machine_id = machine_id  # 0-1023
        self.epoch = epoch
        self.sequence = 0
        self.last_timestamp = -1
    
    def _current_ms(self):
        return int(time.time() * 1000)
    
    def next_id(self):
        timestamp = self._current_ms()
        
        if timestamp < self.last_timestamp:
            # Clock moved backward!
            raise Exception("Clock moved backward!")
        
        if timestamp == self.last_timestamp:
            # Same millisecond, increment sequence
            self.sequence = (self.sequence + 1) & 0xFFF  # 12 bits max
            
            if self.sequence == 0:
                # Sequence overflow, wait for next millisecond
                while timestamp <= self.last_timestamp:
                    timestamp = self._current_ms()
        else:
            # New millisecond, reset sequence
            self.sequence = 0
        
        self.last_timestamp = timestamp
        
        # Combine components into 64-bit ID
        id = ((timestamp - self.epoch) << 22) | \
             (self.machine_id << 12) | \
             self.sequence
        
        return id

# Usage
generator = SnowflakeIDGenerator(machine_id=1)
id1 = generator.next_id()  # 7234729837492873
id2 = generator.next_id()  # 7234729837492874 (sequential)
```

**Example IDs:**

```
Epoch: 2021-01-01 00:00:00 (1609459200000 ms)
Machine ID: 1
Sequence: 0

Generated at 2024-01-01 00:00:00.100:
Timestamp: 2024-01-01 00:00:00.100 = 1704067200100 ms
Offset: 1704067200100 - 1609459200000 = 94608000100 ms

Binary:
|0|00010110000000011010101110111000101000100|0000000001|000000000000|
|0|          94608000100                   |    1      |     0      |

Decimal: 397066990075904000

Next ID (same millisecond):
Sequence: 1
Decimal: 397066990075904001

Next ID (next millisecond):
Timestamp: +1ms
Decimal: 397066990080098304 (jump by 4096)
```

**Properties:**

```
Sortable:
â”œâ”€ IDs generated later have higher values
â”œâ”€ Can ORDER BY id (roughly chronological)
â””â”€ First 41 bits = timestamp (dominant)

Unique:
â”œâ”€ Different machines: Different machine_id
â”œâ”€ Same machine, different ms: Different timestamp
â”œâ”€ Same machine, same ms: Different sequence
â””â”€ Collision impossible (within constraints)

Decoding:
id = 397066990075904000
timestamp = (id >> 22) + epoch = 2024-01-01 00:00:00.100
machine_id = (id >> 12) & 0x3FF = 1
sequence = id & 0xFFF = 0
```

**Challenges:**

```
1. Clock skew:
   Machine A: 10:00:00.000
   Machine B: 10:00:00.500 (500ms ahead)
   
   Machine B generates IDs with higher timestamps
   Machine A catches up â†’ Lower IDs after higher IDs
   Solution: NTP synchronization, reject if clock moves backward

2. Sequence overflow:
   Single machine generates >4096 IDs in 1ms
   Solution: Wait for next millisecond (blocking)

3. Machine ID allocation:
   How to assign unique machine IDs (0-1023)?
   Solutions:
   â”œâ”€ Configuration file (manual)
   â”œâ”€ ZooKeeper (automatic registration)
   â””â”€ Environment variable (container orchestration)
```

### 2.3 Alternative ID Schemes

**UUID v1 (Time-based):**
```
128 bits: timestamp + MAC address + sequence

Pros:
+ Guaranteed unique (MAC address)
+ Time-ordered

Cons:
- 128 bits (larger than 64)
- Reveals MAC address (privacy)
- Not sequential (timestamp is low bits)

Example: 550e8400-e29b-41d4-a716-446655440000
```

**UUID v4 (Random):**
```
128 bits: Random

Pros:
+ Simple generation (no coordination)
+ No privacy concerns

Cons:
- 128 bits
- Not sortable
- Collision possible (very low probability)

Collision probability:
After 103 trillion UUIDs, 1 in a billion chance
```

**ULID (Universally Unique Lexicographically Sortable ID):**
```
128 bits: 48-bit timestamp + 80-bit random

Structure:
 01AN4Z07BY      79KA1307SR9X4MV3
|----------|    |----------------|
 Timestamp          Randomness
  48 bits            80 bits

Pros:
+ Sortable (timestamp first)
+ URL-safe (Base32 encoding)
+ No coordination

Cons:
- 128 bits (larger)

Example: 01ARZ3NDEKTSV4RRFFQ69G5FAV
```

**Instagram ID:**
```
64 bits: 41-bit timestamp + 13-bit shard + 10-bit sequence

Similar to Snowflake, but:
â”œâ”€ Shard ID (database shard)
â””â”€ Different bit allocation

Optimized for Instagram's sharding scheme
```

**Comparison:**

| Scheme | Bits | Sortable | Collision | Coordination | Use Case |
|--------|------|----------|-----------|--------------|----------|
| **Snowflake** | 64 | Yes (ms) | None | Machine ID | Twitter, Discord |
| **UUID v1** | 128 | Partial | None | None | General |
| **UUID v4** | 128 | No | ~0 | None | General |
| **ULID** | 128 | Yes (ms) | ~0 | None | Modern apps |
| **Auto-increment** | 64 | Yes | None | Database | Single DB |

---

## 3. Leases (Time-Based Locks)

### 3.1 What Are Leases?

**Time-limited exclusive access to a resource**

```
Lease = Lock + Expiration time

Traditional lock:
Client acquires lock â†’ Holds indefinitely â†’ Releases (or crashes!)
Problem: Deadlock if client crashes without releasing

Lease:
Client acquires lease â†’ Holds for T seconds â†’ Auto-expires
Benefit: No deadlock (lease expires even if client crashes)
```

**Example:**

```
Client A requests lease for "resource:1"
Server grants: Lease expires at T=10:00:10 (10 seconds)

Timeline:
10:00:00 - Client A: Acquire lease âœ“
10:00:05 - Client A: Use resource
10:00:07 - Client B: Request lease (denied, A holds it)
10:00:10 - Lease expires automatically
10:00:11 - Client B: Request lease âœ“ (now granted)

If Client A crashes at 10:00:05:
â””â”€ Lease still expires at 10:00:10 (no deadlock)
```

### 3.2 Lease Implementation

```python
import time
from threading import Lock

class LeaseManager:
    def __init__(self):
        self.leases = {}  # resource -> (client, expiry)
        self.lock = Lock()
    
    def acquire_lease(self, resource, client, duration=10):
        with self.lock:
            now = time.time()
            
            # Check if lease exists and is valid
            if resource in self.leases:
                holder, expiry = self.leases[resource]
                if now < expiry:
                    # Lease still valid
                    return False, f"Held by {holder} until {expiry}"
                # Lease expired, can grant new one
            
            # Grant lease
            expiry = now + duration
            self.leases[resource] = (client, expiry)
            return True, f"Granted until {expiry}"
    
    def renew_lease(self, resource, client, duration=10):
        with self.lock:
            now = time.time()
            
            if resource not in self.leases:
                return False, "No lease exists"
            
            holder, expiry = self.leases[resource]
            
            if holder != client:
                return False, f"Held by {holder}, not {client}"
            
            if now >= expiry:
                return False, "Lease expired"
            
            # Renew lease
            new_expiry = now + duration
            self.leases[resource] = (client, new_expiry)
            return True, f"Renewed until {new_expiry}"
    
    def release_lease(self, resource, client):
        with self.lock:
            if resource not in self.leases:
                return False, "No lease exists"
            
            holder, expiry = self.leases[resource]
            
            if holder != client:
                return False, f"Held by {holder}, not {client}"
            
            del self.leases[resource]
            return True, "Released"

# Usage
manager = LeaseManager()

# Client A acquires lease
success, msg = manager.acquire_lease("database:1", "client-A", duration=10)
print(success, msg)  # True, "Granted until 1234567890.5"

# Client B tries (denied)
success, msg = manager.acquire_lease("database:1", "client-B")
print(success, msg)  # False, "Held by client-A until 1234567890.5"

# Client A renews
success, msg = manager.renew_lease("database:1", "client-A")
print(success, msg)  # True, "Renewed until 1234567900.5"

# Client A releases
success, msg = manager.release_lease("database:1", "client-A")
print(success, msg)  # True, "Released"

# Client B acquires (now successful)
success, msg = manager.acquire_lease("database:1", "client-B")
print(success, msg)  # True, "Granted until 1234567910.5"
```

### 3.3 Lease Renewal & Heartbeats

```
Problem: Client needs lease longer than initial duration

Solution: Periodic renewal (heartbeat)

Client-side:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ acquire_lease(resource, duration=10)    â”‚
â”‚                                         â”‚
â”‚ while working:                          â”‚
â”‚     work_on_resource()  (5 seconds)     â”‚
â”‚     renew_lease(resource)  (heartbeat)  â”‚
â”‚     time.sleep(5)                       â”‚
â”‚                                         â”‚
â”‚ release_lease(resource)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Timeline:
t=0: Acquire (expires at t=10)
t=5: Renew (expires at t=15) â† Heartbeat
t=10: Renew (expires at t=20) â† Heartbeat
t=15: Work complete, release

If client crashes at t=7:
â”œâ”€ No more renewals
â”œâ”€ Lease expires at t=10 (original + 1 renewal)
â””â”€ Resource available at t=10 (3 second recovery)
```

**Heartbeat Pattern:**

```python
import threading
import time

class LeaseClient:
    def __init__(self, manager, resource, client_id):
        self.manager = manager
        self.resource = resource
        self.client_id = client_id
        self.lease_duration = 10
        self.heartbeat_interval = 5  # Renew every 5 seconds
        self.heartbeat_thread = None
        self.running = False
    
    def acquire(self):
        success, msg = self.manager.acquire_lease(
            self.resource, self.client_id, self.lease_duration
        )
        if success:
            self.running = True
            self.heartbeat_thread = threading.Thread(target=self._heartbeat)
            self.heartbeat_thread.start()
        return success, msg
    
    def _heartbeat(self):
        while self.running:
            time.sleep(self.heartbeat_interval)
            if self.running:
                success, msg = self.manager.renew_lease(
                    self.resource, self.client_id, self.lease_duration
                )
                if not success:
                    print(f"Heartbeat failed: {msg}")
                    self.running = False
    
    def release(self):
        self.running = False
        if self.heartbeat_thread:
            self.heartbeat_thread.join()
        return self.manager.release_lease(self.resource, self.client_id)

# Usage
manager = LeaseManager()
client = LeaseClient(manager, "database:1", "client-A")

client.acquire()
# Work with resource (heartbeat runs in background)
time.sleep(20)  # Lease renewed automatically
client.release()
```

### 3.4 Lease Use Cases

**1. Leader Election:**
```
Multiple servers compete for leadership
Winner holds lease on "leader" resource

Server A: acquire_lease("leader", "server-A", 30s) âœ“
Server B: acquire_lease("leader", "server-B", 30s) âœ— (denied)
Server C: acquire_lease("leader", "server-C", 30s) âœ— (denied)

Server A is leader for 30 seconds
Server A renews every 15 seconds (heartbeat)

If Server A crashes:
â”œâ”€ Lease expires after 30s (no renewal)
â”œâ”€ Server B or C acquires lease (new leader)
â””â”€ Failover time: 30 seconds
```

**2. Distributed Cache Invalidation:**
```
Cache entry has lease: "cache:user:123" â†’ expires 10:00:10

Writer updates database at 10:00:05:
â”œâ”€ Cannot acquire lease (still held by cache)
â”œâ”€ Wait until 10:00:10 (lease expires)
â”œâ”€ Acquire lease, update database
â””â”€ Cache automatically invalid (lease expired)

Guarantees: No stale cache reads during lease period
```

**3. Exclusive Resource Access:**
```
Batch job needs exclusive database access:

Job A: acquire_lease("db:maintenance", duration=3600) # 1 hour
â”œâ”€ Holds lease for maintenance window
â”œâ”€ Other jobs blocked
â””â”€ Lease auto-expires after 1 hour (even if job hangs)

Job B: acquire_lease("db:maintenance") âœ—
â”œâ”€ Denied while Job A holds lease
â””â”€ Retries later
```

**4. GFS (Google File System) Master:**
```
Master holds lease on chunk:
â”œâ”€ Duration: 60 seconds
â”œâ”€ Heartbeat: Every 10 seconds
â””â”€ Clients cache "master is at server X" for lease duration

Reduces load on master (clients don't query constantly)
Failover: New master elected after 60s
```

---

## 4. Consensus Algorithms

### 4.1 What Is Consensus?

**Multiple nodes agree on a single value**

```
Problem:
3 nodes need to agree on leader:
â”œâ”€ Node A thinks: B is leader
â”œâ”€ Node B thinks: A is leader
â””â”€ Node C thinks: C is leader

Split brain! No consensus

Consensus algorithm ensures:
âœ“ Agreement: All nodes agree on same value
âœ“ Validity: Agreed value was proposed by some node
âœ“ Termination: All nodes eventually decide
âœ“ Fault tolerance: Works even if some nodes fail
```

### 4.2 Raft Consensus

**Understandable consensus algorithm**

#### Raft Roles

```
Three roles:
1. Leader: Handles all client requests, replicates log
2. Follower: Passive, follows leader's log
3. Candidate: Competes to become leader during election

State transitions:
Follower â†’ (timeout) â†’ Candidate â†’ (wins election) â†’ Leader
                     â†’ (loses/splits) â†’ Follower
Leader â†’ (detects higher term) â†’ Follower
```

#### Leader Election

```
Initial state: All nodes are Followers

1. Election timeout (150-300ms):
   Node A: Timeout! No heartbeat from leader
   Node A: Becomes Candidate, increments term (term=1)
   Node A: Votes for self (1 vote)
   Node A: Requests votes from others

2. Request Vote RPC:
   Node A â†’ Node B: "Vote for me (term=1, log=...)"
   Node A â†’ Node C: "Vote for me (term=1, log=...)"

3. Voting:
   Node B: Checks term (1 >= 0) âœ“
           Hasn't voted this term âœ“
           Candidate's log up-to-date âœ“
           Votes for A (1 vote to A)
   
   Node C: Same checks, votes for A (1 vote to A)

4. Majority achieved:
   Node A: Received 3 votes (self + B + C)
           Majority = 3/3 âœ“
           Becomes Leader (term=1)

5. Leader announces:
   Node A â†’ Node B: "I'm leader (term=1)" (heartbeat)
   Node A â†’ Node C: "I'm leader (term=1)" (heartbeat)
   Node B, C: Acknowledge, remain Followers
```

**Split Vote:**
```
Scenario: Multiple candidates

Node A: Timeout at t=100ms â†’ Becomes Candidate
Node B: Timeout at t=105ms â†’ Becomes Candidate

Voting:
Node A: Votes for self (1 vote)
Node B: Votes for self (1 vote)
Node C: Votes for A (1 vote to A)
Node D: Votes for B (1 vote to B)
Node E: Votes for A (1 vote to A)

Results:
Node A: 3 votes (A, C, E) â†’ Majority (3/5) â†’ Becomes Leader âœ“
Node B: 2 votes (B, D) â†’ No majority â†’ Reverts to Follower

If tie (e.g., 2-2 in 4 nodes):
â”œâ”€ No majority
â”œâ”€ Timeout, start new election (term++)
â””â”€ Randomized timeout prevents repeated ties
```

#### Log Replication

```
Client request â†’ Leader:

1. Leader receives: "SET x=5"
2. Leader appends to log:
   Log: [..., entry_10: "SET x=5" (term=1, uncommitted)]

3. Leader replicates to Followers:
   Leader â†’ Follower 1: AppendEntries [entry_10]
   Leader â†’ Follower 2: AppendEntries [entry_10]

4. Followers append to log:
   Follower 1: Log: [..., entry_10: "SET x=5"]
               Reply: Success
   
   Follower 2: Log: [..., entry_10: "SET x=5"]
               Reply: Success

5. Leader receives majority ACKs:
   Majority: 3/3 (Leader + Follower 1 + Follower 2) âœ“
   
6. Leader commits entry:
   commitIndex = 10
   Apply to state machine: x=5
   
7. Leader replies to client:
   "Success: SET x=5"

8. Leader informs Followers (next heartbeat):
   "commitIndex=10" â†’ Followers commit entry_10

Guarantee: Entry committed only after majority replication
```
 
**Log Consistency:**
```
Leader's log is source of truth

Follower log inconsistency:
Leader:    [1, 2, 3, 4, 5]
Follower:  [1, 2, 3, 4, 6]  â† Different entry 5

Leader detects:
â”œâ”€ AppendEntries RPC includes previous entry hash
â”œâ”€ Follower checks: entry_4 hash matches âœ“
â”œâ”€ Follower's entry_5 conflicts
â””â”€ Follower deletes entry_5, appends Leader's entry_5

Result: Follower log matches Leader
```

#### Raft Safety

```
Election Restriction:
Candidate can only win if log is "up-to-date"

Up-to-date check:
Compare (term, log_length):
â”œâ”€ Higher term â†’ More up-to-date
â”œâ”€ Same term, longer log â†’ More up-to-date
â””â”€ Prevents old nodes from becoming leader

Example:
Node A: log=[1,2,3] (term=5)
Node B: log=[1,2,3,4] (term=4)

Node A requests vote from Node B:
Node B compares: A's term (5) > B's term (4)
Node B votes for A (even though B has longer log)
Reason: Higher term = more recent authority
```

**Committed Entry Guarantee:**
```
Once entry committed, it will appear in all future leaders' logs

Proof:
1. Entry committed â†’ Majority of nodes have it
2. New leader elected â†’ Requires majority votes
3. Overlapping majorities â†’ At least one voter has entry
4. Up-to-date check â†’ New leader must have entry
5. New leader replicates log â†’ Entry propagates

Result: Committed entry never lost
```

### 4.3 Paxos Consensus

**Original consensus algorithm (more complex than Raft)**

#### Paxos Roles

```
1. Proposer: Proposes values
2. Acceptor: Votes on proposals
3. Learner: Learns chosen value

(Nodes can play multiple roles)
```

#### Paxos Algorithm (Simplified)

**Phase 1: Prepare**
```
Proposer:
1. Choose proposal number N (unique, increasing)
2. Send Prepare(N) to Acceptors

Acceptor:
1. Receive Prepare(N)
2. If N > highest_seen:
     Promise not to accept proposals < N
     Reply: Promise(N, highest_accepted_value)
   Else:
     Reject

Proposer:
1. Receive majority Promises
2. If any Promise includes value:
     Use highest-numbered value
   Else:
     Use own value
```

**Phase 2: Accept**
```
Proposer:
1. Send Accept(N, value) to Acceptors

Acceptor:
1. Receive Accept(N, value)
2. If N >= highest_promised:
     Accept proposal
     Reply: Accepted(N, value)
   Else:
     Reject

Proposer:
1. Receive majority Acceptances
2. Value chosen! Inform Learners
```

**Example:**

```
3 Acceptors: A1, A2, A3

Proposer P1 wants to propose value "X":

Phase 1 (Prepare):
P1 â†’ A1: Prepare(N=1)
P1 â†’ A2: Prepare(N=1)
P1 â†’ A3: Prepare(N=1)

A1: Promise(N=1, no previous value)
A2: Promise(N=1, no previous value)
A3: Promise(N=1, no previous value)

P1 receives majority (3/3), proceeds with value "X"

Phase 2 (Accept):
P1 â†’ A1: Accept(N=1, value="X")
P1 â†’ A2: Accept(N=1, value="X")
P1 â†’ A3: Accept(N=1, value="X")

A1: Accepted(N=1, value="X")
A2: Accepted(N=1, value="X")
A3: Accepted(N=1, value="X")

P1 receives majority (3/3), "X" chosen!

Concurrent Proposer P2 (N=2, value="Y"):
P2 â†’ A1: Prepare(N=2)
A1: Promise(N=2, accepted_value="X") â† Must use "X"!

P2 â†’ A2: Prepare(N=2)
A2: Promise(N=2, accepted_value="X")

P2 receives Promises with value "X"
P2 must propose "X" (not "Y")
P2 â†’ A1: Accept(N=2, value="X")

Result: Consensus on "X" (even though P2 wanted "Y")
```

#### Multi-Paxos

```
Problem: Single-value Paxos inefficient for log replication

Multi-Paxos optimization:
1. Elect stable leader (one-time election)
2. Leader skips Phase 1 for subsequent proposals
3. Leader directly sends Accept (Phase 2)

Similar to Raft but more complex
```

### 4.4 Raft vs Paxos

| Aspect | Raft | Paxos |
|--------|------|-------|
| **Understandability** | Easier (designed for understanding) | Harder (complex, many variants) |
| **Leader** | Strong leader (log only from leader) | Weak leader (any proposer) |
| **Log** | Sequential, no gaps | Can have gaps |
| **Implementation** | Simpler | Complex |
| **Efficiency** | Leader election + replication | Multi-phase per value |
| **Use** | etcd, Consul | Google (Chubby, Spanner) |

### 4.5 Membership Changes

**Adding/removing nodes from cluster**

```
Problem: Change membership while maintaining consensus

Naive approach (doesn't work):
Old config: [A, B, C] (majority: 2)
New config: [A, B, C, D, E] (majority: 3)

Simultaneously switch â†’ Split brain:
Some nodes use old config (majority: 2)
Some nodes use new config (majority: 3)
Two leaders possible!

Raft solution: Joint consensus
1. Transition to joint config: C_old,new
2. Both configs must agree (majority in both)
3. Transition to new config: C_new
```

**Joint Consensus:**

```
Step 1: Add C_old,new configuration entry
â”œâ”€ Must be committed in both C_old and C_new
â”œâ”€ Old nodes (A, B, C) need 2/3 votes
â”œâ”€ All nodes (A, B, C, D, E) need 3/5 votes
â””â”€ Overlap guarantees single leader

Step 2: Once C_old,new committed, add C_new entry
â”œâ”€ Must be committed in C_new only
â”œâ”€ Old nodes can be removed
â””â”€ System runs with new membership

Timeline:
t1: Leader proposes C_old,new
t2: C_old,new committed (majority in both configs)
t3: Leader proposes C_new
t4: C_new committed (majority in new config)
t5: Old nodes shut down (safe)
```

---

## Best Practices Summary

```
Time & Clocks:
âœ“ Use NTP for synchronization (Â±10-100ms accuracy)
âœ“ Use logical clocks for ordering (Lamport, Vector)
âœ“ Use HLC for human-readable + causality
âœ“ Handle clock skew (reject backward movement)
âœ“ Monitor clock drift (alert if >1s)
âœ— Don't rely on physical clocks for strict ordering
âœ— Don't ignore leap seconds (use leap smearing)

ID Generation:
âœ“ Use Snowflake for sortable, 64-bit IDs
âœ“ Use UUID v4 for simple, no-coordination IDs
âœ“ Synchronize clocks for Snowflake (NTP)
âœ“ Handle sequence overflow (wait for next ms)
âœ“ Allocate unique machine IDs (ZooKeeper, config)
âœ— Don't use auto-increment across databases (bottleneck)
âœ— Don't ignore clock skew (causes non-monotonic IDs)

Leases:
âœ“ Use leases for time-bound resources (auto-expiry)
âœ“ Set appropriate duration (balance safety vs availability)
âœ“ Implement heartbeat/renewal (extend lease)
âœ“ Handle lease expiry gracefully (retry, backoff)
âœ“ Monitor lease renewals (detect failures early)
âœ— Don't make leases too short (thrashing)
âœ— Don't make leases too long (slow failover)

Consensus:
âœ“ Use Raft for new systems (easier to understand)
âœ“ Use established libraries (etcd, Consul, ZooKeeper)
âœ“ Size cluster appropriately (3-5 nodes typical)
âœ“ Monitor leader elections (should be rare)
âœ“ Test failure scenarios (network partition, node crash)
âœ— Don't implement consensus yourself (use libraries)
âœ— Don't use even number of nodes (no benefit, higher cost)
âœ— Don't ignore split-brain (quorum prevents)

General:
âœ“ Design for failures (networks partition, nodes crash)
âœ“ Monitor distributed primitives (leader, lag, clock skew)
âœ“ Test edge cases (concurrent elections, clock skew)
âœ“ Document assumptions (clock sync requirements)
âœ“ Use timeouts conservatively (network delays vary)
```

Complete distributed system primitives foundation! âš™ï¸ğŸ”§
