# Cost Optimization, Capacity Planning & FinOps

## What Is FinOps?

**Financial Operations - Manage cloud costs efficiently**

```
FinOps principles:
‚îú‚îÄ Visibility: Know what you're spending (cost breakdown)
‚îú‚îÄ Optimization: Reduce waste (right-sizing, reserved instances)
‚îú‚îÄ Accountability: Teams own their costs (chargeback)
‚îî‚îÄ Collaboration: Engineers + finance work together

Key challenges:
‚îú‚îÄ Cloud costs unpredictable (pay-per-use)
‚îú‚îÄ Over-provisioning (wasted resources)
‚îú‚îÄ Under-provisioning (performance issues)
‚îú‚îÄ Untagged resources (can't attribute costs)
‚îî‚îÄ Egress costs (data transfer expensive)

FinOps cycle:
1. Inform: Allocate costs, benchmark
2. Optimize: Right-size, autoscale, purchase commitments
3. Operate: Continuously monitor, improve
```

---

## 1. Cost Optimization

### 1.1 Right-Sizing

**Match resources to actual usage**

```
Right-sizing: Adjust instance sizes to fit workload

Problem:
‚îú‚îÄ Over-provisioned: 32 vCPU, 128 GB RAM
‚îÇ  ‚îú‚îÄ Actual usage: 4 vCPU (12.5%), 16 GB RAM (12.5%)
‚îÇ  ‚îî‚îÄ Waste: 87.5% of resources unused ($$$)

‚îú‚îÄ Under-provisioned: 2 vCPU, 4 GB RAM
‚îÇ  ‚îú‚îÄ Actual usage: 2 vCPU (100%), 4 GB RAM (100%)
‚îÇ  ‚îî‚îÄ Problem: High latency, timeouts, dropped requests

Solution: Right-size to actual usage + headroom
‚îú‚îÄ Actual usage: 4 vCPU, 16 GB RAM
‚îú‚îÄ Right-sized: 8 vCPU, 32 GB RAM (2x headroom)
‚îî‚îÄ Savings: 75% cost reduction (vs 32 vCPU instance)

Right-sizing process:
1. Monitor utilization (CPU, memory, disk, network)
2. Analyze patterns (peak vs average)
3. Select appropriate size (usage + headroom)
4. Test new size (staging first)
5. Deploy to production
6. Monitor (verify performance)
```

**Right-Sizing Analysis:**

```python
import boto3
from datetime import datetime, timedelta

cloudwatch = boto3.client('cloudwatch')
ec2 = boto3.client('ec2')

def analyze_instance(instance_id, days=30):
    """Analyze instance utilization for right-sizing"""
    
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(days=days)
    
    # Get CPU utilization
    cpu_stats = cloudwatch.get_metric_statistics(
        Namespace='AWS/EC2',
        MetricName='CPUUtilization',
        Dimensions=[{'Name': 'InstanceId', 'Value': instance_id}],
        StartTime=start_time,
        EndTime=end_time,
        Period=3600,  # 1 hour intervals
        Statistics=['Average', 'Maximum']
    )
    
    # Calculate averages
    cpu_avg = sum(d['Average'] for d in cpu_stats['Datapoints']) / len(cpu_stats['Datapoints'])
    cpu_max = max(d['Maximum'] for d in cpu_stats['Datapoints'])
    
    # Get instance type
    instance = ec2.describe_instances(InstanceIds=[instance_id])
    instance_type = instance['Reservations'][0]['Instances'][0]['InstanceType']
    
    # Right-sizing recommendation
    print(f"Instance: {instance_id} ({instance_type})")
    print(f"CPU Average: {cpu_avg:.2f}%")
    print(f"CPU Max: {cpu_max:.2f}%")
    
    if cpu_avg < 20 and cpu_max < 50:
        print("‚ùå OVER-PROVISIONED: Downsize to smaller instance")
        print(f"   Recommendation: {get_smaller_instance(instance_type)}")
    elif cpu_avg > 70 or cpu_max > 90:
        print("‚ö†Ô∏è  UNDER-PROVISIONED: Upsize to larger instance")
        print(f"   Recommendation: {get_larger_instance(instance_type)}")
    else:
        print("‚úì PROPERLY SIZED: No change needed")
    
    # Calculate potential savings
    if cpu_avg < 20:
        current_cost = get_instance_cost(instance_type)
        recommended_type = get_smaller_instance(instance_type)
        recommended_cost = get_instance_cost(recommended_type)
        savings = current_cost - recommended_cost
        
        print(f"   Current cost: ${current_cost:.2f}/month")
        print(f"   New cost: ${recommended_cost:.2f}/month")
        print(f"   Savings: ${savings:.2f}/month ({savings/current_cost*100:.1f}%)")

def get_instance_cost(instance_type):
    """Get monthly cost for instance type"""
    pricing = {
        't3.nano': 3.80,
        't3.micro': 7.59,
        't3.small': 15.18,
        't3.medium': 30.37,
        't3.large': 60.74,
        't3.xlarge': 121.47,
        't3.2xlarge': 242.94,
        # Add more as needed
    }
    return pricing.get(instance_type, 0)

def get_smaller_instance(instance_type):
    """Get next smaller instance type"""
    sizes = ['nano', 'micro', 'small', 'medium', 'large', 'xlarge', '2xlarge']
    family = instance_type.rsplit('.', 1)[0]  # e.g., 't3'
    current_size = instance_type.rsplit('.', 1)[1]  # e.g., 'large'
    
    current_index = sizes.index(current_size)
    if current_index > 0:
        return f"{family}.{sizes[current_index - 1]}"
    return instance_type

# Run analysis on all instances
def analyze_all_instances():
    instances = ec2.describe_instances(
        Filters=[{'Name': 'instance-state-name', 'Values': ['running']}]
    )
    
    for reservation in instances['Reservations']:
        for instance in reservation['Instances']:
            analyze_instance(instance['InstanceId'])
            print("-" * 60)
```

### 1.2 Autoscaling

**Automatically adjust capacity based on demand**

```
Autoscaling: Scale up during peak, scale down during off-peak

Example: E-commerce site
‚îú‚îÄ Peak traffic: 9 AM - 9 PM (12 hours)
‚îÇ  ‚îî‚îÄ Need: 20 instances
‚îú‚îÄ Off-peak: 9 PM - 9 AM (12 hours)
‚îÇ  ‚îî‚îÄ Need: 5 instances

Without autoscaling:
‚îú‚îÄ Run 20 instances 24/7
‚îú‚îÄ Cost: 20 √ó $30/month √ó 730 hours = $14,600/month
‚îî‚îÄ Waste: 15 instances idle for 12 hours/day

With autoscaling:
‚îú‚îÄ Peak: 20 instances (12 hours/day)
‚îú‚îÄ Off-peak: 5 instances (12 hours/day)
‚îú‚îÄ Average: 12.5 instances
‚îú‚îÄ Cost: 12.5 √ó $30/month √ó 730 hours = $9,125/month
‚îî‚îÄ Savings: $5,475/month (37% reduction)

Autoscaling metrics:
‚îú‚îÄ CPU utilization (target: 70%)
‚îú‚îÄ Request count (target: 1000 req/s per instance)
‚îú‚îÄ Network traffic (target: 100 Mbps per instance)
‚îî‚îÄ Queue depth (target: 10 messages per worker)
```

**Kubernetes Horizontal Pod Autoscaler:**

```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 5      # Minimum pods (off-peak)
  maxReplicas: 20     # Maximum pods (peak)
  metrics:
  # Scale based on CPU
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Target 70% CPU
  
  # Scale based on memory
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # Target 80% memory
  
  # Scale based on custom metric (requests per second)
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"  # 1000 req/s per pod
  
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 min before scaling down
      policies:
      - type: Percent
        value: 50  # Scale down max 50% at a time
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0  # Scale up immediately
      policies:
      - type: Percent
        value: 100  # Scale up max 100% at a time
        periodSeconds: 60
```

**AWS Auto Scaling Group:**

```python
import boto3

autoscaling = boto3.client('autoscaling')

# Create Auto Scaling Group
autoscaling.create_auto_scaling_group(
    AutoScalingGroupName='myapp-asg',
    LaunchTemplate={
        'LaunchTemplateId': 'lt-123456789',
        'Version': '$Latest'
    },
    MinSize=5,      # Minimum instances
    MaxSize=20,     # Maximum instances
    DesiredCapacity=10,  # Initial capacity
    VPCZoneIdentifier='subnet-12345,subnet-67890',
    TargetGroupARNs=['arn:aws:elasticloadbalancing:...'],
    HealthCheckType='ELB',
    HealthCheckGracePeriod=300
)

# Target tracking scaling policy (CPU)
autoscaling.put_scaling_policy(
    AutoScalingGroupName='myapp-asg',
    PolicyName='cpu-target-tracking',
    PolicyType='TargetTrackingScaling',
    TargetTrackingConfiguration={
        'PredefinedMetricSpecification': {
            'PredefinedMetricType': 'ASGAverageCPUUtilization'
        },
        'TargetValue': 70.0  # Target 70% CPU
    }
)

# Schedule-based scaling (predictable traffic patterns)
autoscaling.put_scheduled_update_group_action(
    AutoScalingGroupName='myapp-asg',
    ScheduledActionName='scale-up-morning',
    Recurrence='0 8 * * *',  # 8 AM daily
    MinSize=15,
    MaxSize=20,
    DesiredCapacity=15
)

autoscaling.put_scheduled_update_group_action(
    AutoScalingGroupName='myapp-asg',
    ScheduledActionName='scale-down-evening',
    Recurrence='0 20 * * *',  # 8 PM daily
    MinSize=5,
    MaxSize=10,
    DesiredCapacity=5
)
```

### 1.3 Reserved Instances vs Spot Instances

**Purchase commitments for cost savings**

```
Pricing models (AWS):

1. On-Demand:
   ‚îú‚îÄ Pay per hour/second
   ‚îú‚îÄ No commitment
   ‚îú‚îÄ Cost: 100% (baseline)
   ‚îî‚îÄ Use case: Unpredictable workloads, testing

2. Reserved Instances (RI):
   ‚îú‚îÄ 1 or 3 year commitment
   ‚îú‚îÄ Up to 72% discount (vs on-demand)
   ‚îú‚îÄ Cost: 28-60% (depending on commitment)
   ‚îî‚îÄ Use case: Steady-state workloads (databases, baseline capacity)

3. Spot Instances:
   ‚îú‚îÄ Bid on spare capacity
   ‚îú‚îÄ Up to 90% discount (vs on-demand)
   ‚îú‚îÄ Can be terminated anytime (2 min notice)
   ‚îú‚îÄ Cost: 10-50% (varies by demand)
   ‚îî‚îÄ Use case: Fault-tolerant workloads (batch jobs, data processing)

Cost comparison (m5.large instance):
‚îú‚îÄ On-Demand: $0.096/hour = $70/month
‚îú‚îÄ Reserved (1 year): $0.062/hour = $45/month (36% discount)
‚îú‚îÄ Reserved (3 year): $0.042/hour = $31/month (56% discount)
‚îî‚îÄ Spot: $0.029/hour = $21/month (70% discount, but can be interrupted)

Optimal mix:
‚îú‚îÄ Baseline capacity (always needed): Reserved (60%)
‚îú‚îÄ Variable capacity (peak hours): On-Demand (30%)
‚îî‚îÄ Batch processing: Spot (10%)
```

**Reserved Instance Strategy:**

```python
# Analyze usage patterns to determine RI purchases
def analyze_ri_opportunities():
    """Identify instances to convert to reserved"""
    
    ec2 = boto3.client('ec2')
    cloudwatch = boto3.client('cloudwatch')
    
    # Get all running instances
    instances = ec2.describe_instances(
        Filters=[{'Name': 'instance-state-name', 'Values': ['running']}]
    )
    
    # Track instance hours by type
    instance_hours = {}
    
    for reservation in instances['Reservations']:
        for instance in reservation['Instances']:
            instance_type = instance['InstanceType']
            launch_time = instance['LaunchTime']
            
            # Calculate hours running
            hours = (datetime.utcnow() - launch_time.replace(tzinfo=None)).total_seconds() / 3600
            
            if instance_type not in instance_hours:
                instance_hours[instance_type] = []
            instance_hours[instance_type].append(hours)
    
    # Recommend RIs for instances running >90 days consistently
    print("Reserved Instance Recommendations:")
    print("-" * 60)
    
    for instance_type, hours_list in instance_hours.items():
        # Count instances running >2160 hours (90 days)
        long_running = sum(1 for h in hours_list if h > 2160)
        
        if long_running > 0:
            on_demand_cost = get_instance_cost(instance_type) * long_running
            reserved_cost = on_demand_cost * 0.4  # Assume 60% discount
            savings = on_demand_cost - reserved_cost
            
            print(f"{instance_type}:")
            print(f"  Instances: {long_running}")
            print(f"  On-Demand cost: ${on_demand_cost:.2f}/month")
            print(f"  Reserved cost: ${reserved_cost:.2f}/month")
            print(f"  Savings: ${savings:.2f}/month ({savings/on_demand_cost*100:.1f}%)")
            print()

# Purchase Reserved Instance
def purchase_reserved_instance(instance_type, count, term='1y'):
    """Purchase Reserved Instances"""
    
    ec2 = boto3.client('ec2')
    
    # Find available RIs
    offerings = ec2.describe_reserved_instances_offerings(
        InstanceType=instance_type,
        OfferingClass='standard',
        OfferingType='All Upfront',  # or 'Partial Upfront', 'No Upfront'
        ProductDescription='Linux/UNIX',
        Duration=31536000 if term == '1y' else 94608000  # 1 or 3 years
    )
    
    if offerings['ReservedInstancesOfferings']:
        offering = offerings['ReservedInstancesOfferings'][0]
        
        # Purchase RI
        ec2.purchase_reserved_instances_offering(
            ReservedInstancesOfferingId=offering['ReservedInstancesOfferingId'],
            InstanceCount=count
        )
        
        print(f"Purchased {count}x {instance_type} Reserved Instances")
        print(f"Cost: ${offering['FixedPrice']:.2f} upfront")
```

**Spot Instance Usage:**

```python
# Request Spot Instances for batch processing
ec2 = boto3.client('ec2')

# Spot Fleet (mix of instance types for higher availability)
ec2.request_spot_fleet(
    SpotFleetRequestConfig={
        'IamFleetRole': 'arn:aws:iam::123456789012:role/spot-fleet-role',
        'TargetCapacity': 10,  # Total capacity
        'LaunchSpecifications': [
            {
                'ImageId': 'ami-12345678',
                'InstanceType': 'm5.large',
                'KeyName': 'my-key',
                'SpotPrice': '0.05',  # Max price willing to pay
                'SubnetId': 'subnet-12345'
            },
            {
                'ImageId': 'ami-12345678',
                'InstanceType': 'm5a.large',  # Alternative instance type
                'KeyName': 'my-key',
                'SpotPrice': '0.05',
                'SubnetId': 'subnet-12345'
            }
        ],
        'AllocationStrategy': 'lowestPrice',  # or 'diversified'
        'TerminateInstancesWithExpiration': True
    }
)

# Handle Spot interruption
@app.route('/spot-interruption', methods=['POST'])
def handle_spot_interruption():
    """Handle 2-minute warning before Spot termination"""
    
    # Get instance metadata (2-minute warning)
    response = requests.get(
        'http://169.254.169.254/latest/meta-data/spot/instance-action'
    )
    
    if response.status_code == 200:
        # Spot instance being terminated
        action = response.json()
        
        print(f"Spot termination in {action['time']} seconds")
        
        # Graceful shutdown:
        # 1. Stop accepting new requests
        # 2. Finish current requests
        # 3. Save state to S3/database
        # 4. Deregister from load balancer
        
        graceful_shutdown()
    
    return "OK"
```

### 1.4 Data Transfer (Egress) Costs

**Minimize expensive data transfer**

```
Data transfer costs (AWS):

Within same region:
‚îú‚îÄ Same AZ: Free
‚îú‚îÄ Different AZ: $0.01/GB (in) + $0.01/GB (out) = $0.02/GB
‚îî‚îÄ Same region: Cheap

Cross-region:
‚îú‚îÄ US East ‚Üí US West: $0.02/GB
‚îú‚îÄ US ‚Üí EU: $0.02/GB
‚îî‚îÄ Moderate cost

Internet egress (out to internet):
‚îú‚îÄ First 10 TB/month: $0.09/GB
‚îú‚îÄ Next 40 TB/month: $0.085/GB
‚îú‚îÄ Next 100 TB/month: $0.07/GB
‚îî‚îÄ Expensive! (ingress free)

Example: 1 TB/month egress
‚îú‚îÄ Cost: 1000 GB √ó $0.09/GB = $90/month
‚îî‚îÄ 10 TB/month = $900/month

Strategies to reduce egress:
1. CDN (CloudFront): Cache at edge, reduce origin egress
2. Compress data (gzip): 70% size reduction
3. Optimize APIs (return only needed fields)
4. Regional data (keep data in same region as users)
5. Caching (reduce repeated transfers)
```

**Egress Cost Optimization:**

```python
# 1. Use CDN (CloudFront) for static assets
import boto3

cloudfront = boto3.client('cloudfront')

# Create CloudFront distribution
cloudfront.create_distribution(
    DistributionConfig={
        'CallerReference': str(time.time()),
        'Origins': {
            'Quantity': 1,
            'Items': [{
                'Id': 's3-origin',
                'DomainName': 'myapp-assets.s3.amazonaws.com',
                'S3OriginConfig': {
                    'OriginAccessIdentity': ''
                }
            }]
        },
        'DefaultCacheBehavior': {
            'TargetOriginId': 's3-origin',
            'ViewerProtocolPolicy': 'redirect-to-https',
            'Compress': True,  # Automatic compression
            'MinTTL': 3600,     # Cache for 1 hour minimum
            'DefaultTTL': 86400  # Cache for 24 hours default
        },
        'Enabled': True
    }
)

# Egress savings:
# Without CDN: 1 TB/month from S3 = $90/month
# With CDN: 100 GB from S3 (cache hits reduce egress) + CloudFront cheaper = $30/month
# Savings: $60/month

# 2. Compress responses (gzip)
from flask import Flask, make_response
import gzip

app = Flask(__name__)

@app.route('/api/data')
def get_data():
    data = get_large_dataset()  # 1 MB uncompressed
    response_json = jsonify(data)
    
    # Compress with gzip
    response = make_response(response_json)
    
    if 'gzip' in request.headers.get('Accept-Encoding', ''):
        compressed = gzip.compress(response.data)
        response.set_data(compressed)
        response.headers['Content-Encoding'] = 'gzip'
        response.headers['Content-Length'] = len(compressed)
        # Size reduced from 1 MB ‚Üí 300 KB (70% reduction)
    
    return response

# 3. Optimize API responses (select fields)
@app.route('/api/users')
def get_users():
    # Bad: Return everything
    users = db.users.find({}, {'_id': 0})  # 100 KB per user
    
    # Good: Return only needed fields
    users = db.users.find({}, {
        '_id': 0,
        'id': 1,
        'name': 1,
        'email': 1
        # Exclude large fields (profile_picture, bio, etc.)
    })  # 10 KB per user (90% reduction)
    
    return jsonify(list(users))

# 4. Regional endpoints (reduce cross-region transfer)
def get_db_connection():
    """Connect to database in same region"""
    
    # Detect current region
    region = os.environ.get('AWS_REGION', 'us-east-1')
    
    # Connect to database in same region
    if region == 'us-east-1':
        db_host = 'db-us-east-1.example.com'
    elif region == 'eu-west-1':
        db_host = 'db-eu-west-1.example.com'
    else:
        db_host = 'db-us-east-1.example.com'
    
    return psycopg2.connect(host=db_host, ...)
```

---

## 2. Performance Testing

### 2.1 Load Testing

**Test system under expected load**

```
Load testing: Simulate normal + peak traffic

Goal: Verify system handles expected load

Test scenarios:
‚îú‚îÄ Normal load: 1000 requests/second (average)
‚îú‚îÄ Peak load: 5000 requests/second (Black Friday)
‚îî‚îÄ Duration: 1 hour (sustained load)

Metrics to monitor:
‚îú‚îÄ Response time (p50, p95, p99)
‚îú‚îÄ Error rate (%)
‚îú‚îÄ Throughput (requests/second)
‚îú‚îÄ Resource utilization (CPU, memory, network)
‚îî‚îÄ Database connections, queue depth

Success criteria:
‚îú‚îÄ p99 latency < 500ms
‚îú‚îÄ Error rate < 0.1%
‚îú‚îÄ CPU < 70%
‚îî‚îÄ No crashes, no memory leaks
```

**Load Test with Locust:**

```python
# locustfile.py
from locust import HttpUser, task, between

class WebsiteUser(HttpUser):
    wait_time = between(1, 3)  # Wait 1-3 seconds between requests
    
    @task(3)  # Weight 3 (3x more frequent)
    def view_homepage(self):
        self.client.get("/")
    
    @task(2)  # Weight 2
    def view_product(self):
        product_id = random.randint(1, 1000)
        self.client.get(f"/products/{product_id}")
    
    @task(1)  # Weight 1 (least frequent)
    def checkout(self):
        with self.client.post("/checkout", json={
            "product_id": 123,
            "quantity": 1
        }, catch_response=True) as response:
            if response.status_code == 200:
                response.success()
            else:
                response.failure(f"Failed: {response.status_code}")
    
    def on_start(self):
        """Login before starting tasks"""
        self.client.post("/login", json={
            "username": "testuser",
            "password": "password"
        })

# Run load test
# locust -f locustfile.py --host=https://api.example.com --users=1000 --spawn-rate=100

# Parameters:
# --users: Total number of concurrent users (1000)
# --spawn-rate: Users to spawn per second (100)
# --run-time: Test duration (1h)
```

**Load Test with k6:**

```javascript
// load-test.js
import http from 'k6/http';
import { check, sleep } from 'k6';

export let options = {
  stages: [
    { duration: '5m', target: 100 },   // Ramp up to 100 users over 5 min
    { duration: '10m', target: 100 },  // Stay at 100 users for 10 min
    { duration: '5m', target: 500 },   // Ramp up to 500 users over 5 min
    { duration: '10m', target: 500 },  // Stay at 500 users for 10 min
    { duration: '5m', target: 0 },     // Ramp down to 0 users
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'],   // 95% requests < 500ms
    http_req_failed: ['rate<0.01'],     // Error rate < 1%
  },
};

export default function () {
  // Homepage
  let res = http.get('https://api.example.com/');
  check(res, {
    'status is 200': (r) => r.status === 200,
    'response time < 500ms': (r) => r.timings.duration < 500,
  });
  
  sleep(1);
  
  // API call
  res = http.post('https://api.example.com/orders', JSON.stringify({
    product_id: 123,
    quantity: 1
  }), {
    headers: { 'Content-Type': 'application/json' },
  });
  
  check(res, {
    'order created': (r) => r.status === 201,
  });
  
  sleep(2);
}

// Run: k6 run load-test.js
```

### 2.2 Stress Testing

**Test system beyond normal capacity**

```
Stress testing: Push system to breaking point

Goal: Find limits and failure modes

Test scenarios:
‚îú‚îÄ Gradually increase load until system breaks
‚îú‚îÄ 1000 ‚Üí 2000 ‚Üí 5000 ‚Üí 10000 req/s
‚îî‚îÄ Duration: Until failure or resource limits

Questions to answer:
‚îú‚îÄ What is maximum capacity? (10,000 req/s)
‚îú‚îÄ How does system fail? (Timeouts, errors, crashes)
‚îú‚îÄ Does system recover? (Auto-scaling, self-healing)
‚îî‚îÄ What is bottleneck? (Database, CPU, memory)

Example results:
‚îú‚îÄ 1000 req/s: Normal (p99 = 200ms, 0% errors)
‚îú‚îÄ 5000 req/s: Degraded (p99 = 800ms, 0.1% errors)
‚îú‚îÄ 10000 req/s: Overloaded (p99 = 3000ms, 5% errors)
‚îî‚îÄ 15000 req/s: Broken (p99 = 10000ms, 50% errors, crashes)

Bottleneck identified: Database connections maxed out
Solution: Connection pooling, read replicas
```

**Stress Test with k6:**

```javascript
// stress-test.js
import http from 'k6/http';
import { check } from 'k6';

export let options = {
  stages: [
    { duration: '2m', target: 100 },    // Warm up
    { duration: '5m', target: 100 },    // Baseline
    { duration: '2m', target: 200 },    // Scale up
    { duration: '5m', target: 200 },    // Observe
    { duration: '2m', target: 500 },    // Scale up more
    { duration: '5m', target: 500 },    // Observe
    { duration: '2m', target: 1000 },   // Push to limit
    { duration: '5m', target: 1000 },   // Breaking point
    { duration: '2m', target: 2000 },   // Beyond limit
    { duration: '5m', target: 2000 },   // Failure mode
    { duration: '5m', target: 0 },      // Recovery
  ],
};

export default function () {
  let res = http.get('https://api.example.com/');
  check(res, {
    'status is 200': (r) => r.status === 200,
  });
}

// Monitor:
// - At what user count does latency spike?
// - At what user count do errors start?
// - Does system recover after load decreases?
```

### 2.3 Soak Testing (Endurance Testing)

**Test system under sustained load**

```
Soak testing: Run at moderate load for extended period

Goal: Find memory leaks, resource exhaustion, degradation over time

Test scenarios:
‚îú‚îÄ Load: 70% of maximum capacity
‚îú‚îÄ Duration: 24-72 hours (or longer)
‚îî‚îÄ Constant load (no ramp up/down)

Issues to detect:
‚îú‚îÄ Memory leaks (memory grows over time, eventually OOM)
‚îú‚îÄ Disk space exhaustion (logs filling disk)
‚îú‚îÄ Connection leaks (database connections not closed)
‚îú‚îÄ Performance degradation (slows down over time)
‚îî‚îÄ Log file rotation (logs not rotated, disk full)

Example: Memory leak detection
Hour 0:  Memory usage: 2 GB
Hour 6:  Memory usage: 4 GB
Hour 12: Memory usage: 6 GB
Hour 18: Memory usage: 8 GB
Hour 24: Memory usage: 10 GB (approaching limit)

Diagnosis: Memory leak in request handler
Fix: Proper cleanup, garbage collection
```

**Soak Test with k6:**

```javascript
// soak-test.js
import http from 'k6/http';
import { check, sleep } from 'k6';

export let options = {
  stages: [
    { duration: '5m', target: 500 },    // Ramp up to 70% capacity
    { duration: '24h', target: 500 },   // Stay at 500 users for 24 hours
    { duration: '5m', target: 0 },      // Ramp down
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'],   // 95% requests < 500ms
    http_req_failed: ['rate<0.01'],     // Error rate < 1%
  },
};

export default function () {
  let res = http.get('https://api.example.com/');
  check(res, {
    'status is 200': (r) => r.status === 200,
  });
  sleep(1);
}

// Monitor over 24 hours:
// - Memory usage (should be stable)
// - Disk usage (should not grow indefinitely)
// - Response time (should not degrade)
// - Error rate (should remain low)
// - Database connections (should not leak)
```

---

## 3. Capacity Planning

### 3.1 Capacity Planning Loop

**Continuously plan and adjust capacity**

```
Capacity planning loop (iterative):

1. Measure:
   ‚îú‚îÄ Current traffic (requests/second)
   ‚îú‚îÄ Resource utilization (CPU, memory, disk)
   ‚îú‚îÄ Response times (latency)
   ‚îî‚îÄ Business metrics (users, transactions)

2. Forecast:
   ‚îú‚îÄ Expected growth (20% QoQ)
   ‚îú‚îÄ Seasonal peaks (Black Friday, holidays)
   ‚îú‚îÄ New features (marketing campaign)
   ‚îî‚îÄ Time horizon (3-6 months)

3. Plan:
   ‚îú‚îÄ Required capacity (instances, databases)
   ‚îú‚îÄ Budget allocation (cost estimates)
   ‚îú‚îÄ Timeline (when to scale)
   ‚îî‚îÄ Contingencies (burst capacity)

4. Execute:
   ‚îú‚îÄ Provision resources (scale up)
   ‚îú‚îÄ Test capacity (load tests)
   ‚îú‚îÄ Monitor (verify performance)
   ‚îî‚îÄ Optimize (right-size, autoscale)

5. Review:
   ‚îú‚îÄ Compare forecast vs actual
   ‚îú‚îÄ Adjust models (improve accuracy)
   ‚îî‚îÄ Lessons learned (incidents, outages)

Repeat quarterly or when significant changes occur
```

**Capacity Planning Example:**

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

def forecast_capacity():
    """Forecast capacity needs for next quarter"""
    
    # 1. Measure: Collect historical data
    df = pd.read_csv('metrics.csv')  # Date, requests_per_second, cpu_usage
    
    # 2. Forecast: Linear regression
    from sklearn.linear_model import LinearRegression
    
    # Prepare data
    df['timestamp'] = pd.to_datetime(df['date'])
    df['days'] = (df['timestamp'] - df['timestamp'].min()).dt.days
    
    X = df[['days']].values
    y = df['requests_per_second'].values
    
    # Train model
    model = LinearRegression()
    model.fit(X, y)
    
    # Forecast next 90 days
    future_days = np.array([[df['days'].max() + i] for i in range(1, 91)])
    forecast = model.predict(future_days)
    
    print("Capacity Forecast:")
    print(f"Current traffic: {y[-1]:.0f} req/s")
    print(f"Forecast (30 days): {forecast[29]:.0f} req/s")
    print(f"Forecast (60 days): {forecast[59]:.0f} req/s")
    print(f"Forecast (90 days): {forecast[89]:.0f} req/s")
    
    # 3. Plan: Calculate required instances
    current_instances = 10
    current_capacity = y[-1]
    forecast_capacity = forecast[89]
    
    growth_factor = forecast_capacity / current_capacity
    required_instances = int(current_instances * growth_factor)
    
    print(f"\nCapacity Planning:")
    print(f"Current instances: {current_instances}")
    print(f"Required instances (90 days): {required_instances}")
    print(f"Additional instances needed: {required_instances - current_instances}")
    
    # 4. Cost estimate
    instance_cost = 60  # $60/month per instance
    additional_cost = (required_instances - current_instances) * instance_cost
    
    print(f"\nCost Estimate:")
    print(f"Current cost: ${current_instances * instance_cost}/month")
    print(f"Forecast cost: ${required_instances * instance_cost}/month")
    print(f"Additional cost: ${additional_cost}/month")
    
    # 5. Recommendations
    print(f"\nRecommendations:")
    if growth_factor > 1.5:
        print("‚ö†Ô∏è  High growth expected, plan capacity expansion")
        print(f"   - Purchase Reserved Instances (save 40%)")
        print(f"   - Implement autoscaling (handle peaks)")
    elif growth_factor > 1.2:
        print("‚úì Moderate growth, gradual scaling recommended")
    else:
        print("‚úì Stable traffic, current capacity sufficient")
    
    return forecast

# Seasonal adjustment (Black Friday)
def adjust_for_seasonality(base_forecast, date):
    """Adjust forecast for seasonal events"""
    
    # Black Friday: 5x normal traffic
    if date.month == 11 and date.day >= 24 and date.day <= 26:
        return base_forecast * 5
    
    # Cyber Monday: 4x normal traffic
    if date.month == 11 and date.day >= 27 and date.day <= 29:
        return base_forecast * 4
    
    # Holiday season (Dec): 2x normal traffic
    if date.month == 12:
        return base_forecast * 2
    
    return base_forecast
```

### 3.2 Headroom Planning

**Reserve capacity for unexpected spikes**

```
Headroom: Extra capacity above expected peak

Example:
‚îú‚îÄ Expected peak: 5000 req/s
‚îú‚îÄ Headroom: 20% = 1000 req/s
‚îú‚îÄ Total capacity: 6000 req/s
‚îî‚îÄ Purpose: Handle unexpected spikes, failures

Headroom strategies:

1. Fixed percentage (simple):
   ‚îú‚îÄ Target: 70% utilization (30% headroom)
   ‚îî‚îÄ Example: If peak uses 70%, can handle 30% spike

2. N+1 redundancy (high availability):
   ‚îú‚îÄ N instances handle load
   ‚îú‚îÄ +1 instance for failure (one can fail, still operational)
   ‚îî‚îÄ Example: 5 instances (4 needed, 1 spare)

3. N+2 redundancy (critical systems):
   ‚îú‚îÄ +2 instances for failure (two can fail)
   ‚îî‚îÄ Example: 6 instances (4 needed, 2 spare)

4. Autoscaling + buffer:
   ‚îú‚îÄ Autoscale handles growth
   ‚îú‚îÄ Buffer for autoscaling lag (takes 5 minutes to provision)
   ‚îî‚îÄ Example: 10% buffer while new instances start

Trade-off:
‚îú‚îÄ More headroom = Higher cost, better reliability
‚îî‚îÄ Less headroom = Lower cost, higher risk
```

---

## Best Practices Summary

```
Cost Optimization:
‚úì Right-size instances (monitor utilization, adjust)
‚úì Autoscaling (scale up/down based on demand)
‚úì Reserved Instances for baseline (60-70% of capacity)
‚úì Spot Instances for batch jobs (90% discount)
‚úì Use CDN for static assets (reduce egress costs)
‚úì Compress responses (gzip, 70% size reduction)
‚úì Optimize APIs (return only needed data)
‚úì Tag resources (track costs by team, project)
‚úó Don't over-provision (87% waste common)
‚úó Don't ignore egress costs (can be highest cost)
‚úó Don't run dev/test 24/7 (stop when not needed)

Performance Testing:
‚úì Load testing (verify expected traffic)
‚úì Stress testing (find breaking point)
‚úì Soak testing (detect memory leaks, degradation)
‚úì Test in production-like environment (same size, config)
‚úì Monitor during tests (CPU, memory, latency)
‚úì Automate tests (run regularly, catch regressions)
‚úó Don't test in production (use staging)
‚úó Don't skip soak tests (memory leaks only appear over time)
‚úó Don't test single component (test entire system)

Capacity Planning:
‚úì Measure current usage (traffic, resources)
‚úì Forecast future needs (growth, seasonality)
‚úì Plan capacity expansion (timeline, budget)
‚úì Reserve headroom (20-30% for spikes)
‚úì Review forecasts (compare actual vs predicted)
‚úì Quarterly planning cycle (adjust as needed)
‚úó Don't wait until capacity exhausted (plan ahead)
‚úó Don't ignore seasonality (Black Friday, holidays)
‚úó Don't over-provision (waste money)

FinOps:
‚úì Visibility (cost dashboards, alerts)
‚úì Accountability (teams own their costs)
‚úì Optimization (continuous improvement)
‚úì Collaboration (engineers + finance)
‚úì Cost anomaly detection (alert on spikes)
‚úì Showback/chargeback (attribute costs to teams)
‚úó Don't let costs grow unchecked
‚úó Don't optimize once and forget (continuous)
‚úó Don't blame teams (provide tools, education)
```

Complete cost optimization and capacity planning guide! üí∞üìäüöÄ