# Search & Analysis

## What Is Search & Analysis?

**Fast full-text search and large-scale data analytics**

```
Traditional database search (LIKE query):
SELECT * FROM products WHERE name LIKE '%laptop%';
â”œâ”€ Scans entire table (slow)
â”œâ”€ No ranking (all matches equal)
â””â”€ Limited fuzzy matching

Search engine (Elasticsearch):
Query: "laptop"
â”œâ”€ Uses inverted index (fast lookup)
â”œâ”€ Relevance scoring (best matches first)
â”œâ”€ Fuzzy matching ("laptap" â†’ "laptop")
â”œâ”€ Faceted search (filters by brand, price)
â””â”€ Millisecond response (even billions of docs)

Analytics:
â”œâ”€ Aggregations (group by, stats, histograms)
â”œâ”€ Time-series analysis (logs, metrics)
â””â”€ Real-time dashboards (Kibana, Grafana)
```

---

## 1. Inverted Indexes

### 1.1 What Is an Inverted Index?

**Map from terms to documents (reverse of normal index)**

```
Documents:
Doc 1: "The quick brown fox"
Doc 2: "The lazy dog"
Doc 3: "Quick brown dogs"

Forward index (document â†’ words):
Doc 1: [the, quick, brown, fox]
Doc 2: [the, lazy, dog]
Doc 3: [quick, brown, dogs]

Inverted index (word â†’ documents):
"the":    [Doc 1, Doc 2]
"quick":  [Doc 1, Doc 3]
"brown":  [Doc 1, Doc 3]
"fox":    [Doc 1]
"lazy":   [Doc 2]
"dog":    [Doc 2]
"dogs":   [Doc 3]

Query: "quick brown"
â”œâ”€ Look up "quick" â†’ [Doc 1, Doc 3]
â”œâ”€ Look up "brown" â†’ [Doc 1, Doc 3]
â””â”€ Intersection â†’ [Doc 1, Doc 3]

Fast: O(1) lookup per term (vs O(n) table scan)
```

### 1.2 Index Structure (Detailed)

**Term dictionary + posting lists**

```
Term Dictionary (sorted):
"brown"  â†’ Pointer to posting list
"dog"    â†’ Pointer to posting list
"dogs"   â†’ Pointer to posting list
"fox"    â†’ Pointer to posting list
"lazy"   â†’ Pointer to posting list
"quick"  â†’ Pointer to posting list
"the"    â†’ Pointer to posting list

Posting List for "quick":
[Doc 1: position 1, Doc 3: position 0]
       (term frequency, field, position for phrase matching)

Posting List for "brown":
[Doc 1: position 2, Doc 3: position 1]

Enhanced posting list (full information):
Term "quick":
â”œâ”€ Doc 1: tf=1, positions=[1], field="title"
â””â”€ Doc 3: tf=1, positions=[0], field="body"

tf = term frequency (how many times term appears in doc)
positions = where term appears (for phrase queries)
field = which field contains term (title vs body)
```

**Storage Optimizations:**

```
1. Delta encoding (for document IDs):
   Original: [100, 105, 110, 115, 120]
   Delta:    [100, 5, 5, 5, 5]
   Saves space (smaller numbers compress better)

2. Variable-length encoding:
   Small numbers: 1 byte (0-127)
   Larger numbers: Multiple bytes
   Example: 5 â†’ [00000101] (1 byte)
           500 â†’ [10000011, 01110100] (2 bytes)

3. Compression (Roaring Bitmaps):
   Doc IDs [1, 2, 3, 100, 101, 102]
   â”œâ”€ Dense range [1-3]: Bitmap
   â””â”€ Sparse range [100-102]: Array
   
4. Skip lists (for fast intersection):
   Posting list: [1, 2, 3, ..., 1000, 1001, ...]
   Skip pointers: [1 â†’ 100 â†’ 500 â†’ 1000]
   Speeds up intersection of large lists
```

### 1.3 Text Analysis Pipeline

**Convert raw text to searchable terms**

```
Input: "The QUICK Brown Fox's!"

1. Character filters:
   HTML strip: "<p>The QUICK</p>" â†’ "The QUICK"
   Mapping: "Ã©" â†’ "e"

2. Tokenizer:
   Split on whitespace/punctuation:
   "The QUICK Brown Fox's!" â†’ ["The", "QUICK", "Brown", "Fox's"]

3. Token filters:
   a. Lowercase: ["the", "quick", "brown", "fox's"]
   b. Stop words: ["quick", "brown", "fox's"] (remove "the")
   c. Stemming: ["quick", "brown", "fox"] (fox's â†’ fox)
   
Final terms: ["quick", "brown", "fox"]

Indexed terms (inverted index):
"quick" â†’ [Doc 1]
"brown" â†’ [Doc 1]
"fox"   â†’ [Doc 1]
```

**Analyzers:**

```
Standard analyzer (default):
â”œâ”€ Tokenizer: Split on word boundaries
â”œâ”€ Lowercase filter
â””â”€ Stop word filter (optional)

English analyzer:
â”œâ”€ Standard tokenizer
â”œâ”€ Lowercase filter
â”œâ”€ Stop words (the, a, an, etc.)
â””â”€ Stemmer (running â†’ run, foxes â†’ fox)

Keyword analyzer (no tokenization):
Input: "john.doe@example.com"
Output: ["john.doe@example.com"] (single token)

Custom analyzer:
{
  "analysis": {
    "analyzer": {
      "my_analyzer": {
        "tokenizer": "standard",
        "filter": ["lowercase", "asciifolding", "my_stemmer"]
      }
    },
    "filter": {
      "my_stemmer": {
        "type": "stemmer",
        "language": "english"
      }
    }
  }
}
```

**Example Code:**

```python
# Elasticsearch analyzer example
from elasticsearch import Elasticsearch

es = Elasticsearch(['http://localhost:9200'])

# Create index with custom analyzer
es.indices.create(index='products', body={
    "settings": {
        "analysis": {
            "analyzer": {
                "product_analyzer": {
                    "tokenizer": "standard",
                    "filter": ["lowercase", "stop", "snowball"]
                }
            }
        }
    },
    "mappings": {
        "properties": {
            "name": {
                "type": "text",
                "analyzer": "product_analyzer"
            },
            "description": {
                "type": "text",
                "analyzer": "product_analyzer"
            },
            "sku": {
                "type": "keyword"  # No analysis, exact match
            }
        }
    }
})

# Test analyzer
result = es.indices.analyze(index='products', body={
    "analyzer": "product_analyzer",
    "text": "Running Shoes for Women"
})

# Output:
# [
#   {"token": "run", "position": 0},
#   {"token": "shoe", "position": 1},
#   {"token": "women", "position": 2}
# ]
# (stop word "for" removed, "Running" â†’ "run", "Shoes" â†’ "shoe")
```

### 1.4 Query Types

**Different ways to search the inverted index**

```
1. Term query (exact match):
   Query: {"term": {"status": "active"}}
   Looks up "active" in inverted index
   No analysis (case-sensitive, exact)

2. Match query (analyzed):
   Query: {"match": {"title": "Quick Brown"}}
   â”œâ”€ Analyze "Quick Brown" â†’ ["quick", "brown"]
   â”œâ”€ Look up "quick" and "brown"
   â””â”€ OR operation (docs with either term)

3. Match phrase (ordered terms):
   Query: {"match_phrase": {"title": "quick brown"}}
   â”œâ”€ Find docs with "quick" AND "brown"
   â””â”€ Verify adjacent positions (quick at pos 1, brown at pos 2)

4. Bool query (combine queries):
   {
     "bool": {
       "must": [{"match": {"title": "laptop"}}],
       "filter": [{"term": {"status": "in_stock"}}],
       "should": [{"term": {"brand": "apple"}}],
       "must_not": [{"term": {"category": "refurbished"}}]
     }
   }
   
   must: Required (affects score)
   filter: Required (no scoring, cacheable)
   should: Optional (boosts score)
   must_not: Exclude (no scoring)

5. Fuzzy query (typo tolerance):
   Query: {"fuzzy": {"title": "laptap"}}
   â”œâ”€ Edit distance â‰¤ 2 (Levenshtein)
   â””â”€ Matches: "laptop" (1 edit: pâ†’p, aâ†’o)

6. Wildcard query:
   Query: {"wildcard": {"name": "john*"}}
   Matches: "john", "johnson", "johnny"
   Warning: Expensive (prefix scan)

7. Prefix query (optimized):
   Query: {"prefix": {"name": "john"}}
   Uses term dictionary (sorted) for fast prefix scan
```

**Query Execution:**

```
Query: {"match": {"title": "quick brown fox"}}

Step 1: Analyze query
"quick brown fox" â†’ ["quick", "brown", "fox"]

Step 2: Look up posting lists
"quick": [Doc 1, Doc 3, Doc 7]
"brown": [Doc 1, Doc 3, Doc 5]
"fox":   [Doc 1, Doc 8]

Step 3: Merge (OR by default)
Union: [Doc 1, Doc 3, Doc 5, Doc 7, Doc 8]

Step 4: Score each document
Doc 1: Contains all 3 terms (high score)
Doc 3: Contains 2 terms (medium score)
Doc 5: Contains 1 term (low score)
...

Step 5: Sort by score
[Doc 1, Doc 3, Doc 5, Doc 7, Doc 8]

Step 6: Return top 10 (pagination)
```

---

## 2. Elasticsearch / OpenSearch

### 2.1 Architecture

**Distributed search and analytics engine**

```
Cluster:
â”œâ”€ Node 1 (master-eligible, data)
â”‚  â”œâ”€ Index "products" Shard 0 (primary)
â”‚  â””â”€ Index "logs" Shard 1 (replica)
â”œâ”€ Node 2 (data)
â”‚  â”œâ”€ Index "products" Shard 1 (primary)
â”‚  â””â”€ Index "logs" Shard 0 (replica)
â””â”€ Node 3 (data)
   â”œâ”€ Index "products" Shard 0 (replica)
   â””â”€ Index "logs" Shard 0 (primary)

Node roles:
â”œâ”€ Master: Cluster management (metadata, shard assignment)
â”œâ”€ Data: Store data, execute queries
â”œâ”€ Ingest: Pre-process documents
â”œâ”€ Coordinating: Route requests, merge results
â””â”€ ML: Machine learning jobs

Typical setup:
3 master-eligible nodes (quorum)
N data nodes (scale horizontally)
```

### 2.2 Sharding

**Split index into smaller pieces for parallelism**

```
Index "products" (10M documents):
â”œâ”€ Shard 0: Docs 0-2M
â”œâ”€ Shard 1: Docs 2M-4M
â”œâ”€ Shard 2: Docs 4M-6M
â”œâ”€ Shard 3: Docs 6M-8M
â””â”€ Shard 4: Docs 8M-10M

Benefits:
+ Parallel processing (5 shards â†’ 5Ã— throughput)
+ Scale beyond single node (distribute across cluster)
+ Balanced load (even distribution)

Shard sizing:
â”œâ”€ Goal: 10-50 GB per shard (optimal)
â”œâ”€ Too many: Overhead (metadata, merging)
â””â”€ Too few: Poor parallelism

Formula:
num_shards = total_data_size / 30GB (rounded up)

Example:
1 TB data â†’ 1000GB / 30GB â‰ˆ 34 shards
```

**Shard Routing:**

```
Document ID: "product-12345"
â”œâ”€ Hash document ID: hash("product-12345") = 1829304
â”œâ”€ Modulo by shard count: 1829304 % 5 = 4
â””â”€ Route to Shard 4

Result: Documents evenly distributed across shards

Custom routing:
Document: {"user_id": 123, "order_id": 456}
Route by: user_id (all orders for user on same shard)
â”œâ”€ Benefit: Co-located data (faster joins, aggregations)
â””â”€ Risk: Hot shards (popular users)
```

**Shard Allocation:**

```
Cluster state:
Node 1: Shard 0 (primary), Shard 2 (replica)
Node 2: Shard 1 (primary), Shard 0 (replica)
Node 3: Shard 2 (primary), Shard 1 (replica)

Allocation rules:
1. Primary and replica on different nodes (high availability)
2. Balance shards across nodes (even load)
3. Respect disk thresholds (85% watermark)
4. Shard allocation awareness (rack/zone)

Example:
Node 1 (zone: us-east-1a): Shard 0 (primary)
Node 2 (zone: us-east-1b): Shard 0 (replica)
â”œâ”€ Zone failure: us-east-1a down
â””â”€ Replica in us-east-1b promoted to primary
```

### 2.3 Replication

**Redundancy for high availability**

```
Replication setup:
Index "products":
â”œâ”€ number_of_shards: 5
â””â”€ number_of_replicas: 1

Total shards: 5 primary + 5 replica = 10 shards

Distribution:
Node 1: P0, R1, R2
Node 2: P1, R3, R4
Node 3: P2, R0
Node 4: P3, R1, R4
Node 5: P4, R2, R3

P = Primary, R = Replica

Benefit:
â”œâ”€ Fault tolerance (lose 1 node, data still available)
â”œâ”€ Read scalability (queries distributed to replicas)
â””â”€ Zero downtime (rolling upgrades)
```

**Write Path:**

```
Client â†’ Coordinating Node â†’ Primary Shard â†’ Replica Shards

Step-by-step:
1. Client: Index document {"id": 123, "title": "Laptop"}
2. Coordinating node: Route to Shard 2 (based on hash)
3. Coordinating node: Forward to Node 3 (primary Shard 2)
4. Node 3: Index document in primary
5. Node 3: Replicate to replicas (Shard 2 replicas on Node 1, Node 5)
6. Replicas: Acknowledge success
7. Primary: Acknowledge to coordinating node
8. Coordinating node: Respond to client (success)

Consistency level:
â”œâ”€ wait_for_active_shards=1 (primary only, fast)
â”œâ”€ wait_for_active_shards=2 (primary + 1 replica, safer)
â””â”€ wait_for_active_shards=all (all replicas, slowest)
```

**Read Path:**

```
Client â†’ Coordinating Node â†’ Shard (primary or replica)

Step-by-step:
1. Client: Search query {"match": {"title": "laptop"}}
2. Coordinating node: Determine shards (all 5 shards)
3. Coordinating node: Send query to each shard (P or R)
   â”œâ”€ Shard 0: Node 1 (replica) - selected (load balance)
   â”œâ”€ Shard 1: Node 2 (primary) - selected
   â”œâ”€ Shard 2: Node 3 (primary) - selected
   â”œâ”€ Shard 3: Node 4 (replica) - selected
   â””â”€ Shard 4: Node 5 (replica) - selected
4. Each shard: Execute query locally (top 10 results per shard)
5. Coordinating node: Merge results (sort by score)
6. Coordinating node: Fetch documents (top 10 overall)
7. Coordinating node: Return to client

Preference:
â”œâ”€ _primary: Always query primary (consistent)
â”œâ”€ _replica: Always query replica (offload primary)
â”œâ”€ _local: Prefer local node (reduce network)
â””â”€ _prefer_nodes: Specify nodes
```

### 2.4 Relevance Scoring

**Rank documents by relevance (TF-IDF, BM25)**

#### TF-IDF (Classic)

```
TF-IDF = Term Frequency Ã— Inverse Document Frequency

Term Frequency (TF):
How often term appears in document

TF(term, doc) = count(term in doc) / total_terms_in_doc

Example:
Doc 1: "laptop laptop computer" (3 words)
Term "laptop" appears 2 times
TF("laptop", Doc 1) = 2 / 3 = 0.67

Inverse Document Frequency (IDF):
How rare term is across all documents (rarer = more important)

IDF(term) = log(total_docs / docs_containing_term)

Example:
Total docs: 1000
Docs with "laptop": 100
IDF("laptop") = log(1000 / 100) = log(10) = 2.3

TF-IDF Score:
TF-IDF("laptop", Doc 1) = 0.67 Ã— 2.3 = 1.54

Intuition:
+ High TF: Term appears often in doc (relevant)
+ High IDF: Term rare overall (discriminative)
+ Both high: Strong signal of relevance
```

#### BM25 (Modern, Default)

```
BM25 = Improved TF-IDF with saturation

Formula:
BM25(term, doc) = IDF(term) Ã— 
                  (TF Ã— (k1 + 1)) / 
                  (TF + k1 Ã— (1 - b + b Ã— (doc_length / avg_doc_length)))

Parameters:
k1 = 1.2 (term frequency saturation, default)
b = 0.75 (length normalization, default)

Key differences from TF-IDF:
1. TF saturation: Diminishing returns for repeated terms
   TF=1 â†’ high boost
   TF=10 â†’ moderate boost (not 10Ã— better)
   
2. Length normalization: Penalize long documents
   Short doc with term â†’ higher score
   Long doc with term â†’ lower score (term less significant)

Example:
Doc 1: "laptop" (1 word, short)
Doc 2: "laptop laptop laptop ... [1000 words]" (long)

TF-IDF: Doc 2 scores higher (more "laptop")
BM25: Doc 1 scores higher (short, focused)

Intuition: BM25 better handles varying document lengths
```

**Scoring Example:**

```python
# Elasticsearch query with boosting
query = {
    "query": {
        "bool": {
            "should": [
                {
                    "match": {
                        "title": {
                            "query": "laptop",
                            "boost": 2.0  # Title 2Ã— more important
                        }
                    }
                },
                {
                    "match": {
                        "description": {
                            "query": "laptop",
                            "boost": 1.0
                        }
                    }
                }
            ]
        }
    }
}

# Custom scoring with function_score
query = {
    "query": {
        "function_score": {
            "query": {"match": {"title": "laptop"}},
            "functions": [
                {
                    "filter": {"term": {"brand": "apple"}},
                    "weight": 1.5  # Boost Apple products
                },
                {
                    "field_value_factor": {
                        "field": "popularity",
                        "factor": 1.2,
                        "modifier": "log1p"  # log(1 + popularity)
                    }
                },
                {
                    "gauss": {
                        "price": {
                            "origin": "500",
                            "scale": "100"
                        }
                    }
                }
            ],
            "score_mode": "multiply",  # Multiply all function scores
            "boost_mode": "multiply"   # Multiply with query score
        }
    }
}

# Result: score = BM25 Ã— brand_boost Ã— popularity_factor Ã— price_decay
```

**Explain API (Debug Scoring):**

```python
# Get detailed scoring breakdown
result = es.explain(index='products', id='123', body={
    "query": {"match": {"title": "laptop"}}
})

# Output:
# {
#   "matched": true,
#   "explanation": {
#     "value": 4.5,
#     "description": "sum of:",
#     "details": [
#       {
#         "value": 4.5,
#         "description": "weight(title:laptop in 123) [BM25]",
#         "details": [
#           {"value": 2.3, "description": "idf (docFreq=100, docCount=1000)"},
#           {"value": 1.95, "description": "tf (freq=2.0)"},
#           {"value": 1.0, "description": "fieldNorm"}
#         ]
#       }
#     ]
#   }
# }
```

### 2.5 Aggregations

**Group and analyze data (like SQL GROUP BY)**

```
1. Metric aggregations (calculate values):
   â”œâ”€ avg, sum, min, max, stats
   â”œâ”€ cardinality (unique count)
   â””â”€ percentiles

2. Bucket aggregations (group documents):
   â”œâ”€ terms (group by field value)
   â”œâ”€ range (numeric/date ranges)
   â”œâ”€ histogram (bucket by interval)
   â””â”€ date_histogram (time buckets)

3. Pipeline aggregations (aggregate on aggregations):
   â”œâ”€ moving_avg (moving average)
   â”œâ”€ derivative (rate of change)
   â””â”€ cumulative_sum
```

**Examples:**

```python
# Terms aggregation (top brands)
query = {
    "size": 0,  # No documents, only aggregation
    "aggs": {
        "brands": {
            "terms": {
                "field": "brand.keyword",
                "size": 10
            }
        }
    }
}

# Result:
# {
#   "aggregations": {
#     "brands": {
#       "buckets": [
#         {"key": "apple", "doc_count": 1500},
#         {"key": "dell", "doc_count": 1200},
#         {"key": "hp", "doc_count": 900}
#       ]
#     }
#   }
# }

# Nested aggregation (average price per brand)
query = {
    "aggs": {
        "brands": {
            "terms": {"field": "brand.keyword"},
            "aggs": {
                "avg_price": {
                    "avg": {"field": "price"}
                }
            }
        }
    }
}

# Result:
# {
#   "aggregations": {
#     "brands": {
#       "buckets": [
#         {
#           "key": "apple",
#           "doc_count": 1500,
#           "avg_price": {"value": 1200.50}
#         },
#         {
#           "key": "dell",
#           "doc_count": 1200,
#           "avg_price": {"value": 850.25}
#         }
#       ]
#     }
#   }
# }

# Date histogram (sales over time)
query = {
    "aggs": {
        "sales_over_time": {
            "date_histogram": {
                "field": "created_at",
                "calendar_interval": "1d"
            },
            "aggs": {
                "total_sales": {
                    "sum": {"field": "amount"}
                }
            }
        }
    }
}

# Range aggregation (price buckets)
query = {
    "aggs": {
        "price_ranges": {
            "range": {
                "field": "price",
                "ranges": [
                    {"to": 100},
                    {"from": 100, "to": 500},
                    {"from": 500, "to": 1000},
                    {"from": 1000}
                ]
            }
        }
    }
}
```

**Faceted Search (E-commerce):**

```python
# Multi-facet aggregation
query = {
    "query": {
        "match": {"name": "laptop"}
    },
    "aggs": {
        "brands": {
            "terms": {"field": "brand.keyword", "size": 20}
        },
        "price_ranges": {
            "range": {
                "field": "price",
                "ranges": [
                    {"key": "under_500", "to": 500},
                    {"key": "500_1000", "from": 500, "to": 1000},
                    {"key": "over_1000", "from": 1000}
                ]
            }
        },
        "ratings": {
            "terms": {"field": "rating", "size": 5}
        },
        "in_stock": {
            "terms": {"field": "in_stock"}
        }
    }
}

# Result: Facets for filtering
# Brand: Apple (150), Dell (120), HP (90)
# Price: <$500 (200), $500-1000 (100), >$1000 (60)
# Rating: 5â˜… (80), 4â˜… (120), 3â˜… (90)
# In Stock: Yes (250), No (110)

# User clicks "Apple" filter â†’ re-query with filter
query = {
    "query": {
        "bool": {
            "must": [{"match": {"name": "laptop"}}],
            "filter": [{"term": {"brand.keyword": "apple"}}]
        }
    },
    "aggs": {
        # Same aggregations (facets update)
    }
}
```

### 2.6 Performance Optimization

```
1. Index optimization:
   â”œâ”€ Refresh interval: 30s (vs 1s default, reduce overhead)
   â”œâ”€ Merge policy: Fewer segments (faster search)
   â”œâ”€ Force merge: Optimize read-heavy indexes
   â””â”€ Disable replicas during bulk indexing

2. Query optimization:
   â”œâ”€ Filter context (cacheable, no scoring)
   â”œâ”€ Prefix queries â†’ wildcard (avoid leading wildcards)
   â”œâ”€ Scroll API for deep pagination (vs from/size)
   â””â”€ Point in Time (PIT) for consistent pagination

3. Shard sizing:
   â”œâ”€ 10-50 GB per shard (optimal)
   â”œâ”€ Over-sharding: Metadata overhead
   â””â”€ Under-sharding: Poor parallelism

4. Hardware:
   â”œâ”€ SSD (vs HDD, 10Ã— faster)
   â”œâ”€ RAM: 50% for heap, 50% for filesystem cache
   â”œâ”€ CPU: More cores (parallel queries)
   â””â”€ Network: Low latency (cross-node communication)

5. Monitoring:
   â”œâ”€ Query latency (p95, p99)
   â”œâ”€ Indexing rate (docs/sec)
   â”œâ”€ JVM heap usage (<75%)
   â”œâ”€ Disk usage (<85%)
   â””â”€ Thread pool queue (reject if full)
```

---

## 3. Batch vs Streaming

### 3.1 Batch Processing

**Process large volumes of data in scheduled jobs**

```
Characteristics:
â”œâ”€ Periodic execution (hourly, daily)
â”œâ”€ Large data volumes (GB to TB per job)
â”œâ”€ High latency (minutes to hours)
â””â”€ High throughput (millions of records)

Example: Daily sales report
â”œâ”€ Schedule: Run at midnight
â”œâ”€ Input: All sales from previous day (millions of rows)
â”œâ”€ Processing: Aggregate by region, product, etc.
â””â”€ Output: Report dashboard (updated once per day)

Tools:
â”œâ”€ Hadoop MapReduce
â”œâ”€ Apache Spark (batch mode)
â”œâ”€ AWS Batch, Google Dataflow
â””â”€ dbt (data transformation)
```

**MapReduce Example:**

```
Task: Count words in 100 GB of text files

Map phase (parallel):
Input: Text file chunks
Map function: word â†’ (word, 1)

File 1: "hello world" â†’ [("hello", 1), ("world", 1)]
File 2: "hello there" â†’ [("hello", 1), ("there", 1)]
...

Shuffle & Sort:
Group by key:
"hello": [(1), (1)]
"world": [(1)]
"there": [(1)]

Reduce phase (parallel):
Reduce function: (word, [counts]) â†’ (word, sum)

"hello": [1, 1] â†’ ("hello", 2)
"world": [1] â†’ ("world", 1)
"there": [1] â†’ ("there", 1)

Output: Word counts
```

**Spark Batch Job:**

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SalesReport").getOrCreate()

# Read data (batch)
sales = spark.read.parquet("s3://data/sales/2024-01-01/*.parquet")

# Transform
report = sales.groupBy("region", "product") \
              .agg({"amount": "sum", "quantity": "sum"}) \
              .orderBy("region")

# Write output
report.write.parquet("s3://reports/daily/2024-01-01/")

spark.stop()
```

### 3.2 Stream Processing

**Process data in real-time as it arrives**

```
Characteristics:
â”œâ”€ Continuous execution (always running)
â”œâ”€ Small data volumes per event (KB to MB)
â”œâ”€ Low latency (milliseconds to seconds)
â””â”€ Lower throughput (thousands to millions events/sec)

Example: Real-time fraud detection
â”œâ”€ Input: Transaction events (streaming)
â”œâ”€ Processing: Check against rules (real-time)
â””â”€ Output: Alert if fraud detected (immediately)

Tools:
â”œâ”€ Apache Kafka Streams
â”œâ”€ Apache Flink
â”œâ”€ Apache Spark Streaming
â””â”€ AWS Kinesis, Google Dataflow
```

**Stream Processing Example:**

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import window, col

spark = SparkSession.builder.appName("FraudDetection").getOrCreate()

# Read stream (Kafka)
transactions = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "transactions") \
    .load()

# Parse JSON
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, DoubleType

schema = StructType([
    StructField("user_id", StringType()),
    StructField("amount", DoubleType()),
    StructField("merchant", StringType()),
    StructField("timestamp", StringType())
])

parsed = transactions.select(
    from_json(col("value").cast("string"), schema).alias("data")
).select("data.*")

# Detect fraud (amount > $10,000)
fraud = parsed.filter(col("amount") > 10000)

# Write to alert stream
query = fraud.writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("topic", "fraud-alerts") \
    .option("checkpointLocation", "s3://checkpoints/fraud/") \
    .start()

query.awaitTermination()
```

**Windowing (Time-based Aggregations):**

```python
# Tumbling window (non-overlapping, fixed size)
windowed = transactions \
    .groupBy(
        window("timestamp", "1 minute"),  # 1-minute windows
        "user_id"
    ) \
    .agg({"amount": "sum"})

# Result:
# Window [12:00:00 - 12:01:00], user_id=123, sum=500
# Window [12:01:00 - 12:02:00], user_id=123, sum=750

# Sliding window (overlapping)
windowed = transactions \
    .groupBy(
        window("timestamp", "5 minutes", "1 minute"),  # 5-min window, 1-min slide
        "user_id"
    ) \
    .agg({"amount": "sum"})

# Result:
# Window [12:00:00 - 12:05:00], user_id=123, sum=2000
# Window [12:01:00 - 12:06:00], user_id=123, sum=2100
# (overlapping windows)

# Session window (gap-based)
windowed = transactions \
    .groupBy(
        session_window("timestamp", "10 minutes"),  # 10-min inactivity gap
        "user_id"
    ) \
    .agg({"amount": "sum"})

# Result: Groups events until 10-min gap (user session)
```

### 3.3 Lambda Architecture

**Batch + streaming for comprehensive analytics**

```
Architecture:
                 Data Source
                      â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                       â–¼
     Batch Layer            Speed Layer
   (historical data)      (real-time data)
          â”‚                       â”‚
          â–¼                       â–¼
     Batch Views            Real-time Views
          â”‚                       â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
                Serving Layer
                      â”‚
                      â–¼
                   Queries

Layers:
1. Batch Layer:
   â”œâ”€ Process all historical data (immutable)
   â”œâ”€ Recompute views from scratch (accurate)
   â””â”€ High latency (hours)

2. Speed Layer:
   â”œâ”€ Process recent data (last few hours)
   â”œâ”€ Incremental updates (fast)
   â””â”€ Low latency (seconds)

3. Serving Layer:
   â”œâ”€ Merge batch + real-time views
   â”œâ”€ Query interface (API)
   â””â”€ Serve results

Example: Analytics dashboard
â”œâ”€ Batch: Daily aggregations (yesterday and older)
â”œâ”€ Speed: Real-time aggregations (today)
â””â”€ Serving: Merge both (complete view)
```

**Pros & Cons:**

```
Pros:
+ Accuracy (batch reprocessing corrects errors)
+ Fault tolerance (batch layer recovers from failures)
+ Low latency (speed layer provides real-time)

Cons:
- Complexity (maintain two codebases: batch + streaming)
- Data duplication (same data in both layers)
- Eventually consistent (speed layer lags batch)

When to use:
âœ“ Need both accuracy (batch) and speed (streaming)
âœ“ Complex analytics (joins, aggregations)
âœ“ Tolerate eventual consistency
```

### 3.4 Kappa Architecture

**Streaming-only (no batch layer)**

```
Architecture:
     Data Source
          â”‚
          â–¼
    Stream Processor (Kafka Streams, Flink)
          â”‚
          â–¼
     Materialized Views
          â”‚
          â–¼
       Queries

Key difference from Lambda:
â”œâ”€ No separate batch layer
â”œâ”€ All processing in stream (even historical)
â””â”€ Reprocess by replaying stream from beginning

Reprocessing:
1. Historical data in Kafka (long retention)
2. Deploy new stream processor
3. Process from offset 0 (replay all data)
4. Compute new views (same as batch, but streaming)

Benefits:
+ Simpler (one codebase)
+ Unified processing model (stream-only)
+ Easier to maintain

Trade-offs:
- Stream processing more complex than batch
- Requires long retention (expensive)
- Slower reprocessing (stream-based)

When to use:
âœ“ Real-time requirements
âœ“ Event-driven architecture
âœ“ Tolerate streaming complexity
âœ— Don't need complex batch analytics
```

**Kappa Example:**

```python
# Kafka Streams (Kappa architecture)
from kafka import KafkaConsumer, KafkaProducer

consumer = KafkaConsumer('events', bootstrap_servers=['localhost:9092'])
producer = KafkaProducer(bootstrap_servers=['localhost:9092'])

# State store (materialized view)
user_totals = {}

for message in consumer:
    event = json.loads(message.value)
    user_id = event['user_id']
    amount = event['amount']
    
    # Update materialized view
    user_totals[user_id] = user_totals.get(user_id, 0) + amount
    
    # Publish updated view
    producer.send('user-totals', value=json.dumps({
        'user_id': user_id,
        'total': user_totals[user_id]
    }))

# Reprocessing: Reset offset to 0, replay all events
# consumer.seek_to_beginning()
```

### 3.5 ETL vs ELT

**Extract, Transform, Load vs Extract, Load, Transform**

#### ETL (Traditional)

```
Flow:
1. Extract: Read from source (databases, APIs, files)
2. Transform: Clean, aggregate, join (in ETL tool)
3. Load: Write to data warehouse (transformed data)

Example:
1. Extract: Sales data from PostgreSQL
2. Transform: Join with customers, aggregate by region
3. Load: Insert into Redshift (aggregated tables)

Tools: Informatica, Talend, Apache NiFi

Pros:
+ Transform before loading (less data in warehouse)
+ Clean data (validated before storage)
+ Sensitive data filtering (remove PII before storage)

Cons:
- ETL tool bottleneck (limited compute)
- Slow (transform before load)
- Less flexible (transform logic in ETL tool, not warehouse)
```

#### ELT (Modern)

```
Flow:
1. Extract: Read from source
2. Load: Write raw data to warehouse (no transformation)
3. Transform: Transform in warehouse (SQL, dbt)

Example:
1. Extract: Sales data from PostgreSQL
2. Load: Copy raw data to Snowflake
3. Transform: SQL queries in Snowflake (CREATE TABLE AS SELECT)

Tools: Fivetran, Stitch, dbt

Pros:
+ Fast loading (raw data, no transformation)
+ Scalable (warehouse compute for transformation)
+ Flexible (change transforms without re-extracting)
+ Data lineage (raw data always available)

Cons:
- More data in warehouse (raw + transformed)
- Requires powerful warehouse (BigQuery, Snowflake)
- PII exposure (raw data includes sensitive fields)

When to use:
âœ“ Cloud data warehouse (cheap storage, fast compute)
âœ“ Frequent schema changes
âœ“ Need raw data for debugging
âœ— Limited warehouse compute
âœ— Strict PII requirements (transform before load)
```

**dbt Example (ELT Transform):**

```sql
-- models/staging/stg_orders.sql
-- Stage 1: Load raw data (already in warehouse)

{{ config(materialized='view') }}

SELECT
    order_id,
    customer_id,
    order_date,
    total_amount
FROM raw.orders

-- models/marts/fct_orders.sql
-- Stage 2: Transform in warehouse

{{ config(materialized='table') }}

WITH orders AS (
    SELECT * FROM {{ ref('stg_orders') }}
),

customers AS (
    SELECT * FROM {{ ref('stg_customers') }}
)

SELECT
    o.order_id,
    o.customer_id,
    c.customer_name,
    o.order_date,
    o.total_amount,
    c.region
FROM orders o
JOIN customers c ON o.customer_id = c.customer_id

-- dbt run: Transform in warehouse (Snowflake, BigQuery, etc.)
```

### 3.6 Data Lake vs Data Warehouse

```
Data Lake:
â”œâ”€ Schema-on-read (raw files: JSON, Parquet, CSV)
â”œâ”€ Cheap storage (S3, HDFS)
â”œâ”€ All data (structured, semi-structured, unstructured)
â”œâ”€ Flexible (no predefined schema)
â””â”€ Use case: Data science, ML, exploratory analysis

Data Warehouse:
â”œâ”€ Schema-on-write (tables with defined schema)
â”œâ”€ Expensive storage (optimized for queries)
â”œâ”€ Structured data only (relational tables)
â”œâ”€ Rigid (schema changes require migration)
â””â”€ Use case: BI reports, dashboards, SQL analytics

Comparison:
| Feature | Data Lake | Data Warehouse |
|---------|-----------|----------------|
| **Schema** | Flexible (schema-on-read) | Rigid (schema-on-write) |
| **Data** | Raw (JSON, Parquet, logs) | Structured (tables) |
| **Storage** | Cheap (S3: $0.023/GB/month) | Expensive (optimized) |
| **Query** | Slower (scan files) | Fast (indexed tables) |
| **Users** | Data scientists, engineers | Business analysts |
| **Tools** | Spark, Presto, Athena | Redshift, Snowflake, BigQuery |

Lakehouse (Hybrid):
â”œâ”€ Delta Lake, Apache Iceberg, Hudi
â”œâ”€ ACID transactions on data lake
â”œâ”€ Schema enforcement (like warehouse)
â”œâ”€ Cheap storage (like lake)
â””â”€ Best of both worlds
```

**Data Lake Example:**

```python
# Read raw JSON files from S3 (data lake)
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("DataLake").getOrCreate()

# Read raw data (no predefined schema)
raw_logs = spark.read.json("s3://data-lake/logs/2024-01-01/*.json")

# Infer schema on read
raw_logs.printSchema()
# root
#  |-- user_id: string
#  |-- event: string
#  |-- timestamp: string

# Query (ad-hoc)
result = raw_logs.filter("event = 'purchase'") \
                 .groupBy("user_id") \
                 .count()

# Write to Parquet (optimized for queries)
result.write.parquet("s3://data-lake/processed/purchases/")
```

**Data Warehouse Example:**

```sql
-- Create table (schema defined upfront)
CREATE TABLE sales (
    order_id INT PRIMARY KEY,
    customer_id INT,
    order_date DATE,
    total_amount DECIMAL(10, 2)
);

-- Load data (must match schema)
COPY sales FROM 's3://data/sales.csv'
CREDENTIALS '...'
CSV;

-- Query (fast, indexed)
SELECT customer_id, SUM(total_amount)
FROM sales
WHERE order_date >= '2024-01-01'
GROUP BY customer_id;

-- Performance: Fast (indexes, columnar storage)
```

---

## Best Practices Summary

```
Elasticsearch / OpenSearch:
âœ“ Size shards 10-50 GB (optimal performance)
âœ“ Use replicas for HA (1-2 replicas typical)
âœ“ Filter context for non-scoring queries (cacheable)
âœ“ Use keyword type for exact match (no analysis)
âœ“ Monitor JVM heap (<75% usage)
âœ“ Use scroll/PIT for deep pagination (not from/size)
âœ“ Bulk indexing (batch documents, 1000-5000/batch)
âœ“ Refresh interval 30s for write-heavy (vs 1s default)
âœ— Don't over-shard (metadata overhead)
âœ— Don't use leading wildcards (expensive)
âœ— Don't fetch all fields (use _source filtering)

Inverted Indexes:
âœ“ Choose analyzer based on use case (standard, English, keyword)
âœ“ Use BM25 for relevance (default, better than TF-IDF)
âœ“ Boost important fields (title > description)
âœ“ Use explain API for debugging scores
âœ“ Normalize text (lowercase, stemming) for better recall
âœ— Don't over-analyze (too aggressive stemming loses meaning)
âœ— Don't index unnecessary fields (storage overhead)

Batch Processing:
âœ“ Use batch for large historical data (GB-TB)
âœ“ Schedule during off-peak hours (reduce load)
âœ“ Partition data (process in parallel)
âœ“ Use columnar formats (Parquet, ORC) for analytics
âœ“ Idempotent jobs (safe to rerun)
âœ“ Monitor job duration (alert on degradation)
âœ— Don't batch real-time requirements (use streaming)
âœ— Don't process entire dataset if incremental possible

Stream Processing:
âœ“ Use streaming for real-time (<1 min latency)
âœ“ Checkpoint state (recover from failures)
âœ“ Handle late data (watermarks, allowed lateness)
âœ“ Idempotent processing (handle duplicates)
âœ“ Monitor lag (consumer offset behind producer)
âœ“ Backpressure handling (rate limiting, buffering)
âœ— Don't use streaming for simple batch (overkill)
âœ— Don't ignore out-of-order events (use event time)

Lambda vs Kappa:
âœ“ Use Lambda for complex analytics (batch + real-time)
âœ“ Use Kappa for event-driven (stream-only)
âœ“ Kappa requires long retention (Kafka: days to weeks)
âœ“ Lambda tolerates eventual consistency
âœ— Don't use Lambda if can avoid (complexity)
âœ— Don't use Kappa without stream processing expertise

ETL vs ELT:
âœ“ Use ELT for cloud warehouses (Snowflake, BigQuery)
âœ“ Use ETL for on-premise or limited warehouse compute
âœ“ ELT keeps raw data (data lineage, debugging)
âœ“ dbt for ELT transformations (SQL-based, version control)
âœ— Don't use ETL for cloud (underutilizes warehouse)
âœ— Don't expose PII in ELT without masking

Data Lake vs Warehouse:
âœ“ Lake for raw data (cheap storage, flexible schema)
âœ“ Warehouse for analytics (fast queries, BI tools)
âœ“ Use lakehouse (Delta Lake) for ACID on lake
âœ“ Partition data (date, region) for pruning
âœ“ Compress data (Snappy, ZSTD) for storage savings
âœ— Don't query lake directly for BI (slow, use warehouse)
âœ— Don't ignore data catalog (track lake metadata)
```

Complete search and analysis foundation! ðŸ”ðŸ“Š