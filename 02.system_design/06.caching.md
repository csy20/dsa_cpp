# Caching

## What is Caching?

**Store frequently accessed data in fast storage to reduce latency and load**

```
Without cache:
Client â†’ Database (slow, 50ms)

With cache:
Client â†’ Cache (fast, 1ms) â†’ Hit âœ“ Return data
       â†’ Cache â†’ Miss â†’ Database (50ms) â†’ Store in cache â†’ Return data
```

**Benefits:**
```
+ Reduced latency (1ms vs 50ms = 50Ã— faster)
+ Lower database load (fewer queries)
+ Higher throughput (serve more requests)
+ Cost savings (cheaper than scaling DB)
+ Better user experience (faster responses)
```

**Cache Hit Ratio:**
```
Hit Ratio = Cache Hits / Total Requests

Example:
1000 requests:
â”œâ”€ 950 cache hits (1ms each)
â””â”€ 50 cache misses (50ms each)

Hit ratio = 950/1000 = 95%

Average latency:
(950 Ã— 1ms + 50 Ã— 50ms) / 1000 = 3.45ms

Without cache: 50ms
With cache (95% hit): 3.45ms (14.5Ã— faster!)
```

---

## 1. Cache Levels (Layers)

```
Client â†’ Browser Cache
       â†“ (miss)
       â†’ CDN/Edge Cache
       â†“ (miss)
       â†’ Reverse Proxy Cache (NGINX)
       â†“ (miss)
       â†’ Application Cache (Redis)
       â†“ (miss)
       â†’ Database

Each layer caches different things, bringing data closer to user
```

### 1.1 Client-Side Cache (Browser)

**What it caches:** HTML, CSS, JS, images, API responses

**How it works:**
```
First visit to website:
1. Browser: GET /style.css
2. Server: Here's style.css
   Cache-Control: max-age=3600, public
3. Browser: Store in cache for 3600 seconds (1 hour)

Second visit (within 1 hour):
1. Browser: Do I have /style.css cached?
2. Browser: Yes! Use cached version (instant, 0ms)
3. No network request needed!
```

**Cache-Control Headers:**
```http
Cache-Control: max-age=3600
â”œâ”€ Cache for 3600 seconds (1 hour)

Cache-Control: max-age=86400, public
â”œâ”€ Cache for 86400 seconds (1 day)
â”œâ”€ public: Can be cached by CDN/proxies

Cache-Control: max-age=3600, private
â”œâ”€ private: Only browser can cache (not CDN)
â”œâ”€ Use for user-specific data

Cache-Control: no-cache
â”œâ”€ Revalidate before using cached version
â”œâ”€ Browser checks: If-None-Modified

Cache-Control: no-store
â”œâ”€ Don't cache at all
â”œâ”€ Use for sensitive data (credit cards)

Cache-Control: max-age=3600, must-revalidate
â”œâ”€ After 3600s, must check with server
â”œâ”€ Can't use stale cache
```

**ETag & Conditional Requests:**
```
First request:
GET /api/users/123
Response:
ETag: "abc123xyz"
Cache-Control: max-age=60
{user data}

After 60 seconds (expired):
GET /api/users/123
If-None-Match: "abc123xyz"  â† Send ETag

Server checks: Has data changed?
â”œâ”€ No change: 304 Not Modified (no body, fast!)
â””â”€ Changed: 200 OK with new data and new ETag

304 response is small (just headers), saves bandwidth
```

**Best Practices:**
```
Static assets (images, CSS, JS):
Cache-Control: max-age=31536000, immutable  (1 year)
Filename versioning: style.v123.css

API responses:
Cache-Control: max-age=60, private

HTML pages:
Cache-Control: no-cache  (always revalidate)

User-specific data:
Cache-Control: private, max-age=300

Sensitive data:
Cache-Control: no-store, no-cache, must-revalidate
```

### 1.2 CDN (Content Delivery Network)

**What it caches:** Static assets, media files, API responses

**How it works:**
```
CDN has servers worldwide (edge locations):

User in New York:
1. Request: GET /image.jpg
2. CDN edge (NYC): Do I have this?
   â”œâ”€ Yes: Return cached copy (fast, 10ms)
   â””â”€ No: Fetch from origin server (slow, 100ms)
       â†’ Cache at edge
       â†’ Return to user

User in London:
1. Request: GET /image.jpg
2. CDN edge (London): Do I have this?
   â”œâ”€ Yes: Return cached copy (fast, 10ms)
   â””â”€ No: Fetch from origin (slow, 200ms)
       â†’ Cache at edge
       â†’ Return to user

Each edge location caches independently
Users routed to nearest edge (anycast)
```

**CDN Architecture:**
```
                    Origin Server
                    (Your backend)
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
   [CDN Edge NYC]   [CDN Edge London]  [CDN Edge Tokyo]
   Caches: 50%      Caches: 60%        Caches: 40%
        â”‚                 â”‚                 â”‚
   Users in US      Users in EU        Users in Asia

Popular content cached at all edges
Unpopular content cached on-demand
```

**CDN Cache Configuration:**

**Cloudflare:**
```
Cache rules:
â”œâ”€ /static/*: Cache 1 year
â”œâ”€ /api/*: Cache 5 minutes
â”œâ”€ /user/*: Don't cache (user-specific)
â””â”€ /*.jpg, /*.png: Cache 1 month

Cache-Control from origin:
Cache-Control: max-age=3600, s-maxage=86400
â”œâ”€ max-age=3600: Browser caches 1 hour
â””â”€ s-maxage=86400: CDN caches 1 day (different TTL!)
```

**Cache Purging/Invalidation:**
```
Problem: Updated content, but CDN has old version

Solutions:
1. Wait for TTL to expire (simple, but slow)
2. Purge cache (manual or API):
   - Purge by URL: /api/users/123
   - Purge by tag: tag:user-123
   - Purge all: /* (expensive, avoid)
3. Versioned URLs: /v2/api/users/123
```

**Benefits:**
```
+ Low latency (10-50ms vs 100-300ms)
+ Reduced origin load (90%+ offloaded to CDN)
+ DDoS protection (attack distributed across edges)
+ Global reach (consistent performance worldwide)
+ Cost savings (cheaper bandwidth from CDN)

Drawbacks:
- Extra cost (CDN service fees)
- Cache invalidation complexity
- Cold start (first request to each edge is slow)
```

**Popular CDN Providers:**
```
Cloudflare:
â”œâ”€ Free tier available
â”œâ”€ Global network (300+ cities)
â”œâ”€ DDoS protection included
â””â”€ Easy setup

AWS CloudFront:
â”œâ”€ Integrates with AWS services
â”œâ”€ Pay-as-you-go pricing
â”œâ”€ 400+ edge locations
â””â”€ Lambda@Edge (run code at edge)

Fastly:
â”œâ”€ Real-time purging (150ms)
â”œâ”€ Instant configuration updates
â”œâ”€ Great for dynamic content
â””â”€ Higher cost

Akamai:
â”œâ”€ Largest network
â”œâ”€ Enterprise-grade
â”œâ”€ Very expensive
â””â”€ Best performance
```

### 1.3 Reverse Proxy Cache (NGINX, Varnish)

**What it caches:** API responses, rendered pages, dynamic content

**How it works:**
```
Client â†’ Reverse Proxy (NGINX) â†’ Backend Servers

Request: GET /api/products/123
NGINX:
â”œâ”€ Check cache: cache_key = "GET/api/products/123"
â”œâ”€ Hit: Return cached response (1ms)
â””â”€ Miss: Forward to backend â†’ Cache response â†’ Return

Next request for same URL:
NGINX: Cache hit! Return immediately (no backend call)
```

**NGINX Caching Configuration:**
```nginx
# Define cache zone
proxy_cache_path /var/cache/nginx 
    levels=1:2 
    keys_zone=my_cache:10m 
    max_size=1g 
    inactive=60m 
    use_temp_path=off;

server {
    location /api/ {
        proxy_cache my_cache;
        proxy_cache_key "$scheme$request_method$host$request_uri";
        
        # Cache 200 responses for 10 minutes
        proxy_cache_valid 200 10m;
        
        # Cache 404 for 1 minute
        proxy_cache_valid 404 1m;
        
        # Don't cache errors
        proxy_cache_valid 500 502 503 504 0;
        
        # Bypass cache for certain conditions
        proxy_cache_bypass $http_pragma $http_authorization;
        
        # Add cache status header
        add_header X-Cache-Status $upstream_cache_status;
        
        proxy_pass http://backend;
    }
    
    location /static/ {
        proxy_cache my_cache;
        proxy_cache_valid 200 1d;  # Cache static files for 1 day
        proxy_pass http://backend;
    }
}

# X-Cache-Status values: HIT, MISS, BYPASS, EXPIRED, STALE, UPDATING, REVALIDATED
```

**Varnish Cache:**
```vcl
vcl 4.1;

backend default {
    .host = "backend.example.com";
    .port = "8080";
}

sub vcl_recv {
    # Don't cache POST requests
    if (req.method == "POST") {
        return (pass);
    }
    
    # Don't cache authenticated requests
    if (req.http.Authorization) {
        return (pass);
    }
    
    # Remove cookies for static files
    if (req.url ~ "\.(jpg|jpeg|png|css|js)$") {
        unset req.http.Cookie;
    }
    
    return (hash);
}

sub vcl_backend_response {
    # Cache for 5 minutes by default
    set beresp.ttl = 5m;
    
    # Cache static files for 1 hour
    if (bereq.url ~ "\.(jpg|jpeg|png|css|js)$") {
        set beresp.ttl = 1h;
    }
    
    # Don't cache errors
    if (beresp.status >= 500) {
        set beresp.ttl = 0s;
    }
}
```

**Cache Bypass:**
```
Force cache miss for specific requests:

Bypass by header:
GET /api/users/123
Cache-Control: no-cache  â† Force backend query

Bypass by query parameter:
GET /api/users/123?nocache=1  â† Force backend query

Bypass by cookie:
GET /api/users/123
Cookie: admin=true  â† Bypass cache for admins

Use cases:
â”œâ”€ Admin users (always fresh data)
â”œâ”€ Debugging (see backend behavior)
â””â”€ Cache warming (pre-populate cache)
```

### 1.4 Application Cache (Redis, Memcached)

**What it caches:** Query results, computed data, session data, objects

**Redis vs Memcached:**

| Feature | Redis | Memcached |
|---------|-------|-----------|
| **Data Structures** | String, List, Set, Hash, Sorted Set | String only |
| **Persistence** | Yes (RDB, AOF) | No (in-memory only) |
| **Replication** | Yes (master-replica) | No (client-side) |
| **Transactions** | Yes | No |
| **Pub/Sub** | Yes | No |
| **Lua Scripting** | Yes | No |
| **Performance** | ~100K ops/sec | ~200K ops/sec |
| **Memory** | More memory overhead | More memory efficient |
| **Use Case** | Complex data, persistence needed | Simple key-value, maximum speed |

**Redis Architecture:**
```
Application Servers:
â”œâ”€ Server 1 â”€â”
â”œâ”€ Server 2 â”€â”¼â”€> Redis (shared cache)
â””â”€ Server 3 â”€â”˜

All servers access same Redis instance
Cache shared across all servers
```

**Basic Redis Operations:**
```bash
# String operations
SET user:123:name "Alice"
GET user:123:name  # Returns "Alice"
SETEX user:123:session "abc123" 3600  # Expires in 3600 seconds
DEL user:123:name

# Hash operations (store objects)
HSET user:123 name "Alice" age 30 email "alice@example.com"
HGET user:123 name  # Returns "Alice"
HGETALL user:123  # Returns all fields

# List operations (queues)
LPUSH queue:tasks "task1"
RPOP queue:tasks  # Returns "task1"

# Set operations (unique items)
SADD user:123:followers 456 789
SISMEMBER user:123:followers 456  # Returns 1 (true)

# Sorted Set (leaderboard)
ZADD leaderboard 1000 "player1" 950 "player2"
ZRANGE leaderboard 0 9 WITHSCORES  # Top 10 players

# TTL (Time To Live)
EXPIRE user:123:name 300  # Expire in 300 seconds
TTL user:123:name  # Check remaining seconds
```

**Application Cache Pattern (Python):**
```python
import redis
import json

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def get_user(user_id):
    # Try cache first
    cache_key = f"user:{user_id}"
    cached = redis_client.get(cache_key)
    
    if cached:
        print("Cache HIT")
        return json.loads(cached)
    
    print("Cache MISS")
    # Query database
    user = database.query("SELECT * FROM users WHERE id = ?", user_id)
    
    # Store in cache (expire in 5 minutes)
    redis_client.setex(
        cache_key,
        300,  # 5 minutes
        json.dumps(user)
    )
    
    return user

def update_user(user_id, data):
    # Update database
    database.update("UPDATE users SET ... WHERE id = ?", user_id, data)
    
    # Invalidate cache
    redis_client.delete(f"user:{user_id}")
```

**Node.js with Redis:**
```javascript
const redis = require('redis');
const client = redis.createClient();

async function getUser(userId) {
    const cacheKey = `user:${userId}`;
    
    // Try cache
    const cached = await client.get(cacheKey);
    if (cached) {
        console.log('Cache HIT');
        return JSON.parse(cached);
    }
    
    console.log('Cache MISS');
    // Query database
    const user = await db.query('SELECT * FROM users WHERE id = ?', [userId]);
    
    // Cache for 5 minutes
    await client.setEx(cacheKey, 300, JSON.stringify(user));
    
    return user;
}

async function updateUser(userId, data) {
    // Update database
    await db.query('UPDATE users SET ... WHERE id = ?', [userId, data]);
    
    // Invalidate cache
    await client.del(`user:${userId}`);
}
```

---

## 2. Cache Keys

**Good cache key design is critical for correctness and efficiency**

### 2.1 Key Naming Conventions

```
Pattern: {entity}:{id}:{attribute}

Examples:
user:123                    # User object
user:123:profile            # User profile
user:123:posts              # User's posts
post:456                    # Post object
post:456:comments           # Post comments
session:abc123              # Session data
query:SELECT_users_WHERE_id # Query result

Benefits:
+ Organized (easy to find related keys)
+ Predictable (easy to construct)
+ Debuggable (clear what data is cached)
```

### 2.2 Key Components

```
Include relevant parameters in key:

API endpoint: GET /api/products?category=electronics&page=2
Cache key: products:category:electronics:page:2

Database query: SELECT * FROM users WHERE age > 25 ORDER BY created_at
Cache key: query:users:age_gt_25:order_created_at

Function call: calculateRecommendations(userId=123, limit=10)
Cache key: recommendations:user:123:limit:10

Rule: Include ALL parameters that affect the result
```

**Bad Key Examples:**
```
âŒ user  (which user?)
âŒ data  (what data?)
âŒ result  (result of what?)
âŒ temp  (unclear purpose)
âŒ query  (which query?)

These are too generic, will cause collisions!
```

**Good Key Examples:**
```
âœ“ user:123:profile
âœ“ post:456:comments:page:2
âœ“ search:query:laptop:sort:price:page:1
âœ“ stats:daily:2024-01-15
âœ“ leaderboard:global:top:100
```

### 2.3 Key Collisions

```
Problem: Different data sharing same key

Bad:
user:123  stores Alice
user:123  stores Bob  â† Collision! Alice data lost

Solution: Include entity type in key
user:id:123
admin:id:123  â† Different keys, no collision
```

### 2.4 Key Patterns for Different Use Cases

#### Scalar Values
```
user:123:email = "alice@example.com"
user:123:age = 30
product:456:price = 29.99
```

#### Objects (use Hash)
```
HSET user:123 name "Alice" email "alice@example.com" age 30
HGET user:123 name  # Returns "Alice"
HGETALL user:123  # Returns all fields
```

#### Lists
```
user:123:notifications = [notif1, notif2, notif3]
LPUSH user:123:notifications "New message from Bob"
LRANGE user:123:notifications 0 9  # Get first 10
```

#### Sets (unique items)
```
user:123:followers = {user456, user789, user321}
SADD user:123:followers 456 789
SISMEMBER user:123:followers 456  # Check membership
SCARD user:123:followers  # Count followers
```

#### Sorted Sets (rankings, leaderboards)
```
leaderboard:2024-01 = {(player1, 1000), (player2, 950), ...}
ZADD leaderboard:2024-01 1000 "player1" 950 "player2"
ZREVRANGE leaderboard:2024-01 0 9 WITHSCORES  # Top 10
```

---

## 3. TTL (Time To Live)

**How long to keep data in cache before expiring**

### 3.1 Setting TTL

```bash
# Redis
SET key "value" EX 3600  # Expire in 3600 seconds (1 hour)
SETEX key 3600 "value"   # Same as above
EXPIRE key 300           # Set TTL on existing key

# Check TTL
TTL key  # Returns seconds remaining (-1 = no expiry, -2 = doesn't exist)
```

### 3.2 TTL Strategies

```
Static content (images, CSS):
TTL: 1 year (31536000 seconds)
Rarely changes, safe to cache long

Reference data (countries, categories):
TTL: 1 day (86400 seconds)
Changes infrequently

User data (profile, settings):
TTL: 5-15 minutes (300-900 seconds)
Balances freshness and performance

Real-time data (stock prices, scores):
TTL: 1-10 seconds
Changes frequently, need fresh data

Query results:
TTL: Depends on query
â”œâ”€ SELECT COUNT(*): 5 minutes (expensive, ok to be slightly stale)
â”œâ”€ SELECT * WHERE: 1 minute (moderate)
â””â”€ SELECT by ID: 10 minutes (cheap query, can cache longer)
```

### 3.3 TTL Trade-offs

```
Long TTL (hours/days):
+ Fewer cache misses
+ Lower database load
+ Better performance
- Stale data (users see old information)
- Memory usage (data stays in cache longer)

Short TTL (seconds/minutes):
+ Fresher data
+ Lower memory usage
- More cache misses
- Higher database load

Balance based on:
1. How often data changes
2. How critical is freshness
3. Cost of cache miss (DB query cost)
4. Memory constraints
```

### 3.4 Sliding Window TTL

```
Problem: Fixed TTL, data expires even if frequently accessed

Solution: Refresh TTL on each access

def get_with_sliding_ttl(key, ttl=300):
    value = redis.get(key)
    if value:
        # Refresh TTL on hit
        redis.expire(key, ttl)
    return value

Benefit: Hot data stays in cache, cold data expires
```

### 3.5 TTL Expiration Callbacks

```python
# Redis keyspace notifications (requires config: notify-keyspace-events Ex)
import redis

r = redis.Redis()
pubsub = r.pubsub()
pubsub.psubscribe('__keyevent@0__:expired')

for message in pubsub.listen():
    if message['type'] == 'pmessage':
        expired_key = message['data']
        print(f"Key expired: {expired_key}")
        # Handle expiration (e.g., refresh from DB)
```

---

## 4. Eviction Policies

**What to remove when cache is full?**

### 4.1 Redis Eviction Policies

```
maxmemory-policy <policy>

1. noeviction (default)
   - Don't evict anything
   - Return error when memory full
   - Use when: Cache must never lose data

2. allkeys-lru
   - Remove least recently used (LRU) key
   - Considers ALL keys
   - Use when: General-purpose cache

3. volatile-lru
   - Remove LRU key with TTL set
   - Only considers keys with expiry
   - Use when: Mix of persistent + temporary data

4. allkeys-lfu
   - Remove least frequently used (LFU) key
   - Better than LRU for some workloads
   - Use when: Want to keep popular data

5. volatile-lfu
   - Remove LFU key with TTL set

6. allkeys-random
   - Remove random key
   - Fast, but unpredictable

7. volatile-random
   - Remove random key with TTL set

8. volatile-ttl
   - Remove key with shortest TTL
   - Prioritizes keeping data that expires later
```

**Choosing Eviction Policy:**
```
Use allkeys-lru when:
âœ“ All data can be evicted (pure cache)
âœ“ Access patterns have temporal locality
âœ“ Example: API response cache

Use volatile-lru when:
âœ“ Some data must persist (no TTL)
âœ“ Some data is temporary (with TTL)
âœ“ Example: Sessions (no TTL) + query cache (TTL)

Use allkeys-lfu when:
âœ“ Popular data accessed frequently
âœ“ Want to keep "hot" data regardless of recency
âœ“ Example: Product catalog (popular products)

Use volatile-ttl when:
âœ“ Want to free space by removing soon-to-expire data
âœ“ TTLs are meaningful for importance
```

### 4.2 LRU (Least Recently Used)

```
Cache tracks access order:

Cache (capacity: 3):
Access order: [key3, key2, key1]  (key3 = most recent)

New access: key2
Access order: [key2, key3, key1]  (key2 moved to front)

Cache full, add key4:
Access order: [key4, key2, key3]  (key1 evicted = least recent)

Pros: Simple, effective for temporal locality
Cons: Doesn't consider access frequency
```

### 4.3 LFU (Least Frequently Used)

```
Cache tracks access count:

Cache (capacity: 3):
key1: 10 accesses
key2: 5 accesses
key3: 2 accesses  â† Least frequent

Cache full, add key4:
key3 evicted (lowest frequency)

New cache:
key1: 10 accesses
key2: 5 accesses
key4: 1 access

Pros: Keeps popular data
Cons: Doesn't forget old popular data (use decay)
```

### 4.4 Memcached Eviction

```
Memcached uses LRU only

maxmemory: 1GB
When full: Evict least recently used item

Slab allocation:
â”œâ”€ Memory divided into slabs (different sizes)
â”œâ”€ Small objects: 64-byte slab
â”œâ”€ Medium objects: 1KB slab
â”œâ”€ Large objects: 1MB slab
â””â”€ Each slab has its own LRU

Eviction per slab (not global)
Prevents large objects from evicting many small objects
```

---

## 5. Cache Stampede (Thundering Herd)

**Problem:** Many requests for expired cache key hit database simultaneously

```
Scenario:
1. Popular key expires at 10:00:00
2. At 10:00:01, 1000 requests arrive for that key
3. All 1000 requests see cache miss
4. All 1000 requests query database simultaneously
5. Database overloaded! âŒ

Timeline:
10:00:00 - Cache expires
10:00:01 - Request 1: Miss â†’ Query DB
10:00:01 - Request 2: Miss â†’ Query DB
10:00:01 - Request 3: Miss â†’ Query DB
...
10:00:01 - Request 1000: Miss â†’ Query DB

Result: 1000 concurrent DB queries (thundering herd!)
```

### 5.1 Solution 1: Cache Locking

```
Only one request queries database, others wait

Process:
1. Request 1: Miss â†’ Acquire lock â†’ Query DB â†’ Update cache â†’ Release lock
2. Request 2: Miss â†’ Try lock (locked) â†’ Wait â†’ Read from cache
3. Request 3: Miss â†’ Try lock (locked) â†’ Wait â†’ Read from cache

Timeline:
10:00:01 - Request 1: Lock acquired, querying DB...
10:00:01 - Request 2: Lock busy, waiting...
10:00:01 - Request 3: Lock busy, waiting...
10:00:02 - Request 1: Cache updated, lock released
10:00:02 - Request 2: Read from cache âœ“
10:00:02 - Request 3: Read from cache âœ“

Result: Only 1 DB query!
```

**Redis Lock Implementation:**
```python
import redis
import time
import json

redis_client = redis.Redis()

def get_with_lock(key, fetch_func, ttl=300, lock_timeout=10):
    # Try cache first
    value = redis_client.get(key)
    if value:
        return json.loads(value)
    
    # Cache miss, try to acquire lock
    lock_key = f"lock:{key}"
    lock_acquired = redis_client.set(lock_key, "1", nx=True, ex=lock_timeout)
    
    if lock_acquired:
        try:
            # We have the lock, query database
            print("Lock acquired, querying database...")
            value = fetch_func()
            
            # Update cache
            redis_client.setex(key, ttl, json.dumps(value))
            return value
        finally:
            # Release lock
            redis_client.delete(lock_key)
    else:
        # Lock is held by another request, wait and retry
        print("Lock busy, waiting...")
        for _ in range(20):  # Retry for up to 2 seconds
            time.sleep(0.1)
            value = redis_client.get(key)
            if value:
                return json.loads(value)
        
        # Timeout, query database anyway (fallback)
        return fetch_func()

# Usage
def get_user(user_id):
    def fetch():
        return database.query("SELECT * FROM users WHERE id = ?", user_id)
    
    return get_with_lock(f"user:{user_id}", fetch, ttl=300)
```

**Node.js Lock Implementation:**
```javascript
const redis = require('redis');
const client = redis.createClient();

async function getWithLock(key, fetchFunc, ttl = 300, lockTimeout = 10) {
    // Try cache first
    const cached = await client.get(key);
    if (cached) {
        return JSON.parse(cached);
    }
    
    // Try to acquire lock
    const lockKey = `lock:${key}`;
    const lockAcquired = await client.set(lockKey, '1', {
        NX: true,
        EX: lockTimeout
    });
    
    if (lockAcquired) {
        try {
            console.log('Lock acquired, querying database...');
            const value = await fetchFunc();
            
            // Update cache
            await client.setEx(key, ttl, JSON.stringify(value));
            return value;
        } finally {
            // Release lock
            await client.del(lockKey);
        }
    } else {
        // Wait for lock to be released
        console.log('Lock busy, waiting...');
        for (let i = 0; i < 20; i++) {
            await new Promise(resolve => setTimeout(resolve, 100));
            const value = await client.get(key);
            if (value) {
                return JSON.parse(value);
            }
        }
        
        // Timeout, query anyway
        return await fetchFunc();
    }
}
```

### 5.2 Solution 2: Probabilistic Early Expiration (Jitter)

```
Don't wait for exact TTL expiry, refresh early randomly

Standard TTL:
All caches expire at exactly 10:00:00 â†’ Stampede!

With jitter:
Cache 1 expires at 09:59:45
Cache 2 expires at 09:59:52
Cache 3 expires at 09:59:58
Cache 4 expires at 10:00:03
Cache 5 expires at 10:00:07

Expires spread out, no stampede!
```

**Implementation with Jitter:**
```python
import random

def set_with_jitter(key, value, ttl=300, jitter_pct=0.1):
    # Add random jitter: Â±10% of TTL
    jitter = int(ttl * jitter_pct)
    actual_ttl = ttl + random.randint(-jitter, jitter)
    
    redis_client.setex(key, actual_ttl, value)

# Example: TTL=300s, jitter=10%
# Actual TTL: 270-330 seconds (randomly chosen)

# Multiple instances set same key at same time:
# Instance 1: TTL=285s â†’ Expires at 10:04:45
# Instance 2: TTL=315s â†’ Expires at 10:05:15
# Instance 3: TTL=292s â†’ Expires at 10:04:52

# Expiry times spread out, no simultaneous expiration
```

**Probabilistic Early Refresh:**
```python
import random
import time

def get_with_early_refresh(key, fetch_func, ttl=300):
    value = redis_client.get(key)
    
    if value:
        # Check if we should refresh early (probabilistically)
        remaining_ttl = redis_client.ttl(key)
        
        # Probability increases as TTL decreases
        # When TTL=300s (fresh): 0% probability
        # When TTL=150s (half): 50% probability
        # When TTL=30s (near expiry): 90% probability
        probability = 1 - (remaining_ttl / ttl)
        
        if random.random() < probability:
            # Refresh cache early (asynchronously)
            # This request returns cached value
            # Background job refreshes cache
            schedule_background_refresh(key, fetch_func)
        
        return json.loads(value)
    else:
        # Cache miss, fetch normally
        value = fetch_func()
        redis_client.setex(key, ttl, json.dumps(value))
        return value
```

### 5.3 Solution 3: Serve Stale While Revalidate

```
Serve expired cache while refreshing in background

Process:
1. Cache expires at 10:00:00
2. Request at 10:00:01: 
   - Return expired cache immediately (fast!)
   - Trigger background refresh
3. Request at 10:00:02:
   - Return expired cache (still refreshing)
4. Request at 10:00:03:
   - Cache refreshed, return fresh data

User never waits for database query!
```

**HTTP Header:**
```http
Cache-Control: max-age=300, stale-while-revalidate=60

Meaning:
- Fresh for 300 seconds
- After 300s, serve stale for up to 60 more seconds while refreshing
- Total: Up to 360 seconds old, but users don't wait
```

**Redis Implementation:**
```python
import threading

def get_with_stale_while_revalidate(key, fetch_func, ttl=300, grace=60):
    value = redis_client.get(key)
    remaining = redis_client.ttl(key)
    
    if value and remaining > 0:
        # Cache is fresh
        return json.loads(value)
    elif value and remaining >= -grace:
        # Cache is stale but within grace period
        # Serve stale value immediately
        print("Serving stale, refreshing in background...")
        
        # Refresh in background (non-blocking)
        def refresh():
            new_value = fetch_func()
            redis_client.setex(key, ttl, json.dumps(new_value))
        
        threading.Thread(target=refresh).start()
        
        return json.loads(value)
    else:
        # No cache or too stale, fetch normally
        value = fetch_func()
        redis_client.setex(key, ttl, json.dumps(value))
        return value
```

### 5.4 Solution Comparison

| Solution | Pros | Cons | Best For |
|----------|------|------|----------|
| **Locking** | Prevents all redundant queries | Adds latency (waiting) | Critical data, expensive queries |
| **Jitter** | Simple, no coordination | Doesn't eliminate stampede completely | General purpose, easy to implement |
| **Early Refresh** | Smooth load distribution | Complex logic | High-traffic keys |
| **Stale While Revalidate** | Zero perceived latency | Serves stale data | User-facing, acceptable staleness |

---

## 6. Cache Write Patterns

### 6.1 Cache-Aside (Lazy Loading)

**Most common pattern**

```
Read:
1. Check cache
2. If miss: Query database â†’ Update cache â†’ Return
3. If hit: Return cached data

Write:
1. Update database
2. Invalidate cache (delete cache key)
3. Next read will cache miss and reload

Process:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Read:                               â”‚
â”‚ App â†’ Cache (miss) â†’ Database       â”‚
â”‚     â†’ Cache (store) â†’ Return        â”‚
â”‚                                     â”‚
â”‚ Write:                              â”‚
â”‚ App â†’ Database (update)             â”‚
â”‚     â†’ Cache (delete)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Code Example:**
```python
def get_user(user_id):
    cache_key = f"user:{user_id}"
    
    # Try cache
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Cache miss, query database
    user = db.query("SELECT * FROM users WHERE id = ?", user_id)
    
    # Store in cache
    redis_client.setex(cache_key, 300, json.dumps(user))
    return user

def update_user(user_id, data):
    # Update database
    db.execute("UPDATE users SET ... WHERE id = ?", user_id, data)
    
    # Invalidate cache
    redis_client.delete(f"user:{user_id}")
```

**Pros:**
```
+ Simple to implement
+ Only caches requested data (no waste)
+ Cache miss doesn't break app (just slower)
```

**Cons:**
```
- Cache miss penalty (query DB + cache write)
- Potential for stale data (if invalidation fails)
- Thundering herd on popular keys
```

### 6.2 Write-Through

**Write to cache and database simultaneously**

```
Write:
1. Update database
2. Update cache (set new value)
3. Return

Read:
1. Check cache (always present)
2. Return cached data

Process:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Write:                              â”‚
â”‚ App â†’ Database (update)             â”‚
â”‚     â†’ Cache (set)                   â”‚
â”‚                                     â”‚
â”‚ Read:                               â”‚
â”‚ App â†’ Cache (always hit)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Code Example:**
```python
def update_user(user_id, data):
    cache_key = f"user:{user_id}"
    
    # Update database
    db.execute("UPDATE users SET ... WHERE id = ?", user_id, data)
    
    # Query updated data
    user = db.query("SELECT * FROM users WHERE id = ?", user_id)
    
    # Update cache (write-through)
    redis_client.setex(cache_key, 300, json.dumps(user))
    
    return user

def get_user(user_id):
    cache_key = f"user:{user_id}"
    
    # Cache should always be present (write-through)
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Fallback if cache miss (shouldn't happen)
    user = db.query("SELECT * FROM users WHERE id = ?", user_id)
    redis_client.setex(cache_key, 300, json.dumps(user))
    return user
```

**Pros:**
```
+ Cache always consistent with database
+ Read performance (always cache hit)
+ No stale data
```

**Cons:**
```
- Slower writes (two operations)
- Caches data that may never be read (waste)
- Extra complexity
```

### 6.3 Write-Back (Write-Behind)

**Write to cache first, update database asynchronously**

```
Write:
1. Update cache immediately
2. Queue database write (async)
3. Return (fast!)

Background:
4. Process queue
5. Update database

Read:
1. Check cache (always present)
2. Return cached data

Process:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Write:                              â”‚
â”‚ App â†’ Cache (update immediately)    â”‚
â”‚     â†’ Queue (async DB update)       â”‚
â”‚                                     â”‚
â”‚ Background Worker:                  â”‚
â”‚ Queue â†’ Database (update)           â”‚
â”‚                                     â”‚
â”‚ Read:                               â”‚
â”‚ App â†’ Cache (always hit)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Code Example:**
```python
import queue
import threading

# Write queue
write_queue = queue.Queue()

def background_writer():
    """Background thread to process writes"""
    while True:
        user_id, data = write_queue.get()
        try:
            # Update database
            db.execute("UPDATE users SET ... WHERE id = ?", user_id, data)
        except Exception as e:
            print(f"Failed to write to DB: {e}")
        write_queue.task_done()

# Start background thread
threading.Thread(target=background_writer, daemon=True).start()

def update_user(user_id, data):
    cache_key = f"user:{user_id}"
    
    # Update cache immediately (fast!)
    user = {"id": user_id, **data}
    redis_client.setex(cache_key, 300, json.dumps(user))
    
    # Queue database write (async)
    write_queue.put((user_id, data))
    
    return user  # Return immediately!

def get_user(user_id):
    cache_key = f"user:{user_id}"
    cached = redis_client.get(cache_key)
    
    if cached:
        return json.loads(cached)
    
    # Cache miss, query database
    user = db.query("SELECT * FROM users WHERE id = ?", user_id)
    redis_client.setex(cache_key, 300, json.dumps(user))
    return user
```

**Pros:**
```
+ Very fast writes (cache only)
+ Reduced database load (batching possible)
+ Great for write-heavy workloads
```

**Cons:**
```
- Risk of data loss (if cache fails before DB write)
- Complex to implement correctly
- Potential inconsistency between cache and DB
```

**Use Case: Session Store**
```
Writes are frequent (every request updates session)
Reads are frequent (every request reads session)

Write-back ideal:
â”œâ”€ Fast writes (update cache only)
â”œâ”€ Batch DB writes (every 10 seconds)
â””â”€ Acceptable to lose few seconds of session data if crash
```

### 6.4 Refresh-Ahead

**Proactively refresh cache before expiration**

```
Process:
1. Set TTL = 300s, refresh threshold = 60s
2. On read, check TTL
3. If TTL < 60s: Trigger async refresh
4. Return cached data immediately
5. Background: Refresh cache

Timeline:
00:00 - Cache set, TTL=300s
04:00 - TTL=60s (threshold reached)
04:01 - Read request â†’ Return cache â†’ Trigger refresh
04:02 - Background: Query DB â†’ Update cache
04:03 - Cache refreshed, TTL=300s again

Cache never expires (always refreshed before expiry)
```

**Implementation:**
```python
import threading

def get_with_refresh_ahead(key, fetch_func, ttl=300, refresh_threshold=60):
    value = redis_client.get(key)
    
    if value:
        remaining = redis_client.ttl(key)
        
        # Check if below threshold
        if remaining < refresh_threshold and remaining > 0:
            print(f"TTL low ({remaining}s), refreshing in background...")
            
            # Refresh asynchronously
            def refresh():
                new_value = fetch_func()
                redis_client.setex(key, ttl, json.dumps(new_value))
            
            threading.Thread(target=refresh).start()
        
        return json.loads(value)
    else:
        # Cache miss, fetch normally
        value = fetch_func()
        redis_client.setex(key, ttl, json.dumps(value))
        return value
```

**Pros:**
```
+ Predictable performance (no cache miss latency)
+ Reduces thundering herd
+ Hot data always cached
```

**Cons:**
```
- Unnecessary refreshes (if data not read)
- Complex to implement
- Extra background load
```

### 6.5 Pattern Comparison

| Pattern | Write Speed | Read Speed | Consistency | Complexity | Use Case |
|---------|-------------|------------|-------------|------------|----------|
| **Cache-Aside** | Fast | Miss: Slow | Eventual | Low | General purpose |
| **Write-Through** | Slow | Fast | Strong | Medium | Read-heavy, consistency critical |
| **Write-Back** | Very Fast | Fast | Weak | High | Write-heavy, loss acceptable |
| **Refresh-Ahead** | Fast | Very Fast | Eventual | High | Predictable load, hot data |

---

## Complete Example: E-commerce Product Cache

```python
import redis
import json
import threading
import time
from typing import Optional, Dict, Any

class ProductCache:
    def __init__(self):
        self.redis = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.db = Database()  # Assume database connection
    
    def get_product(self, product_id: int) -> Optional[Dict[str, Any]]:
        """
        Cache-aside pattern with:
        - Lock to prevent stampede
        - TTL with jitter
        - Stale-while-revalidate
        """
        cache_key = f"product:{product_id}"
        
        # Try cache
        cached = self.redis.get(cache_key)
        ttl = self.redis.ttl(cache_key)
        
        if cached:
            # Check if stale but within grace period
            if ttl < 0 and ttl >= -60:  # Grace period: 60 seconds
                print("Serving stale, refreshing...")
                self._refresh_async(cache_key, product_id)
            
            return json.loads(cached)
        
        # Cache miss - use lock to prevent stampede
        return self._fetch_with_lock(cache_key, product_id)
    
    def _fetch_with_lock(self, cache_key: str, product_id: int) -> Dict[str, Any]:
        """Fetch from DB with distributed lock"""
        lock_key = f"lock:{cache_key}"
        
        # Try to acquire lock
        if self.redis.set(lock_key, "1", nx=True, ex=10):
            try:
                print(f"Lock acquired for {product_id}, querying DB...")
                product = self._fetch_from_db(product_id)
                
                # Cache with jitter (5 min Â± 30s)
                ttl = 300 + random.randint(-30, 30)
                self.redis.setex(cache_key, ttl, json.dumps(product))
                
                return product
            finally:
                self.redis.delete(lock_key)
        else:
            # Lock held by another request, wait
            print(f"Lock busy for {product_id}, waiting...")
            for _ in range(20):  # Wait up to 2 seconds
                time.sleep(0.1)
                cached = self.redis.get(cache_key)
                if cached:
                    return json.loads(cached)
            
            # Timeout, fetch anyway
            return self._fetch_from_db(product_id)
    
    def _refresh_async(self, cache_key: str, product_id: int):
        """Refresh cache in background"""
        def refresh():
            product = self._fetch_from_db(product_id)
            ttl = 300 + random.randint(-30, 30)
            self.redis.setex(cache_key, ttl, json.dumps(product))
        
        threading.Thread(target=refresh).start()
    
    def _fetch_from_db(self, product_id: int) -> Dict[str, Any]:
        """Fetch product from database"""
        return self.db.query(
            "SELECT * FROM products WHERE id = ?",
            [product_id]
        )
    
    def update_product(self, product_id: int, data: Dict[str, Any]):
        """
        Write-through pattern:
        - Update database
        - Update cache
        - Invalidate related caches
        """
        # Update database
        self.db.execute(
            "UPDATE products SET ... WHERE id = ?",
            [product_id, data]
        )
        
        # Fetch updated product
        product = self._fetch_from_db(product_id)
        
        # Update cache (write-through)
        cache_key = f"product:{product_id}"
        ttl = 300 + random.randint(-30, 30)
        self.redis.setex(cache_key, ttl, json.dumps(product))
        
        # Invalidate related caches
        self._invalidate_related(product_id, product)
    
    def _invalidate_related(self, product_id: int, product: Dict[str, Any]):
        """Invalidate caches that depend on this product"""
        # Invalidate category cache
        category_id = product.get('category_id')
        if category_id:
            self.redis.delete(f"category:{category_id}:products")
        
        # Invalidate search caches (pattern delete)
        self._delete_pattern("search:*")
    
    def _delete_pattern(self, pattern: str):
        """Delete all keys matching pattern"""
        cursor = 0
        while True:
            cursor, keys = self.redis.scan(cursor, match=pattern, count=100)
            if keys:
                self.redis.delete(*keys)
            if cursor == 0:
                break

# Usage
cache = ProductCache()

# Get product (fast if cached)
product = cache.get_product(123)

# Update product (updates cache too)
cache.update_product(123, {"name": "New Name", "price": 29.99})
```

---

## Best Practices Summary

```
âœ“ Use appropriate cache layer (CDN for static, Redis for dynamic)
âœ“ Set reasonable TTLs (balance freshness vs performance)
âœ“ Use good key naming (entity:id:attribute)
âœ“ Implement stampede protection (lock, jitter, or stale-while-revalidate)
âœ“ Choose right eviction policy (LRU for general, LFU for popular)
âœ“ Monitor cache hit ratio (aim for 80-95%+)
âœ“ Use write-through for consistency-critical data
âœ“ Use cache-aside for general purpose
âœ“ Handle cache failures gracefully (fallback to DB)
âœ“ Log cache operations (hits, misses, evictions)
âœ“ Invalidate related caches on write
âœ“ Use compression for large cached values
âœ— Don't cache everything (some data better uncached)
âœ— Don't forget to set TTL (memory leaks)
âœ— Don't use cache as primary data store (persistence matters)
âœ— Don't ignore cache invalidation (stale data issues)
```

Perfect caching foundation for system design! ğŸš€âš¡

