# File, Media & Edge

## What Is File & Media Storage?

**Store and serve large files efficiently at global scale**

```
Traditional storage (filesystem):
Server: /var/www/uploads/image.jpg
â”œâ”€ Single server (no redundancy)
â”œâ”€ Limited capacity (disk space)
â”œâ”€ No geographic distribution
â””â”€ Slow for distant users

Object storage (S3, GCS):
Bucket: s3://my-bucket/images/image.jpg
â”œâ”€ Replicated across zones (durability: 99.999999999%)
â”œâ”€ Unlimited capacity (petabytes+)
â”œâ”€ Pay-per-use ($0.023/GB/month)
â”œâ”€ Global access (API, HTTPS)
â””â”€ Versioning, lifecycle management

CDN (CloudFront, Cloudflare):
Edge locations (200+ worldwide)
â”œâ”€ Cache files near users (low latency)
â”œâ”€ Offload origin (reduce traffic)
â”œâ”€ DDoS protection
â””â”€ HTTPS, compression
```

---

## 1. Object Storage (S3, GCS)

### 1.1 Architecture

**Distributed key-value store for large objects**

```
S3 Bucket: my-app-uploads
â”œâ”€ Key: users/123/profile.jpg (object path)
â”œâ”€ Value: Binary data (image file)
â”œâ”€ Metadata: Content-Type, ETag, modified time
â””â”€ Permissions: ACL, bucket policy

Storage classes:
â”œâ”€ S3 Standard: Frequent access (millisecond latency)
â”œâ”€ S3 Infrequent Access (IA): Monthly access (cheaper)
â”œâ”€ S3 Glacier: Archive (minutes to hours retrieval)
â””â”€ S3 Intelligent-Tiering: Auto-move based on access

Durability: 99.999999999% (11 nines)
â”œâ”€ Replicated across 3+ availability zones
â”œâ”€ Checksum validation (detect bit rot)
â””â”€ Versioning (recover deleted files)

Availability: 99.9% - 99.99%
```

**Consistency Model:**

```
S3 (after Dec 2020):
â”œâ”€ Strong consistency (read-after-write)
â”œâ”€ PUT object â†’ immediately GET same object
â””â”€ DELETE object â†’ immediately 404

Before Dec 2020:
â”œâ”€ Eventual consistency (reads might return old data)
â””â”€ PUT object â†’ GET might return old version briefly

GCS:
â”œâ”€ Strong consistency (always)
â””â”€ Same as modern S3
```

### 1.2 Presigned URLs

**Grant temporary access without credentials**

```
Problem:
User uploads file to S3 from browser
â”œâ”€ Option 1: Send file to server â†’ server uploads to S3
â”‚  â””â”€ Cons: Server bandwidth, latency, single point of failure
â”œâ”€ Option 2: Give user AWS credentials
â”‚  â””â”€ Cons: Security risk (credentials leak)

Solution: Presigned URL
User â†’ Server: Request upload URL
Server â†’ User: Presigned URL (temporary, signed)
User â†’ S3: Upload directly using presigned URL
S3: Validate signature, accept upload
```

**How Presigned URLs Work:**

```
Components:
â”œâ”€ URL: https://my-bucket.s3.amazonaws.com/users/123/photo.jpg
â”œâ”€ Query params:
â”‚  â”œâ”€ X-Amz-Algorithm=AWS4-HMAC-SHA256
â”‚  â”œâ”€ X-Amz-Credential=AKIAIOSFODNN7EXAMPLE/20240101/us-east-1/s3/aws4_request
â”‚  â”œâ”€ X-Amz-Date=20240101T120000Z
â”‚  â”œâ”€ X-Amz-Expires=3600 (URL valid for 1 hour)
â”‚  â””â”€ X-Amz-Signature=<HMAC signature>
â””â”€ Signature: HMAC(secret_key, URL + expiration + permissions)

Validation:
1. S3 receives request with presigned URL
2. Extract signature from URL
3. Recompute signature using AWS secret key
4. Compare signatures (match â†’ accept, mismatch â†’ reject)
5. Check expiration (expired â†’ reject)
6. Grant temporary permission (upload/download)

Security:
+ No credentials exposed to client
+ Time-limited (URL expires)
+ Specific permissions (upload only, download only)
+ Cannot be used after expiration
```

**Code Examples:**

```python
import boto3
from datetime import timedelta

s3_client = boto3.client('s3', region_name='us-east-1')

# Generate presigned URL for upload (PUT)
def generate_upload_url(bucket, key, expiration=3600):
    url = s3_client.generate_presigned_url(
        'put_object',
        Params={
            'Bucket': bucket,
            'Key': key,
            'ContentType': 'image/jpeg'  # Enforce content type
        },
        ExpiresIn=expiration  # URL valid for 1 hour
    )
    return url

# Usage
upload_url = generate_upload_url('my-bucket', 'users/123/profile.jpg')
# Returns: https://my-bucket.s3.amazonaws.com/users/123/profile.jpg?X-Amz-Algorithm=...

# Client uploads directly to S3
import requests

with open('profile.jpg', 'rb') as f:
    response = requests.put(
        upload_url,
        data=f,
        headers={'Content-Type': 'image/jpeg'}
    )
    
print(response.status_code)  # 200 if success

# Generate presigned URL for download (GET)
def generate_download_url(bucket, key, expiration=3600):
    url = s3_client.generate_presigned_url(
        'get_object',
        Params={
            'Bucket': bucket,
            'Key': key
        },
        ExpiresIn=expiration
    )
    return url

# Usage
download_url = generate_download_url('my-bucket', 'users/123/profile.jpg')
# User downloads: GET https://my-bucket.s3.amazonaws.com/users/123/profile.jpg?X-Amz-...
```

**Advanced Presigned POST:**

```python
# Presigned POST (with field constraints)
def generate_presigned_post(bucket, key, max_size_mb=10):
    conditions = [
        {'bucket': bucket},
        ['starts-with', '$key', 'users/'],  # Key must start with users/
        {'Content-Type': 'image/jpeg'},
        ['content-length-range', 0, max_size_mb * 1024 * 1024]  # Max size
    ]
    
    response = s3_client.generate_presigned_post(
        Bucket=bucket,
        Key=key,
        Fields={
            'Content-Type': 'image/jpeg'
        },
        Conditions=conditions,
        ExpiresIn=3600
    )
    
    return response

# Returns: {'url': 'https://...', 'fields': {'key': '...', 'policy': '...', 'signature': '...'}}

# Client uploads with HTML form
"""
<form action="{url}" method="post" enctype="multipart/form-data">
  <input type="hidden" name="key" value="{fields[key]}" />
  <input type="hidden" name="policy" value="{fields[policy]}" />
  <input type="hidden" name="signature" value="{fields[signature]}" />
  <input type="file" name="file" />
  <input type="submit" value="Upload" />
</form>
"""
```

### 1.3 Multipart Upload

**Upload large files (>100MB) in parallel chunks**

```
Problem:
Single PUT request for 5 GB file:
â”œâ”€ Long upload time (hours on slow connection)
â”œâ”€ Network failure â†’ restart entire upload
â””â”€ No parallelism (single stream)

Solution: Multipart Upload
1. Initiate upload â†’ Receive upload ID
2. Split file into parts (5 MB - 5 GB each)
3. Upload parts in parallel (multiple threads)
4. Complete upload (combine parts)

Benefits:
+ Parallel uploads (faster)
+ Resume failed parts (don't restart entire file)
+ Upload while file is being created (streaming)
```

**Multipart Upload Flow:**

```
Step 1: Initiate multipart upload
Client â†’ S3: Initiate upload for key "videos/movie.mp4"
S3 â†’ Client: Upload ID "abc123"

Step 2: Upload parts (parallel)
Client â†’ S3: Upload part 1 (bytes 0-5MB), upload ID "abc123"
Client â†’ S3: Upload part 2 (bytes 5MB-10MB), upload ID "abc123"
Client â†’ S3: Upload part 3 (bytes 10MB-15MB), upload ID "abc123"
...
S3 â†’ Client: ETag for each part (checksum)

Step 3: Complete upload
Client â†’ S3: Complete upload with [part 1 ETag, part 2 ETag, ...]
S3: Assemble parts into single object
S3 â†’ Client: Success

Failure handling:
â”œâ”€ Part upload fails â†’ Retry only that part
â”œâ”€ Incomplete upload â†’ Abort (cleanup)
â””â”€ Lifecycle policy: Auto-delete incomplete uploads after 7 days
```

**Code Example:**

```python
import boto3
from concurrent.futures import ThreadPoolExecutor
import math

s3_client = boto3.client('s3')

def multipart_upload(bucket, key, file_path, part_size=10*1024*1024):  # 10 MB parts
    # Step 1: Initiate multipart upload
    response = s3_client.create_multipart_upload(
        Bucket=bucket,
        Key=key,
        ContentType='video/mp4'
    )
    upload_id = response['UploadId']
    
    try:
        # Step 2: Upload parts
        file_size = os.path.getsize(file_path)
        num_parts = math.ceil(file_size / part_size)
        
        parts = []
        
        def upload_part(part_number):
            offset = (part_number - 1) * part_size
            
            with open(file_path, 'rb') as f:
                f.seek(offset)
                data = f.read(part_size)
            
            response = s3_client.upload_part(
                Bucket=bucket,
                Key=key,
                PartNumber=part_number,
                UploadId=upload_id,
                Body=data
            )
            
            return {
                'PartNumber': part_number,
                'ETag': response['ETag']
            }
        
        # Upload parts in parallel (4 threads)
        with ThreadPoolExecutor(max_workers=4) as executor:
            parts = list(executor.map(upload_part, range(1, num_parts + 1)))
        
        # Step 3: Complete upload
        s3_client.complete_multipart_upload(
            Bucket=bucket,
            Key=key,
            UploadId=upload_id,
            MultipartUpload={'Parts': parts}
        )
        
        print(f"Upload complete: {key}")
        
    except Exception as e:
        # Abort upload on failure
        s3_client.abort_multipart_upload(
            Bucket=bucket,
            Key=key,
            UploadId=upload_id
        )
        raise e

# Usage
multipart_upload('my-bucket', 'videos/movie.mp4', '/path/to/movie.mp4')
```

**S3 Transfer Acceleration:**

```
Problem: Slow uploads from distant regions
User (Singapore) â†’ S3 bucket (US East)
â”œâ”€ Public internet (slow, unpredictable)
â””â”€ High latency (200+ ms)

Solution: Transfer Acceleration
User (Singapore) â†’ CloudFront edge (Singapore)
Edge (Singapore) â†’ S3 (US East) via AWS network
â”œâ”€ Edge location near user (low latency)
â”œâ”€ AWS backbone network (fast, optimized)
â””â”€ 50-500% faster uploads

Enable:
s3_client.put_bucket_accelerate_configuration(
    Bucket='my-bucket',
    AccelerateConfiguration={'Status': 'Enabled'}
)

Use accelerate endpoint:
URL: https://my-bucket.s3-accelerate.amazonaws.com/
```

### 1.4 Lifecycle Policies

**Automate data transitions and deletions**

```
Use cases:
â”œâ”€ Transition old files to cheaper storage (Standard â†’ Glacier)
â”œâ”€ Delete temporary files after expiration
â”œâ”€ Delete incomplete multipart uploads (cleanup)
â””â”€ Expire old versions (versioned buckets)

Lifecycle rules:
1. Transition (move to cheaper storage class)
2. Expiration (delete object)
3. Abort incomplete uploads
```

**Lifecycle Configuration:**

```python
# Lifecycle policy example
lifecycle_config = {
    'Rules': [
        {
            'Id': 'Archive old logs',
            'Status': 'Enabled',
            'Filter': {
                'Prefix': 'logs/'  # Apply to logs/ prefix
            },
            'Transitions': [
                {
                    'Days': 30,
                    'StorageClass': 'STANDARD_IA'  # After 30 days â†’ Infrequent Access
                },
                {
                    'Days': 90,
                    'StorageClass': 'GLACIER'  # After 90 days â†’ Glacier
                }
            ],
            'Expiration': {
                'Days': 365  # Delete after 1 year
            }
        },
        {
            'Id': 'Delete temp files',
            'Status': 'Enabled',
            'Filter': {
                'Prefix': 'temp/'
            },
            'Expiration': {
                'Days': 7  # Delete after 7 days
            }
        },
        {
            'Id': 'Cleanup incomplete uploads',
            'Status': 'Enabled',
            'AbortIncompleteMultipartUpload': {
                'DaysAfterInitiation': 7  # Abort after 7 days
            }
        },
        {
            'Id': 'Expire old versions',
            'Status': 'Enabled',
            'NoncurrentVersionExpiration': {
                'NoncurrentDays': 30  # Delete old versions after 30 days
            }
        }
    ]
}

s3_client.put_bucket_lifecycle_configuration(
    Bucket='my-bucket',
    LifecycleConfiguration=lifecycle_config
)
```

**Storage Class Comparison:**

```
| Class | Latency | Durability | Availability | Cost ($/GB/month) | Use Case |
|-------|---------|------------|--------------|-------------------|----------|
| Standard | ms | 11 9's | 99.99% | $0.023 | Frequent access |
| Intelligent-Tiering | ms | 11 9's | 99.9% | $0.023 + automation | Unknown access |
| Standard-IA | ms | 11 9's | 99.9% | $0.0125 | Monthly access |
| One Zone-IA | ms | 11 9's | 99.5% | $0.01 | Recreatable data |
| Glacier Instant | ms | 11 9's | 99.9% | $0.004 | Archive, instant |
| Glacier Flexible | min-hrs | 11 9's | 99.99% | $0.0036 | Archive, flexible |
| Glacier Deep | hrs | 11 9's | 99.99% | $0.00099 | Long-term archive |

Retrieval costs (Glacier):
â”œâ”€ Expedited: 1-5 min, $0.03/GB
â”œâ”€ Standard: 3-5 hours, $0.01/GB
â””â”€ Bulk: 5-12 hours, $0.0025/GB
```

**Cost Optimization Example:**

```
Scenario: Video platform with 1 PB data

Without lifecycle:
â”œâ”€ 1 PB Ã— $0.023/GB = $23,000/month
â””â”€ All data in Standard (expensive)

With lifecycle:
â”œâ”€ 100 TB recent (30 days): Standard
â”‚  â””â”€ 100 TB Ã— $0.023/GB = $2,300/month
â”œâ”€ 300 TB older (30-90 days): Standard-IA
â”‚  â””â”€ 300 TB Ã— $0.0125/GB = $3,750/month
â””â”€ 600 TB archive (90+ days): Glacier Flexible
   â””â”€ 600 TB Ã— $0.0036/GB = $2,160/month

Total: $8,210/month (64% savings!)
```

### 1.5 Versioning

**Keep multiple versions of objects**

```
Enable versioning:
s3_client.put_bucket_versioning(
    Bucket='my-bucket',
    VersioningConfiguration={'Status': 'Enabled'}
)

Object versions:
PUT file.txt (version 1) â†’ Version ID: v1
PUT file.txt (version 2) â†’ Version ID: v2 (current)
DELETE file.txt â†’ Delete marker (current)

Retrieve versions:
GET file.txt â†’ 404 (delete marker)
GET file.txt?versionId=v2 â†’ Returns version 2
GET file.txt?versionId=v1 â†’ Returns version 1

Restore:
DELETE delete marker â†’ Version 2 becomes current again

Benefits:
+ Accidental deletion protection (recover files)
+ Audit trail (track changes)
+ Compliance (retain all versions)

Trade-off:
- Storage costs (all versions stored)

Mitigation:
â””â”€ Lifecycle policy to expire old versions
```

---

## 2. Content Delivery Networks (CDNs)

### 2.1 CDN Architecture

**Cache content at edge locations near users**

```
Without CDN:
User (Tokyo) â†’ Origin (US East)
â”œâ”€ Round-trip: 200+ ms
â”œâ”€ Long distance (poor performance)
â””â”€ Origin handles all traffic (bottleneck)

With CDN:
User (Tokyo) â†’ Edge (Tokyo) â†’ Origin (US East)
                  â†“ cached
               Return (10 ms)
â”œâ”€ First request: Edge fetches from origin (cache miss)
â”œâ”€ Subsequent requests: Edge serves from cache (cache hit)
â””â”€ Low latency (edge near user)

Benefits:
+ Low latency (edge near users)
+ Reduced origin load (90%+ cache hit rate)
+ DDoS protection (absorb traffic at edge)
+ Global scalability (200+ edge locations)
```

**Edge Locations:**

```
CloudFront: 400+ edge locations in 90+ cities
Cloudflare: 300+ cities in 100+ countries
Fastly: 80+ PoPs (Points of Presence)

Request routing:
1. User requests: cdn.example.com/image.jpg
2. DNS resolves to nearest edge (geolocation, latency-based)
3. Edge serves cached content (cache hit)
4. If cache miss, edge fetches from origin

Edge hierarchy:
User â†’ Edge location (cache)
     â†’ Regional edge cache (larger cache)
     â†’ Origin (S3, server)
```

### 2.2 Cache Keys

**Identify unique cached objects**

```
Default cache key:
URL path only: /images/logo.png

Problem: Same URL, different content based on:
â”œâ”€ Query params: /image.png?size=small vs /image.png?size=large
â”œâ”€ Headers: Accept-Encoding (gzip vs brotli)
â”œâ”€ Cookies: User-specific content
â””â”€ User location: Region-specific content

Solution: Custom cache key
Include relevant factors:
â”œâ”€ URL path: /image.png
â”œâ”€ Query params: ?size=small
â”œâ”€ Headers: Accept-Encoding
â””â”€ Cookies: session_id (if user-specific)

Result: Multiple cache entries for same URL
/image.png?size=small + gzip â†’ Cache entry 1
/image.png?size=large + gzip â†’ Cache entry 2
/image.png?size=small + brotli â†’ Cache entry 3
```

**Cache Key Configuration:**

```javascript
// CloudFront cache key policy
{
  "Name": "ImageCachePolicy",
  "MinTTL": 3600,  // Min cache time: 1 hour
  "DefaultTTL": 86400,  // Default: 24 hours
  "MaxTTL": 31536000,  // Max: 1 year
  "ParametersInCacheKeyAndForwardedToOrigin": {
    "EnableAcceptEncodingGzip": true,  // Include encoding in cache key
    "EnableAcceptEncodingBrotli": true,
    "QueryStringsConfig": {
      "QueryStringBehavior": "whitelist",  // Include specific params
      "QueryStrings": ["size", "quality", "format"]  // Only these params affect cache
    },
    "HeadersConfig": {
      "HeaderBehavior": "whitelist",
      "Headers": ["Accept", "CloudFront-Viewer-Country"]  // Region-based caching
    },
    "CookiesConfig": {
      "CookieBehavior": "none"  // Don't include cookies (public content)
    }
  }
}

// Result:
// /image.png?size=small&quality=80 â†’ Cached separately
// /image.png?size=large&quality=80 â†’ Cached separately
// /image.png?size=small&user=123 â†’ Same as first (user param ignored)
```

**Cache Efficiency:**

```
Good cache key (high hit rate):
URL path + essential query params
â””â”€ /products/123?lang=en (language affects content)

Bad cache key (low hit rate):
URL path + all query params + timestamp
â””â”€ /products/123?lang=en&timestamp=1234567890 (unique every time!)

Cache hit rate:
â”œâ”€ 90%+ â†’ Good (most requests served from cache)
â”œâ”€ 70-90% â†’ Acceptable
â””â”€ <70% â†’ Poor (review cache key, TTL)

Metrics:
Cache hit rate = (cache hits) / (total requests)

Example:
1000 requests:
â”œâ”€ 950 cache hits â†’ 95% hit rate âœ“
â””â”€ 50 cache misses (fetch from origin)
```

### 2.3 Cache Invalidation

**Remove stale content from cache**

```
Problem:
File updated on origin (S3):
â”œâ”€ Old version cached at edge (stale)
â”œâ”€ TTL: 24 hours (cached for 1 day)
â””â”€ Users see old content until TTL expires

Solutions:
1. Wait for TTL expiration (simple, slow)
2. Cache invalidation (immediate, costs money)
3. Versioned URLs (best practice, free)
```

**Cache Invalidation Methods:**

```
1. Invalidate by path (CloudFront):
aws cloudfront create-invalidation \
  --distribution-id E123456 \
  --paths "/images/*" "/css/style.css"

Cost: $0.005 per path (first 1000 free/month)
Time: 5-15 minutes to propagate

2. Purge all (Cloudflare):
curl -X POST "https://api.cloudflare.com/client/v4/zones/{zone_id}/purge_cache" \
  -H "Authorization: Bearer {api_token}" \
  -d '{"purge_everything": true}'

3. Purge by tags (Fastly):
curl -X POST "https://api.fastly.com/service/{service_id}/purge/{surrogate_key}"

Surrogate-Key header from origin:
Surrogate-Key: product-123 category-electronics
â””â”€ Purge all objects with key "product-123"
```

**Versioned URLs (Best Practice):**

```
Problem: Invalidation costs money, takes time

Solution: Version in URL (never invalidate)
Old: /css/style.css (same URL, cache must be invalidated)
New: /css/style.v2.css (different URL, new cache entry)

Or hash-based:
/css/style.abc123.css (hash of file content)
â””â”€ File changes â†’ hash changes â†’ new URL

Webpack example:
output: {
  filename: '[name].[contenthash].js'
}
// Generates: main.a1b2c3d4.js (hash changes with content)

HTML:
<link rel="stylesheet" href="/css/style.abc123.css">
â””â”€ Update HTML when CSS changes (no invalidation needed)

Benefits:
+ No invalidation costs
+ Immediate updates (new URL)
+ Long TTL (1 year, aggressive caching)
+ Cache immutable content forever
```

**Cache-Control Headers:**

```
Origin sets cache headers:
Cache-Control: max-age=86400  // Cache for 24 hours
Cache-Control: no-cache  // Revalidate every time
Cache-Control: no-store  // Don't cache
Cache-Control: public, max-age=31536000, immutable  // Cache 1 year, never revalidate

Examples:
â”œâ”€ Static assets (images, CSS, JS): max-age=31536000, immutable
â”œâ”€ HTML pages: max-age=300, must-revalidate (5 min)
â”œâ”€ API responses: no-cache (always revalidate)
â””â”€ Sensitive data: no-store (never cache)

CloudFront respects origin headers (by default)
Or override with CloudFront cache policy
```

### 2.4 Signed URLs

**Restrict access to cached content**

```
Use cases:
â”œâ”€ Paid content (videos, courses)
â”œâ”€ User-specific files (private photos)
â”œâ”€ Time-limited access (download expires)
â””â”€ Prevent hotlinking (others embedding your content)

Unsigned URL (public):
https://cdn.example.com/videos/movie.mp4
â””â”€ Anyone can access (no restrictions)

Signed URL (restricted):
https://cdn.example.com/videos/movie.mp4?Expires=1234567890&Signature=abc123
â”œâ”€ Signature validates request (tamper-proof)
â”œâ”€ Expires after timestamp (time-limited)
â””â”€ Invalid signature â†’ 403 Forbidden
```

**CloudFront Signed URLs:**

```python
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric import padding
from cryptography.hazmat.backends import default_backend
import base64
import json
from datetime import datetime, timedelta

# Generate signed URL
def generate_signed_url(url, key_pair_id, private_key_path, expires_in_hours=1):
    # Expiration time
    expire_time = datetime.utcnow() + timedelta(hours=expires_in_hours)
    expire_timestamp = int(expire_time.timestamp())
    
    # Policy statement
    policy = {
        "Statement": [
            {
                "Resource": url,
                "Condition": {
                    "DateLessThan": {
                        "AWS:EpochTime": expire_timestamp
                    }
                }
            }
        ]
    }
    
    policy_json = json.dumps(policy, separators=(',', ':'))
    
    # Load private key
    with open(private_key_path, 'rb') as f:
        private_key = serialization.load_pem_private_key(
            f.read(),
            password=None,
            backend=default_backend()
        )
    
    # Sign policy
    signature = private_key.sign(
        policy_json.encode('utf-8'),
        padding.PKCS1v15(),
        hashes.SHA1()
    )
    
    # Base64 encode (URL-safe)
    signature_b64 = base64.b64encode(signature).decode('utf-8')
    signature_b64 = signature_b64.replace('+', '-').replace('=', '_').replace('/', '~')
    
    policy_b64 = base64.b64encode(policy_json.encode('utf-8')).decode('utf-8')
    policy_b64 = policy_b64.replace('+', '-').replace('=', '_').replace('/', '~')
    
    # Construct signed URL
    signed_url = f"{url}?Policy={policy_b64}&Signature={signature_b64}&Key-Pair-Id={key_pair_id}"
    
    return signed_url

# Usage
url = "https://d111111abcdef8.cloudfront.net/videos/movie.mp4"
signed_url = generate_signed_url(url, "APKAEXAMPLE", "/path/to/private_key.pem")

# User accesses signed URL:
# https://...cloudfront.net/videos/movie.mp4?Policy=...&Signature=...&Key-Pair-Id=...
```

**Signed Cookies (Alternative):**

```python
# Set signed cookies (for multiple files)
def set_signed_cookies(domain, key_pair_id, private_key_path, expires_in_hours=1):
    # Same policy and signature generation as signed URLs
    # ...
    
    # Set cookies instead of URL parameters
    response.set_cookie(
        'CloudFront-Policy',
        policy_b64,
        domain=domain,
        secure=True,
        httponly=True
    )
    response.set_cookie(
        'CloudFront-Signature',
        signature_b64,
        domain=domain,
        secure=True,
        httponly=True
    )
    response.set_cookie(
        'CloudFront-Key-Pair-Id',
        key_pair_id,
        domain=domain,
        secure=True,
        httponly=True
    )

# Benefit: All resources under domain automatically signed
# No need to sign each URL individually
```

**Signed URLs vs Presigned URLs:**

```
Presigned URLs (S3):
â”œâ”€ Purpose: Grant temporary access to S3 objects
â”œâ”€ Use case: Direct upload/download to S3
â”œâ”€ Signature: HMAC (AWS credentials)
â””â”€ No CDN (direct S3 access)

Signed URLs (CloudFront):
â”œâ”€ Purpose: Restrict access to CDN-cached content
â”œâ”€ Use case: Paid/private content via CDN
â”œâ”€ Signature: RSA (private key)
â””â”€ CDN benefits (low latency, caching)

Combined:
User â†’ Server: Request video
Server â†’ User: CloudFront signed URL
User â†’ CloudFront: Access video (signed URL)
CloudFront â†’ S3: Fetch video (if cache miss, using presigned URL or IAM role)
```

### 2.5 Image Resize at Edge

**Dynamic image transformation without origin processing**

```
Problem:
Multiple image sizes needed:
â”œâ”€ Thumbnail: 100Ã—100
â”œâ”€ Mobile: 640Ã—480
â”œâ”€ Desktop: 1920Ã—1080
â””â”€ Retina: 3840Ã—2160

Traditional approach:
â”œâ”€ Pre-generate all sizes (storage cost, maintenance)
â”œâ”€ Or resize on-demand at origin (CPU cost, latency)

Edge solution:
â”œâ”€ Store original image only (single copy)
â”œâ”€ Resize at edge based on request (on-demand)
â””â”€ Cache resized versions (subsequent requests fast)
```

**CloudFront Functions (Lightweight):**

```javascript
// CloudFront Function (viewer request)
function handler(event) {
    var request = event.request;
    var uri = request.uri;
    var querystring = request.querystring;
    
    // Extract size from query params
    var width = querystring.width ? querystring.width.value : '800';
    var height = querystring.height ? querystring.height.value : '600';
    
    // Rewrite URI to include size
    request.uri = uri.replace(/\.(jpg|png)$/, `-${width}x${height}.$1`);
    
    return request;
}

// Request: /images/photo.jpg?width=300&height=200
// Rewritten: /images/photo-300x200.jpg
// (Lambda@Edge or origin generates this if not cached)
```

**Lambda@Edge (Full Processing):**

```javascript
const AWS = require('aws-sdk');
const Sharp = require('sharp');  // Image processing library
const S3 = new AWS.S3();

exports.handler = async (event) => {
    const response = event.Records[0].cf.response;
    const request = event.Records[0].cf.request;
    
    // If image not found (404), generate it
    if (response.status == '404') {
        const params = request.querystring;
        const width = parseInt(params.width || '800');
        const height = parseInt(params.height || '600');
        
        // Extract original image key
        const key = request.uri.substring(1);  // Remove leading /
        const originalKey = key.replace(/-\d+x\d+/, '');  // Remove size suffix
        
        try {
            // Fetch original from S3
            const s3Object = await S3.getObject({
                Bucket: 'my-bucket',
                Key: originalKey
            }).promise();
            
            // Resize image
            const resizedImage = await Sharp(s3Object.Body)
                .resize(width, height, {
                    fit: 'cover',  // Crop to fit
                    position: 'center'
                })
                .jpeg({ quality: 80 })
                .toBuffer();
            
            // Save resized image to S3
            await S3.putObject({
                Bucket: 'my-bucket',
                Key: key,
                Body: resizedImage,
                ContentType: 'image/jpeg'
            }).promise();
            
            // Return resized image
            return {
                status: '200',
                statusDescription: 'OK',
                headers: {
                    'content-type': [{ value: 'image/jpeg' }],
                    'cache-control': [{ value: 'max-age=31536000' }]
                },
                body: resizedImage.toString('base64'),
                bodyEncoding: 'base64'
            };
            
        } catch (error) {
            return response;  // Return original 404
        }
    }
    
    return response;
};
```

**Image Optimization Services:**

```
Cloudinary:
URL-based transformations:
https://res.cloudinary.com/demo/image/upload/w_300,h_200,c_fill/sample.jpg
â”œâ”€ w_300: Width 300px
â”œâ”€ h_200: Height 200px
â”œâ”€ c_fill: Crop to fill
â””â”€ Automatic format (WebP, AVIF) based on browser support

Imgix:
https://demo.imgix.net/image.jpg?w=300&h=200&fit=crop&auto=format
â”œâ”€ auto=format: Automatic format selection
â”œâ”€ auto=compress: Automatic compression
â””â”€ DPR-aware: Serve retina images based on device

Cloudflare Images:
https://imagedelivery.net/account-hash/image-id/variant
Variants defined in dashboard:
â”œâ”€ thumbnail: 100Ã—100
â”œâ”€ mobile: 640Ã—480
â””â”€ desktop: 1920Ã—1080

Benefits:
+ No origin processing (edge handles resizing)
+ Single source image (store original only)
+ Caching (resized images cached at edge)
+ Format optimization (WebP, AVIF)
+ DPR-aware (retina images)
```

**Responsive Images (HTML):**

```html
<!-- Srcset: Browser chooses appropriate size -->
<img src="image-800.jpg"
     srcset="image-400.jpg 400w,
             image-800.jpg 800w,
             image-1600.jpg 1600w"
     sizes="(max-width: 600px) 400px,
            (max-width: 1200px) 800px,
            1600px"
     alt="Responsive image">

<!-- Picture: Different formats -->
<picture>
  <source srcset="image.avif" type="image/avif">
  <source srcset="image.webp" type="image/webp">
  <img src="image.jpg" alt="Fallback">
</picture>

<!-- Combined with CDN -->
<img src="https://cdn.example.com/image.jpg?w=800"
     srcset="https://cdn.example.com/image.jpg?w=400 400w,
             https://cdn.example.com/image.jpg?w=800 800w,
             https://cdn.example.com/image.jpg?w=1600 1600w">
```

---

## 3. Advanced Patterns

### 3.1 Upload Flow (Direct to S3)

**Secure client-side upload with presigned URL**

```
Flow:
1. Client â†’ Server: Request upload (filename, content type)
2. Server: Validate request (auth, file size, type)
3. Server: Generate presigned URL (S3)
4. Server â†’ Client: Presigned URL
5. Client â†’ S3: Upload file (PUT request)
6. S3 â†’ Client: Success
7. Client â†’ Server: Notify completion (metadata)
8. Server: Create database record (file reference)

Benefits:
+ Scalable (no server bandwidth)
+ Fast (direct to S3, no proxy)
+ Secure (presigned URL expires)
```

**Implementation:**

```python
# Server endpoint
from flask import Flask, request, jsonify
import boto3
import uuid

app = Flask(__name__)
s3_client = boto3.client('s3')

@app.route('/upload/request', methods=['POST'])
def request_upload():
    # Validate request
    user_id = request.user_id  # From auth middleware
    filename = request.json['filename']
    content_type = request.json['content_type']
    file_size = request.json['file_size']
    
    # Validate
    if file_size > 100 * 1024 * 1024:  # 100 MB max
        return jsonify({'error': 'File too large'}), 400
    
    if content_type not in ['image/jpeg', 'image/png', 'video/mp4']:
        return jsonify({'error': 'Invalid file type'}), 400
    
    # Generate unique key
    file_id = str(uuid.uuid4())
    extension = filename.split('.')[-1]
    key = f'uploads/{user_id}/{file_id}.{extension}'
    
    # Generate presigned URL
    presigned_url = s3_client.generate_presigned_url(
        'put_object',
        Params={
            'Bucket': 'my-bucket',
            'Key': key,
            'ContentType': content_type,
            'ContentLength': file_size  # Enforce size
        },
        ExpiresIn=3600  # 1 hour
    )
    
    return jsonify({
        'upload_url': presigned_url,
        'file_id': file_id,
        'key': key
    })

@app.route('/upload/complete', methods=['POST'])
def upload_complete():
    user_id = request.user_id
    file_id = request.json['file_id']
    key = request.json['key']
    
    # Verify file exists in S3
    try:
        s3_client.head_object(Bucket='my-bucket', Key=key)
    except:
        return jsonify({'error': 'File not found'}), 404
    
    # Save metadata to database
    db.files.insert({
        'file_id': file_id,
        'user_id': user_id,
        'key': key,
        'uploaded_at': datetime.utcnow()
    })
    
    return jsonify({'success': True})

# Client-side (JavaScript)
async function uploadFile(file) {
    // Request presigned URL
    const response = await fetch('/upload/request', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
            filename: file.name,
            content_type: file.type,
            file_size: file.size
        })
    });
    
    const { upload_url, file_id, key } = await response.json();
    
    // Upload to S3
    await fetch(upload_url, {
        method: 'PUT',
        headers: { 'Content-Type': file.type },
        body: file
    });
    
    // Notify server
    await fetch('/upload/complete', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ file_id, key })
    });
}
```

### 3.2 Video Streaming (HLS)

**Adaptive bitrate streaming for videos**

```
HLS (HTTP Live Streaming):
1. Transcode video to multiple bitrates
   â”œâ”€ 360p: 500 Kbps
   â”œâ”€ 720p: 2 Mbps
   â””â”€ 1080p: 5 Mbps

2. Split each into segments (10 sec chunks)
   â”œâ”€ 360p: segment0.ts, segment1.ts, ...
   â”œâ”€ 720p: segment0.ts, segment1.ts, ...
   â””â”€ 1080p: segment0.ts, segment1.ts, ...

3. Create manifest (playlist.m3u8)
   Lists all bitrates and segments

4. Client downloads manifest, chooses bitrate
   â”œâ”€ Fast connection: 1080p
   â”œâ”€ Slow connection: 360p
   â””â”€ Adaptive: Switch bitrate based on bandwidth

5. Client downloads segments sequentially
   Smooth playback, no buffering
```

**Transcoding with FFmpeg:**

```bash
# Generate HLS playlist
ffmpeg -i input.mp4 \
  -vf scale=w=640:h=360 -c:v h264 -b:v 500k -c:a aac -b:a 128k -f hls -hls_time 10 -hls_list_size 0 360p.m3u8 \
  -vf scale=w=1280:h=720 -c:v h264 -b:v 2000k -c:a aac -b:a 192k -f hls -hls_time 10 -hls_list_size 0 720p.m3u8 \
  -vf scale=w=1920:h=1080 -c:v h264 -b:v 5000k -c:a aac -b:a 256k -f hls -hls_time 10 -hls_list_size 0 1080p.m3u8

# Master playlist (lists all bitrates)
cat > master.m3u8 << EOF
#EXTM3U
#EXT-X-STREAM-INF:BANDWIDTH=500000,RESOLUTION=640x360
360p.m3u8
#EXT-X-STREAM-INF:BANDWIDTH=2000000,RESOLUTION=1280x720
720p.m3u8
#EXT-X-STREAM-INF:BANDWIDTH=5000000,RESOLUTION=1920x1080
1080p.m3u8
EOF

# Upload to S3
aws s3 sync . s3://my-bucket/videos/video-id/ --acl public-read
```

**Client Playback:**

```html
<video id="video" controls></video>

<script src="https://cdn.jsdelivr.net/npm/hls.js@latest"></script>
<script>
  const video = document.getElementById('video');
  const url = 'https://cdn.example.com/videos/video-id/master.m3u8';
  
  if (Hls.isSupported()) {
    const hls = new Hls();
    hls.loadSource(url);
    hls.attachMedia(video);
    
    hls.on(Hls.Events.MANIFEST_PARSED, () => {
      video.play();
    });
    
    // Auto-switch quality based on bandwidth
    hls.on(Hls.Events.LEVEL_SWITCHED, (event, data) => {
      console.log('Quality:', hls.levels[data.level].height + 'p');
    });
  } else if (video.canPlayType('application/vnd.apple.mpegurl')) {
    // Native HLS (Safari)
    video.src = url;
  }
</script>
```

### 3.3 DDoS Protection

**Protect origin from traffic spikes**

```
CDN as shield:
Attacker â†’ CDN (absorbs traffic)
         â†’ Origin (protected, rate limited)

Protection layers:
1. CDN absorbs volumetric attacks (Tbps scale)
2. WAF (Web Application Firewall) filters malicious requests
3. Rate limiting (per IP, per endpoint)
4. Challenge (CAPTCHA for suspicious traffic)
5. Origin shielding (CDN-only access to origin)

CloudFront + AWS Shield:
â”œâ”€ Shield Standard: Free (DDoS protection)
â””â”€ Shield Advanced: $3000/month (24/7 response team)

Cloudflare:
â”œâ”€ DDoS mitigation (automatic)
â”œâ”€ Rate limiting (customizable)
â””â”€ Bot management (block bots)
```

---

## Best Practices Summary

```
Object Storage (S3, GCS):
âœ“ Use presigned URLs for client uploads (scalable, secure)
âœ“ Enable versioning for critical data (accidental deletion protection)
âœ“ Implement lifecycle policies (archive old data, reduce costs)
âœ“ Use multipart upload for large files (>100 MB, parallel, resumable)
âœ“ Set appropriate storage class (Standard, IA, Glacier based on access)
âœ“ Enable transfer acceleration for distant uploads (50-500% faster)
âœ“ Use bucket policies and IAM for access control (least privilege)
âœ— Don't use server bandwidth for uploads (use presigned URLs)
âœ— Don't store all data in Standard (expensive, use lifecycle)
âœ— Don't expose bucket publicly (use presigned URLs, signed URLs)

CDN:
âœ“ Use versioned URLs (cache busting, no invalidation costs)
âœ“ Set long TTL for static assets (1 year, aggressive caching)
âœ“ Configure cache keys carefully (balance hit rate and uniqueness)
âœ“ Use signed URLs for private content (paid videos, user files)
âœ“ Compress assets (gzip, brotli, reduce bandwidth)
âœ“ Monitor cache hit rate (90%+ ideal)
âœ“ Use CDN for API responses (cache GET requests, reduce origin load)
âœ— Don't invalidate frequently (expensive, use versioned URLs)
âœ— Don't include unnecessary params in cache key (low hit rate)
âœ— Don't cache user-specific content publicly (privacy risk)

Image Optimization:
âœ“ Resize at edge (Lambda@Edge, Cloudinary, Imgix)
âœ“ Store original only (dynamic resize on-demand)
âœ“ Use modern formats (WebP, AVIF, 30-50% smaller)
âœ“ Implement responsive images (srcset, sizes)
âœ“ Lazy load images (below fold, save bandwidth)
âœ“ Compress images (lossy for photos, lossless for graphics)
âœ— Don't pre-generate all sizes (storage cost, maintenance)
âœ— Don't resize at origin (CPU cost, latency)

Video Streaming:
âœ“ Use HLS or DASH (adaptive bitrate)
âœ“ Transcode to multiple bitrates (360p, 720p, 1080p)
âœ“ Use CDN for delivery (low latency, scalability)
âœ“ Protect with signed URLs (paid content)
âœ“ Monitor playback quality (buffering, startup time)
âœ— Don't use progressive download (no adaptive bitrate)
âœ— Don't serve from origin (bandwidth cost, poor performance)

Security:
âœ“ Use HTTPS everywhere (encrypt in transit)
âœ“ Implement signed URLs for private content (time-limited)
âœ“ Set CORS policies (restrict cross-origin access)
âœ“ Rate limit API endpoints (prevent abuse)
âœ“ WAF rules (block SQL injection, XSS)
âœ“ Origin shielding (CDN-only access)
âœ— Don't expose S3 buckets publicly (use presigned URLs)
âœ— Don't reuse presigned URLs (one-time use preferred)

Cost Optimization:
âœ“ Use lifecycle policies (auto-archive, auto-delete)
âœ“ Compression (reduce storage and bandwidth)
âœ“ Versioned URLs (avoid invalidation costs)
âœ“ CloudFront â†’ S3 (no data transfer charges within AWS)
âœ“ Reserved capacity (committed use discount)
âœ— Don't store all versions forever (expire old versions)
âœ— Don't invalidate frequently (use versioned URLs)
```

Complete file, media, and edge delivery foundation! ğŸ“ğŸ¬ğŸŒ