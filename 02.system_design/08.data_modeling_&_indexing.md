# Data Modeling & Indexing

## What is Data Modeling?

**Structuring data for efficient storage, retrieval, and integrity**

```
Goals:
â”œâ”€ Minimize redundancy (avoid duplicate data)
â”œâ”€ Maintain integrity (relationships, constraints)
â”œâ”€ Optimize access patterns (fast queries)
â””â”€ Balance trade-offs (consistency vs performance)

Approaches:
â”œâ”€ Normalization (relational, minimize redundancy)
â””â”€ Denormalization (NoSQL, optimize reads)
```

---

## 1. Normalization vs Denormalization

### 1.1 Normalization (Relational Databases)

**Split data into multiple tables to reduce redundancy**

#### Normal Forms

**0NF (Unnormalized) - All data in one table:**
```
Orders table:
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID â”‚ User â”‚ UserEmail â”‚ Product  â”‚ Price    â”‚ Categoryâ”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1  â”‚Alice â”‚a@e.com    â”‚Laptop    â”‚999       â”‚Electronicsâ”‚
â”‚ 2  â”‚Alice â”‚a@e.com    â”‚Mouse     â”‚25        â”‚Electronicsâ”‚
â”‚ 3  â”‚Bob   â”‚b@e.com    â”‚Laptop    â”‚999       â”‚Electronicsâ”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Problems:
âŒ Redundancy: "Alice", "a@e.com", "Laptop", "999" repeated
âŒ Update anomaly: Change Alice's email â†’ must update multiple rows
âŒ Insertion anomaly: Can't add user without order
âŒ Deletion anomaly: Delete Bob's order â†’ lose Bob's info
```

**1NF (First Normal Form) - Atomic values, no repeating groups:**
```
Requirements:
âœ“ Each column contains atomic (indivisible) values
âœ“ No repeating groups or arrays
âœ“ Each row is unique (primary key)

Bad (violates 1NF):
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID â”‚ User â”‚ Products            â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1  â”‚Alice â”‚Laptop, Mouse        â”‚  âŒ Non-atomic (comma-separated)
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Good (1NF):
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID â”‚ User â”‚ Product â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1  â”‚Alice â”‚ Laptop  â”‚
â”‚ 2  â”‚Alice â”‚ Mouse   â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**2NF (Second Normal Form) - Remove partial dependencies:**
```
Requirement: 1NF + No partial dependency on composite key

Bad (violates 2NF):
Orders table (composite key: OrderID + ProductID):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OrderID â”‚ ProductID â”‚ Product  â”‚ Price    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1       â”‚ 101       â”‚ Laptop   â”‚ 999      â”‚
â”‚ 1       â”‚ 102       â”‚ Mouse    â”‚ 25       â”‚
â”‚ 2       â”‚ 101       â”‚ Laptop   â”‚ 999      â”‚  âŒ Price depends only on ProductID
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†‘
          Partial dependency: ProductID â†’ Product, Price

Good (2NF):
Orders table:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OrderID â”‚ ProductID â”‚ Quantity â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1       â”‚ 101       â”‚ 1        â”‚
â”‚ 1       â”‚ 102       â”‚ 2        â”‚
â”‚ 2       â”‚ 101       â”‚ 1        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Products table:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ProductID â”‚ Product â”‚ Price â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 101       â”‚ Laptop  â”‚ 999   â”‚
â”‚ 102       â”‚ Mouse   â”‚ 25    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
```

**3NF (Third Normal Form) - Remove transitive dependencies:**
```
Requirement: 2NF + No transitive dependency

Bad (violates 3NF):
Orders table:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OrderID â”‚ UserID â”‚ UserEmail â”‚ Product  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1       â”‚ 123    â”‚ a@e.com   â”‚ Laptop   â”‚
â”‚ 2       â”‚ 123    â”‚ a@e.com   â”‚ Mouse    â”‚
â”‚ 3       â”‚ 456    â”‚ b@e.com   â”‚ Laptop   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“          â†‘
        UserID â†’ UserEmail (transitive dependency)

Good (3NF):
Orders table:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OrderID â”‚ UserID â”‚ ProductID â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1       â”‚ 123    â”‚ 101       â”‚
â”‚ 2       â”‚ 123    â”‚ 102       â”‚
â”‚ 3       â”‚ 456    â”‚ 101       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Users table:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ UserID â”‚ Name  â”‚ Email     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 123    â”‚ Alice â”‚ a@e.com   â”‚
â”‚ 456    â”‚ Bob   â”‚ b@e.com   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Products table:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ProductID â”‚ Product â”‚ Price â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 101       â”‚ Laptop  â”‚ 999   â”‚
â”‚ 102       â”‚ Mouse   â”‚ 25    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
```

**BCNF (Boyce-Codd Normal Form) - Stricter 3NF:**
```
Requirement: Every determinant must be a candidate key

Example: Student courses with professors
Bad (violates BCNF):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ StudentID â”‚ Course â”‚ Professor â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1         â”‚ DB     â”‚ Smith     â”‚
â”‚ 2         â”‚ DB     â”‚ Smith     â”‚  â† Course determines Professor
â”‚ 3         â”‚ AI     â”‚ Jones     â”‚     but Course is not a key
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Problem: Professor depends on Course, not StudentID+Course

Good (BCNF):
StudentCourses:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ StudentID â”‚ CourseID â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1         â”‚ CS101    â”‚
â”‚ 2         â”‚ CS101    â”‚
â”‚ 3         â”‚ CS201    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Courses:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CourseID â”‚ Course â”‚ ProfessorID â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CS101    â”‚ DB     â”‚ 1           â”‚
â”‚ CS201    â”‚ AI     â”‚ 2           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Professors:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ProfessorID â”‚ Name      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1           â”‚ Smith     â”‚
â”‚ 2           â”‚ Jones     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Normalized Schema Example (E-commerce)

```sql
-- Users
CREATE TABLE users (
    user_id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Products
CREATE TABLE products (
    product_id SERIAL PRIMARY KEY,
    name VARCHAR(200) NOT NULL,
    description TEXT,
    price DECIMAL(10, 2) NOT NULL,
    category_id INT REFERENCES categories(category_id),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Categories
CREATE TABLE categories (
    category_id SERIAL PRIMARY KEY,
    name VARCHAR(100) UNIQUE NOT NULL,
    parent_id INT REFERENCES categories(category_id)
);

-- Orders
CREATE TABLE orders (
    order_id SERIAL PRIMARY KEY,
    user_id INT REFERENCES users(user_id),
    status VARCHAR(20) NOT NULL,
    total DECIMAL(10, 2),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Order Items (junction table)
CREATE TABLE order_items (
    order_item_id SERIAL PRIMARY KEY,
    order_id INT REFERENCES orders(order_id),
    product_id INT REFERENCES products(product_id),
    quantity INT NOT NULL,
    price DECIMAL(10, 2) NOT NULL,  -- Price at time of order
    UNIQUE(order_id, product_id)
);

-- Query: Get order details with product info
SELECT 
    o.order_id,
    u.username,
    p.name AS product_name,
    oi.quantity,
    oi.price,
    oi.quantity * oi.price AS subtotal
FROM orders o
JOIN users u ON o.user_id = u.user_id
JOIN order_items oi ON o.order_id = oi.order_id
JOIN products p ON oi.product_id = p.product_id
WHERE o.order_id = 123;

-- Multiple JOINs needed (normalized design)
```

**Benefits of Normalization:**
```
âœ“ No redundancy (data stored once)
âœ“ Data integrity (foreign keys, constraints)
âœ“ Easier updates (change in one place)
âœ“ Smaller storage (less duplication)
âœ“ Consistency (ACID properties)

Drawbacks:
âœ— Complex queries (multiple JOINs)
âœ— Slower reads (JOIN overhead)
âœ— Not suitable for distributed systems (JOINs across nodes expensive)
```

### 1.2 Denormalization (NoSQL Databases)

**Combine data into fewer tables/documents to optimize reads**

#### Denormalized Schema Example (MongoDB)

```javascript
// Single document contains all order data
{
  "_id": ObjectId("..."),
  "order_id": 123,
  "user": {
    "user_id": 456,
    "username": "alice",
    "email": "alice@example.com"
  },
  "status": "completed",
  "items": [
    {
      "product_id": 101,
      "name": "Laptop",
      "price": 999.99,
      "quantity": 1,
      "category": "Electronics"
    },
    {
      "product_id": 102,
      "name": "Mouse",
      "price": 24.99,
      "quantity": 2,
      "category": "Electronics"
    }
  ],
  "total": 1049.97,
  "created_at": ISODate("2024-01-15T10:30:00Z")
}

// Query: Get order details (single document lookup, fast!)
db.orders.findOne({ order_id: 123 })

// No JOINs needed, all data in one document
```

**Cassandra Wide-Row Example:**
```sql
-- Denormalized table optimized for queries
CREATE TABLE orders_by_user (
    user_id UUID,
    order_date TIMESTAMP,
    order_id UUID,
    product_name TEXT,
    product_price DECIMAL,
    quantity INT,
    total DECIMAL,
    PRIMARY KEY (user_id, order_date, order_id)
) WITH CLUSTERING ORDER BY (order_date DESC);

-- Query: Get user's recent orders (single partition read, fast!)
SELECT * FROM orders_by_user 
WHERE user_id = 456 
AND order_date > '2024-01-01'
LIMIT 10;

-- All data in one partition, no JOINs
```

#### Denormalization Strategies

**1. Embedding (Nested Documents):**
```javascript
// User with embedded addresses
{
  "user_id": 123,
  "name": "Alice",
  "addresses": [
    {
      "type": "home",
      "street": "123 Main St",
      "city": "NYC",
      "zip": "10001"
    },
    {
      "type": "work",
      "street": "456 Office Blvd",
      "city": "NYC",
      "zip": "10002"
    }
  ]
}

// One query to get user and all addresses
// Good when: Addresses always accessed with user, few addresses per user
```

**2. Duplication (Redundant Data):**
```javascript
// Order duplicates user data
{
  "order_id": 123,
  "user_id": 456,
  "user_name": "Alice",       // â† Duplicated from users collection
  "user_email": "alice@e.com", // â† Duplicated
  "items": [...]
}

// User updates require updating all orders (eventual consistency)
// Good when: User data rarely changes, read-heavy workload
```

**3. Materialized Views (Pre-computed Aggregations):**
```javascript
// Real-time: users collection (normalized)
{
  "user_id": 123,
  "name": "Alice",
  "total_orders": 0,
  "total_spent": 0
}

// Materialized view: user_stats (denormalized)
{
  "user_id": 123,
  "name": "Alice",
  "total_orders": 47,           // â† Pre-computed
  "total_spent": 4532.50,       // â† Pre-computed
  "avg_order_value": 96.43,     // â† Pre-computed
  "last_order_date": "2024-01-15"
}

// Updated on each order (write overhead)
// Good when: Expensive aggregations, read-heavy analytics
```

**4. Query Table Pattern (Cassandra):**
```sql
-- Multiple tables for different query patterns

-- Query 1: Orders by user
CREATE TABLE orders_by_user (
    user_id UUID,
    order_date TIMESTAMP,
    order_id UUID,
    total DECIMAL,
    PRIMARY KEY (user_id, order_date)
);

-- Query 2: Orders by product
CREATE TABLE orders_by_product (
    product_id UUID,
    order_date TIMESTAMP,
    order_id UUID,
    user_id UUID,
    quantity INT,
    PRIMARY KEY (product_id, order_date)
);

-- Query 3: Orders by date range
CREATE TABLE orders_by_date (
    date_bucket TEXT,  -- "2024-01"
    order_date TIMESTAMP,
    order_id UUID,
    user_id UUID,
    total DECIMAL,
    PRIMARY KEY (date_bucket, order_date)
);

-- Same data stored 3 times (write amplification)
-- Each table optimized for specific query
```

#### Normalization vs Denormalization Trade-offs

| Aspect | Normalized (SQL) | Denormalized (NoSQL) |
|--------|------------------|----------------------|
| **Redundancy** | Minimal | High (intentional duplication) |
| **Storage** | Smaller | Larger (duplicate data) |
| **Writes** | Faster (single location) | Slower (multiple locations) |
| **Reads** | Slower (JOINs) | Faster (single query) |
| **Consistency** | Strong (ACID) | Eventual (multiple copies) |
| **Scalability** | Vertical (harder to scale) | Horizontal (easier to scale) |
| **Schema Changes** | Harder (migrations) | Easier (flexible schema) |
| **Use Case** | OLTP, complex queries | OLAP, simple queries, high scale |

**When to Normalize:**
```
âœ“ ACID transactions required (banking, inventory)
âœ“ Complex queries with multiple JOINs
âœ“ Data frequently updated (minimize update points)
âœ“ Storage cost critical
âœ“ Single-node database (PostgreSQL, MySQL)

Example: Financial system, ERP, CRM
```

**When to Denormalize:**
```
âœ“ Read-heavy workload (reads >> writes)
âœ“ Simple query patterns (no complex JOINs)
âœ“ Horizontal scaling needed (distributed system)
âœ“ Latency critical (single-query access)
âœ“ Eventual consistency acceptable

Example: Social media feeds, analytics, caching
```

#### Hybrid Approach (Best of Both Worlds)

```
Primary database (PostgreSQL): Normalized, ACID
â”œâ”€ Orders, users, products (source of truth)
â””â”€ Complex updates, transactions

Read replicas: Denormalized, optimized for reads
â”œâ”€ Materialized views for analytics
â”œâ”€ Pre-joined tables for dashboards
â””â”€ Updated asynchronously from primary

Cache (Redis): Heavily denormalized
â”œâ”€ User sessions with full profile
â”œâ”€ Product catalog with reviews, ratings
â””â”€ Shopping cart with product details

Search (Elasticsearch): Denormalized documents
â”œâ”€ Products with category, brand, attributes
â”œâ”€ Full-text search optimized
â””â”€ Updated from primary (eventual consistency)

Example architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Write â†’ PostgreSQL (normalized)        â”‚
â”‚           â†“                             â”‚
â”‚         CDC (Change Data Capture)       â”‚
â”‚           â†“                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚    â†“             â†“              â†“       â”‚
â”‚  Redis      Elasticsearch   Snowflake  â”‚
â”‚ (cache)      (search)       (analytics)â”‚
â”‚ denorm       denorm          denorm     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Indexes

**Data structures to speed up queries**

### 2.1 Primary Index (Clustered Index)

```
Determines physical storage order

SQL Server, MySQL (InnoDB):
CREATE TABLE users (
    id INT PRIMARY KEY,  -- â† Clustered index
    name VARCHAR(100),
    email VARCHAR(100)
);

Data stored in B-tree ordered by `id`:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Leaf nodes contain actual rows:   â”‚
â”‚ [id=1, name=Alice, email=a@e.com] â”‚
â”‚ [id=2, name=Bob, email=b@e.com]   â”‚
â”‚ [id=3, name=Charlie, email=c@e.com]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
+ Fast range queries on primary key (ORDER BY id)
+ No extra lookup (data in index)

Limitations:
- Only one per table (physical order is unique)
- Expensive to reorder (if key changes frequently)
```

### 2.2 Secondary Index (Non-Clustered Index)

```
Separate structure pointing to data

PostgreSQL, MySQL (InnoDB secondary):
CREATE INDEX idx_users_email ON users(email);

Index structure:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Index B-tree (email â†’ row pointer): â”‚
â”‚ a@e.com â†’ Row 1                     â”‚
â”‚ b@e.com â†’ Row 2                     â”‚
â”‚ c@e.com â†’ Row 3                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Table (actual data):                â”‚
â”‚ Row 1: [id=1, name=Alice, ...]      â”‚
â”‚ Row 2: [id=2, name=Bob, ...]        â”‚
â”‚ Row 3: [id=3, name=Charlie, ...]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Query: SELECT * FROM users WHERE email = 'a@e.com'
1. Search index: email='a@e.com' â†’ Row pointer 1
2. Fetch row: Row 1 â†’ [id=1, name=Alice, email=a@e.com]

Two lookups (index + table), but still much faster than full scan
```

**Multiple Secondary Indexes:**
```sql
CREATE TABLE products (
    product_id SERIAL PRIMARY KEY,
    name VARCHAR(200),
    category VARCHAR(50),
    price DECIMAL(10, 2),
    brand VARCHAR(50),
    created_at TIMESTAMP
);

-- Indexes for different query patterns
CREATE INDEX idx_products_category ON products(category);
CREATE INDEX idx_products_price ON products(price);
CREATE INDEX idx_products_brand ON products(brand);
CREATE INDEX idx_products_created_at ON products(created_at);

-- Query optimizer chooses best index
SELECT * FROM products WHERE category = 'Electronics';
-- Uses: idx_products_category

SELECT * FROM products WHERE price < 100;
-- Uses: idx_products_price

SELECT * FROM products WHERE category = 'Electronics' AND price < 100;
-- Uses: idx_products_category OR idx_products_price (depends on selectivity)
-- Better: Composite index
```

#### Composite Index (Multi-Column)

```sql
CREATE INDEX idx_products_category_price ON products(category, price);

Index structure (B-tree ordered by category, then price):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Electronics, $50  â†’ Row 1             â”‚
â”‚ Electronics, $100 â†’ Row 2             â”‚
â”‚ Electronics, $500 â†’ Row 3             â”‚
â”‚ Furniture, $200   â†’ Row 4             â”‚
â”‚ Furniture, $500   â†’ Row 5             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Helps queries:
âœ“ WHERE category = 'Electronics'  (leftmost prefix)
âœ“ WHERE category = 'Electronics' AND price < 100  (both columns)
âœ“ WHERE category = 'Electronics' ORDER BY price  (sorted by price within category)

Doesn't help:
âœ— WHERE price < 100  (price not leftmost, can't use index)

Rule: Order matters! (category, price) â‰  (price, category)
```

**Leftmost Prefix Rule:**
```sql
Index: (col1, col2, col3)

Can use index:
âœ“ WHERE col1 = ?
âœ“ WHERE col1 = ? AND col2 = ?
âœ“ WHERE col1 = ? AND col2 = ? AND col3 = ?
âœ“ WHERE col1 = ? AND col3 = ?  (partially, only col1)

Cannot use index:
âœ— WHERE col2 = ?
âœ— WHERE col3 = ?
âœ— WHERE col2 = ? AND col3 = ?

Think of index as a phone book:
â”œâ”€ Sorted by last name (col1), then first name (col2)
â”œâ”€ Can find: last name "Smith" âœ“
â”œâ”€ Can find: last name "Smith", first name "John" âœ“
â””â”€ Cannot find: first name "John" (no last name) âœ—
```

**Index Column Order Optimization:**
```sql
-- Bad order (low selectivity first)
CREATE INDEX idx_bad ON users(gender, country, user_id);
-- gender: 2 values (M/F)
-- country: ~200 values
-- user_id: millions of values

Index tree: [M, USA, 123], [M, USA, 124], [M, USA, 125], ...
Not very selective at first levels (many rows with M, USA)

-- Good order (high selectivity first)
CREATE INDEX idx_good ON users(user_id, country, gender);
-- user_id: millions of values (very selective)
-- country: ~200 values
-- gender: 2 values

Index tree: [123, USA, M], [124, UK, F], [125, Canada, M], ...
Very selective at first level (unique user_id)

Rule: Order by cardinality (high â†’ low) for equality queries
Exception: If filtering by low-cardinality column frequently, consider it first
```

### 2.3 Covering Index (Index-Only Scan)

```
Index contains all columns needed by query (no table access)

Example:
CREATE INDEX idx_users_email_name ON users(email, name);

Query:
SELECT name FROM users WHERE email = 'alice@example.com';

Execution:
1. Search index: email='alice@...' â†’ Find entry
2. Index contains: [email='alice@...', name='Alice']
3. Return name='Alice' (no table access!)

vs. non-covering index:
CREATE INDEX idx_users_email ON users(email);

1. Search index: email='alice@...' â†’ Get row pointer
2. Fetch row from table â†’ Get name='Alice'  (extra I/O)
3. Return name='Alice'

Covering index is faster (one lookup vs. two)
```

**INCLUDE Clause (PostgreSQL, SQL Server):**
```sql
-- PostgreSQL 11+
CREATE INDEX idx_users_email_covering ON users(email) INCLUDE (name, created_at);

-- Index structure:
-- Key: email (used for searching, sorting)
-- Included: name, created_at (only stored, not searchable)

Query (uses covering index):
SELECT name, created_at FROM users WHERE email = 'alice@example.com';

Query (cannot use covering index):
SELECT * FROM users WHERE name = 'Alice';  -- name is included, not indexed
```

**When to Use Covering Indexes:**
```
âœ“ Frequently run queries with few columns
âœ“ High-traffic queries (performance critical)
âœ“ Analytical queries (SELECT few columns from large table)

âœ— SELECT * queries (can't cover all columns efficiently)
âœ— Infrequent queries (index overhead not worth it)

Example: Leaderboard query
CREATE INDEX idx_users_score_covering ON users(score DESC, username, avatar_url);

SELECT username, avatar_url FROM users ORDER BY score DESC LIMIT 10;
-- Index-only scan (very fast, no table access)
```

### 2.4 Partial Index (Filtered Index)

```
Index only subset of rows

Example:
CREATE INDEX idx_active_users ON users(email) WHERE active = true;

Index only contains rows where active = true
Smaller index â†’ Faster queries, less storage

Query (uses index):
SELECT * FROM users WHERE email = 'alice@example.com' AND active = true;

Query (doesn't use index):
SELECT * FROM users WHERE email = 'alice@example.com' AND active = false;
-- Condition doesn't match index filter

Query (doesn't use index):
SELECT * FROM users WHERE email = 'alice@example.com';
-- Missing active = true condition
```

**Use Cases:**
```sql
-- Pending orders only (completed orders archived, not queried often)
CREATE INDEX idx_pending_orders ON orders(user_id) WHERE status = 'pending';

-- Recent data only (old data queried infrequently)
CREATE INDEX idx_recent_logs ON logs(timestamp) WHERE timestamp > NOW() - INTERVAL '7 days';

-- Non-null values only (many null values)
CREATE INDEX idx_users_phone ON users(phone) WHERE phone IS NOT NULL;

Benefits:
+ Smaller index (faster, less storage)
+ Faster writes (fewer rows to index)
+ More efficient queries (when filter matches)
```

### 2.5 Expression Index (Functional Index)

```sql
-- Index on computed value
CREATE INDEX idx_users_lower_email ON users(LOWER(email));

Query (uses index):
SELECT * FROM users WHERE LOWER(email) = 'alice@example.com';

Query (doesn't use index):
SELECT * FROM users WHERE email = 'alice@example.com';  -- No LOWER()

Other examples:
-- Date extraction
CREATE INDEX idx_orders_month ON orders(EXTRACT(MONTH FROM created_at));

SELECT * FROM orders WHERE EXTRACT(MONTH FROM created_at) = 12;

-- JSON field
CREATE INDEX idx_users_country ON users((metadata->>'country'));

SELECT * FROM users WHERE metadata->>'country' = 'USA';

-- Computed column
CREATE INDEX idx_products_discount_price ON products((price * 0.9));

SELECT * FROM products WHERE price * 0.9 < 100;
```

### 2.6 Index Overhead

```
Trade-offs:

Storage:
â”œâ”€ Each index: 10-100% of table size
â”œâ”€ 5 indexes on 100GB table: +50-500GB storage
â””â”€ Cost consideration in cloud ($$$)

Write performance:
â”œâ”€ INSERT: Update all indexes
â”œâ”€ UPDATE: Update affected indexes
â”œâ”€ DELETE: Update all indexes
â””â”€ More indexes = slower writes

Example: 5 indexes
INSERT 1 row:
â”œâ”€ Write to table (1 write)
â”œâ”€ Update index 1 (1 write)
â”œâ”€ Update index 2 (1 write)
â”œâ”€ Update index 3 (1 write)
â”œâ”€ Update index 4 (1 write)
â””â”€ Update index 5 (1 write)
Total: 6 writes (6Ã— slower than no indexes)

Maintenance:
â”œâ”€ Index fragmentation (need rebuilding)
â”œâ”€ Statistics updates (query planner accuracy)
â””â”€ Monitoring (unused indexes)

Best practices:
âœ“ Index frequently queried columns (WHERE, JOIN, ORDER BY)
âœ“ Remove unused indexes (check pg_stat_user_indexes)
âœ“ Monitor query performance (slow query log)
âœ— Don't index everything (diminishing returns)
âœ— Don't index low-cardinality columns (gender, boolean)
âœ— Don't index write-heavy tables excessively
```

---

## 3. Partitioning Problems

### 3.1 Hot Partitions (Skewed Access)

**Problem: Uneven load distribution across partitions**

```
Scenario: Social media (followers count)

Partition by user_id (hash):
Partition 1: Users 0-1M     â†’ 1K requests/sec
Partition 2: Users 1M-2M    â†’ 1K requests/sec
Partition 3: Users 2M-3M    â†’ 100K requests/sec  â† HOT! (celebrity users)
Partition 4: Users 3M-4M    â†’ 1K requests/sec

Partition 3 bottleneck: Celebrity users (millions of followers)
â”œâ”€ All follower reads hit same partition
â”œâ”€ Partition 3 saturated (CPU, I/O, network)
â””â”€ Other partitions idle (wasted capacity)

Result:
âŒ Poor performance (slow queries on hot partition)
âŒ Unbalanced resource usage
âŒ Limited by single partition capacity
```

#### Hot Partition Causes

**1. Skewed Data Distribution:**
```
E-commerce: Products table partitioned by product_id

Product 12345 (iPhone): 1M orders
Product 67890 (obscure item): 10 orders

All iPhone orders in same partition â†’ Hot partition
```

**2. Temporal Skew (Time-based Partitioning):**
```
Logs table partitioned by date:
â”œâ”€ Partition: 2024-01-15 (today)  â†’ 1M writes/sec â† HOT!
â”œâ”€ Partition: 2024-01-14 (yesterday) â†’ 0 writes/sec
â””â”€ Partition: 2024-01-13 (2 days ago) â†’ 0 writes/sec

All writes go to "today" partition (hot partition)
Old partitions idle (wasted capacity)
```

**3. Monotonic Keys (Auto-increment, Timestamp):**
```
Users table partitioned by user_id (auto-increment):

Partition ranges:
â”œâ”€ Partition 1: user_id 0-1M     â†’ Created 2020-2021 (old users)
â”œâ”€ Partition 2: user_id 1M-2M    â†’ Created 2021-2022
â”œâ”€ Partition 3: user_id 2M-3M    â†’ Created 2022-2023
â””â”€ Partition 4: user_id 3M-4M    â†’ Created 2023-2024 â† HOT! (new users)

All INSERTs go to Partition 4 (monotonically increasing)
Partition 1-3 receive no writes (only reads)
```

#### Solutions to Hot Partitions

**1. Hash Partitioning (Even Distribution):**
```sql
-- Instead of range partitioning by user_id
-- Use hash partitioning

Partition = HASH(user_id) % 4

user_id 12345 â†’ HASH â†’ 2 â†’ Partition 2
user_id 67890 â†’ HASH â†’ 0 â†’ Partition 0
user_id 99999 â†’ HASH â†’ 3 â†’ Partition 3

Evenly distributed across partitions
No single partition is hot (assuming uniform access)

DynamoDB:
Partition key: user_id (hash distributed automatically)
```

**2. Composite Partition Key (Add Randomness):**
```
Add random suffix to spread load:

Original: partition_key = user_id (celebrity user â†’ hot partition)
Modified: partition_key = user_id + random(0-9)

Celebrity user_id=12345:
â”œâ”€ 12345-0 â†’ Partition A
â”œâ”€ 12345-1 â†’ Partition B
â”œâ”€ 12345-2 â†’ Partition C
â”œâ”€ ...
â””â”€ 12345-9 â†’ Partition J

Reads: Query all 10 partitions (12345-0 to 12345-9), merge results
Writes: Pick random suffix (12345-{random})

Trade-off:
+ Load distributed (no hot partition)
- Read amplification (query multiple partitions)
```

**3. Dedicated Partition (Isolate Hot Data):**
```
Separate celebrity users to dedicated cluster:

Regular users: Main database (partitioned by user_id)
Celebrity users (user_id in special list): Dedicated high-capacity nodes

if user_id in CELEBRITY_LIST:
    route_to(celebrity_cluster)
else:
    route_to(main_cluster)

Celebrity cluster:
â”œâ”€ More powerful hardware
â”œâ”€ Higher replication factor
â””â”€ Dedicated caching layer

Twitter approach: Treat celebrity tweets differently
```

**4. Caching (Offload Reads):**
```
Cache hot data (celebrity profiles, popular products):

Cache layer (Redis):
â”œâ”€ Celebrity user profiles (99% hit rate)
â”œâ”€ Popular products
â””â”€ Trending posts

Database:
â”œâ”€ Less load (cache absorbs traffic)
â””â”€ Uniform distribution

Example: Product catalog
Hot product (iPhone): Cached â†’ 100K reads/sec from cache
Cold product: Not cached â†’ 10 reads/sec from DB
```

**5. Read Replicas (Scale Reads):**
```
Hot partition replicated to multiple read replicas:

Partition 3 (hot):
â”œâ”€ Primary (writes)
â””â”€ Replicas: [R1, R2, R3, R4, R5] (reads)

Load balancer:
Writes â†’ Primary
Reads â†’ Round-robin across replicas

Scales read capacity (not write capacity)
```

**6. Rate Limiting (Prevent Abuse):**
```
Limit requests per user/partition:

Per-user rate limit: 100 requests/sec
Celebrity user (millions of followers):
â”œâ”€ Requests: 10,000/sec
â”œâ”€ Allowed: 100/sec
â””â”€ Throttled: 9,900/sec (429 Too Many Requests)

Protects partition from overload
Requires fair queuing/throttling
```

### 3.2 Write Amplification

**Problem: Single logical write causes multiple physical writes**

```
Scenario: LSM Tree (Cassandra, RocksDB, HBase)

Logical write (user perspective): 1 write
Physical writes (actual disk I/O):
1. Write to CommitLog (WAL): 1 write
2. Write to MemTable: 1 write (in-memory, counts later)
3. Flush MemTable â†’ SSTable: 1 write
4. Compaction: SSTable merged â†’ Rewrite data: 5-10 writes

Write amplification factor: 10-30Ã—
(10-30 physical writes per logical write)
```

#### LSM Tree Write Path (Detailed)

```
1. Write arrives:
   INSERT user_id=123, name='Alice'

2. Append to CommitLog (WAL):
   CommitLog: [... , INSERT(123, Alice)]
   Sequential write to disk (fast, ~1ms)

3. Write to MemTable (in-memory):
   MemTable (sorted tree):
   â”œâ”€ 100: 'Bob'
   â”œâ”€ 123: 'Alice'  â† New entry
   â””â”€ 200: 'Charlie'
   
   In-memory, very fast

4. MemTable full (e.g., 64MB):
   Flush to SSTable (disk):
   SSTable-1: [100â†’Bob, 123â†’Alice, 200â†’Charlie, ...]
   
   Write to disk (~100ms)

5. Multiple SSTables accumulate:
   SSTable-1: [100â†’Bob, 123â†’Alice, ...]
   SSTable-2: [105â†’Dave, 123â†’Alice2, ...]  â† Updated value
   SSTable-3: [110â†’Eve, 150â†’Frank, ...]

6. Compaction (background):
   Merge SSTable-1, SSTable-2, SSTable-3:
   â”œâ”€ Read: SSTable-1 + SSTable-2 + SSTable-3
   â”œâ”€ Merge: Combine, deduplicate, sort
   â””â”€ Write: SSTable-4 (compacted)
   
   SSTable-4: [100â†’Bob, 105â†’Dave, 110â†’Eve, 123â†’Alice2, ...]
   
   Rewrites all data (write amplification!)

Total writes:
1 logical write â†’ 1 CommitLog + 1 SSTable + N compactions
= 1 + 1 + 5-10 = 7-12 physical writes
```

#### Write Amplification in Different Systems

**LSM Tree (Cassandra, RocksDB):**
```
Write amplification: 10-30Ã—

Factors:
â”œâ”€ Number of SSTable levels (more levels = more compaction)
â”œâ”€ Compaction strategy (size-tiered, leveled)
â”œâ”€ Data volume (larger data = more compaction)
â””â”€ Update frequency (more updates = more merging)

Calculation:
WA = (Total bytes written to disk) / (Bytes written by app)

Example:
App writes: 100GB
Disk writes: 1TB (compaction overhead)
WA = 1TB / 100GB = 10Ã—
```

**B-Tree (PostgreSQL, MySQL):**
```
Write amplification: 1-2Ã—

In-place updates (no compaction):
â”œâ”€ Update row â†’ Write to page
â”œâ”€ Page split (if full) â†’ Write 2 pages
â””â”€ WAL entry â†’ 1 write

WA = 1-2Ã— (much lower than LSM)

Trade-off:
+ Lower write amplification
- Slower writes (random I/O, locking)
- Lower write throughput than LSM
```

**DynamoDB (LSM-based):**
```
Write amplification: ~10Ã—

Managed service (hidden from user):
â”œâ”€ User sees: 1 WCU (write capacity unit) per write
â”œâ”€ Actual disk writes: ~10Ã— due to replication + compaction
â””â”€ AWS handles: Compaction, storage management

Cost consideration:
1,000 writes/sec Ã— 86,400 sec/day = 86.4M writes/day
Actual disk I/O: ~864M writes/day (write amplification)
```

#### Reducing Write Amplification

**1. Leveled Compaction (RocksDB, Cassandra):**
```
Size-Tiered Compaction (default):
â”œâ”€ Merge SSTables of similar size
â”œâ”€ Creates large SSTables quickly
â””â”€ High write amplification (20-30Ã—)

Leveled Compaction:
â”œâ”€ Fixed-size SSTables per level
â”œâ”€ Level 0: 10MB SSTables
â”œâ”€ Level 1: 100MB SSTables (10Ã— Level 0)
â”œâ”€ Level 2: 1GB SSTables (10Ã— Level 1)
â””â”€ Only compact overlapping ranges

Write amplification: 10Ã— (better than size-tiered)
Read performance: Better (less overlap)

Configure in Cassandra:
CREATE TABLE users (...)
WITH compaction = {
  'class': 'LeveledCompactionStrategy',
  'sstable_size_in_mb': 160
};
```

**2. Batch Writes (Accumulate Before Flushing):**
```
Flush MemTable less frequently:
â”œâ”€ Larger MemTable (128MB vs 64MB)
â”œâ”€ Fewer SSTables created
â””â”€ Less compaction needed

PostgreSQL:
shared_buffers = 4GB  (default: 128MB)
More buffering â†’ Fewer writes â†’ Lower WA

Trade-off:
+ Lower write amplification
- More memory usage
- Longer recovery time (larger WAL)
```

**3. Tiered Storage (Hot/Cold Separation):**
```
Hot data (recent, frequently updated):
â”œâ”€ Fast SSD
â”œâ”€ Frequent compaction
â””â”€ Accept higher WA for performance

Cold data (old, rarely updated):
â”œâ”€ Slow HDD or object storage (S3)
â”œâ”€ Infrequent compaction
â””â”€ Lower WA (less churn)

Cassandra TTL:
INSERT INTO logs (...) USING TTL 86400;  -- 1 day
After TTL: Data deleted, no compaction needed
```

**4. Compression (Reduce Data Size):**
```
Compress SSTables:
â”œâ”€ Less data to write during compaction
â”œâ”€ Lower WA (fewer bytes written)
â””â”€ Trade-off: CPU for compression/decompression

Cassandra:
CREATE TABLE users (...)
WITH compression = {
  'class': 'LZ4Compressor',
  'chunk_length_in_kb': 64
};

Compression ratio: 3-10Ã— (depending on data)
WA reduction: ~30-50% (less data to rewrite)
```

### 3.3 Compaction Strategies

**Problem: Merge multiple SSTables, remove deleted/updated data**

#### Size-Tiered Compaction Strategy (STCS)

```
Merge SSTables of similar size:

Initial state:
SSTable-1 (10MB), SSTable-2 (10MB), SSTable-3 (10MB), SSTable-4 (10MB)

Trigger: 4 SSTables of similar size
Action: Merge into SSTable-5 (40MB)

Result:
SSTable-5 (40MB), SSTable-6 (10MB), SSTable-7 (10MB), SSTable-8 (10MB)

Next compaction:
Merge SSTable-6, 7, 8, 9 â†’ SSTable-10 (40MB)

Result:
SSTable-5 (40MB), SSTable-10 (40MB)

Next compaction:
Merge SSTable-5, SSTable-10 â†’ SSTable-11 (80MB)

Exponential growth: 10MB â†’ 40MB â†’ 80MB â†’ 160MB â†’ ...
```

**STCS Characteristics:**
```
Pros:
+ Fast writes (minimal compaction during writes)
+ Simple to understand and implement
+ Good for write-heavy workloads

Cons:
- High space amplification (2Ã— space needed during compaction)
- Slow reads (many SSTables to check)
- High write amplification (20-30Ã—)

Use case: Time-series data (write-once, never update)
```

#### Leveled Compaction Strategy (LCS)

```
Fixed-size SSTables organized in levels:

Level 0: 10MB SSTables (any key range)
â”œâ”€ SSTable-0-1 [keys: 1-1000]
â”œâ”€ SSTable-0-2 [keys: 500-1500]
â””â”€ SSTable-0-3 [keys: 800-1800]

Level 1: 10MB SSTables (non-overlapping key ranges)
â”œâ”€ SSTable-1-1 [keys: 1-500]
â”œâ”€ SSTable-1-2 [keys: 501-1000]
â”œâ”€ SSTable-1-3 [keys: 1001-1500]
â””â”€ SSTable-1-4 [keys: 1501-2000]

Level 2: 100MB SSTables (non-overlapping, 10Ã— Level 1)

Compaction:
1. Level 0 fills up â†’ Compact to Level 1
2. Merge overlapping SSTables in Level 1
3. If Level 1 > threshold â†’ Compact to Level 2
4. Repeat for higher levels
```

**LCS Characteristics:**
```
Pros:
+ Fast reads (few SSTables per level, no overlap)
+ Low space amplification (10% overhead)
+ Predictable performance

Cons:
- Slower writes (more compaction)
- Higher write amplification (10-15Ã—)

Use case: Read-heavy workloads, random updates
```

#### Time-Window Compaction Strategy (TWCS)

```
Partition SSTables by time window:

Window: 1 day
â”œâ”€ SSTable-2024-01-01 [data from Jan 1]
â”œâ”€ SSTable-2024-01-02 [data from Jan 2]
â””â”€ SSTable-2024-01-03 [data from Jan 3]

Compaction within window:
â”œâ”€ Compact SSTables within same day
â””â”€ Never compact across days (windows immutable)

TTL (Time To Live):
â”œâ”€ After 7 days: Delete entire SSTable
â””â”€ No tombstone compaction needed (efficient deletion)
```

**TWCS Characteristics:**
```
Pros:
+ Efficient for time-series (no cross-window compaction)
+ Fast TTL deletion (drop entire SSTable)
+ Low write amplification (5-10Ã—)
+ Predictable I/O (only current window compacted)

Cons:
- Requires time-based data (timestamp-driven)
- Not suitable for random updates

Use case: Time-series (logs, metrics, IoT data with TTL)

Cassandra configuration:
CREATE TABLE metrics (
    sensor_id UUID,
    timestamp TIMESTAMP,
    value DOUBLE,
    PRIMARY KEY (sensor_id, timestamp)
) WITH compaction = {
    'class': 'TimeWindowCompactionStrategy',
    'compaction_window_unit': 'DAYS',
    'compaction_window_size': 1
} AND default_time_to_live = 604800;  -- 7 days
```

#### Compaction Impact

**CPU & I/O:**
```
Compaction process:
1. Read multiple SSTables (disk I/O)
2. Merge, sort, deduplicate (CPU)
3. Write new SSTable (disk I/O)
4. Delete old SSTables

I/O amplification:
Read: 5 SSTables Ã— 100MB = 500MB
Write: 1 SSTable Ã— 100MB = 100MB
Total I/O: 600MB (for 100MB of actual data)

CPU usage:
â”œâ”€ Sorting: O(N log N)
â”œâ”€ Deduplication: O(N)
â””â”€ Compression: CPU-intensive

Background compaction:
â”œâ”€ Runs continuously (background threads)
â”œâ”€ Competes with foreground requests (read/write)
â””â”€ Must throttle to avoid overloading system
```

**Throttling Compaction:**
```
RocksDB:
options.level0_slowdown_writes_trigger = 20;  // Slow writes if L0 > 20 files
options.level0_stop_writes_trigger = 36;      // Stop writes if L0 > 36 files

Cassandra:
compaction_throughput_mb_per_sec = 16;  // Limit compaction I/O

Prevents:
â”œâ”€ Compaction overwhelming system
â”œâ”€ Foreground queries starved
â””â”€ Uncontrolled disk usage
```

---

## Best Practices Summary

```
Data Modeling:
âœ“ Normalize for ACID, consistency, complex queries (OLTP)
âœ“ Denormalize for performance, scale, simple queries (OLAP, NoSQL)
âœ“ Use hybrid approach (normalized primary + denormalized replicas)
âœ“ Design schema for query patterns (NoSQL)
âœ“ Use materialized views for expensive aggregations

Indexing:
âœ“ Index columns in WHERE, JOIN, ORDER BY
âœ“ Use composite indexes for multi-column queries (order by selectivity)
âœ“ Use covering indexes for high-traffic queries
âœ“ Use partial indexes for subset queries
âœ“ Monitor index usage (remove unused indexes)
âœ— Don't over-index (write performance, storage cost)
âœ— Don't index low-cardinality columns (gender, boolean)

Partitioning:
âœ“ Use hash partitioning for even distribution
âœ“ Identify hot partitions (monitoring, metrics)
âœ“ Cache hot data (offload database)
âœ“ Use read replicas for read-heavy partitions
âœ“ Consider composite keys to spread load
âœ— Avoid monotonic keys (timestamp, auto-increment)
âœ— Don't ignore temporal skew (time-based partitioning)

Write Amplification:
âœ“ Choose compaction strategy based on workload
  - STCS: Write-heavy, time-series
  - LCS: Read-heavy, random updates
  - TWCS: Time-series with TTL
âœ“ Throttle compaction (prevent overload)
âœ“ Use compression (reduce data size)
âœ“ Monitor write amplification (track I/O metrics)
âœ— Don't ignore compaction backlog (query performance impact)
âœ— Don't disable compaction (disk space explosion)
```

Complete data modeling and indexing foundation! ğŸ“ŠğŸ”