# Load Balancing & API Gateway

## What is a Load Balancer?

**Distributes incoming traffic across multiple servers**

```
Without Load Balancer:
Client â†’ Server (single point of failure, limited capacity)

With Load Balancer:
                    â”Œâ”€> Server 1
Client â†’ Load Balancer â”€â”¼â”€> Server 2
                    â”œâ”€> Server 3
                    â””â”€> Server 4

Benefits:
+ High availability (if one server fails, others continue)
+ Horizontal scaling (add more servers)
+ Better performance (distribute load)
+ Maintenance without downtime
```

---

## 1. Layer 4 (L4) vs Layer 7 (L7) Load Balancing

### OSI Layers Recap

```
Layer 7: Application  (HTTP, HTTPS, WebSocket, gRPC)
Layer 6: Presentation (SSL/TLS)
Layer 5: Session      
Layer 4: Transport    (TCP, UDP)
Layer 3: Network      (IP)
Layer 2: Data Link    
Layer 1: Physical     
```

### Layer 4 Load Balancing (Transport Layer)

**Operates at TCP/UDP level**

**What L4 LB sees:**
```
Source IP:      192.168.1.100
Source Port:    54321
Dest IP:        10.0.1.5 (LB)
Dest Port:      443
Protocol:       TCP

L4 LB does NOT see:
- HTTP headers
- Request path (/api/users vs /api/posts)
- Cookies
- Host header
```

**How it works:**
```
1. Client establishes TCP connection with LB
2. LB selects backend server (based on algorithm)
3. LB forwards TCP packets to backend
4. Backend responds through LB
5. LB forwards response to client

         TCP Connection 1              TCP Connection 2
Client â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ L4 LB â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Server

LB acts as TCP proxy (connection-level routing)
```

**L4 Load Balancing Algorithms:**

#### 1. Round Robin
```
Request 1 â†’ Server 1
Request 2 â†’ Server 2
Request 3 â†’ Server 3
Request 4 â†’ Server 1 (loop back)
Request 5 â†’ Server 2
...

Pros: Simple, equal distribution
Cons: Doesn't account for server capacity or load
```

#### 2. Least Connections
```
Servers:
â”œâ”€ Server 1: 10 active connections
â”œâ”€ Server 2: 5 active connections  â† Choose this
â””â”€ Server 3: 8 active connections

New request â†’ Server 2 (fewest connections)

Pros: Better for long-lived connections
Cons: Requires tracking connection count
```

#### 3. IP Hash
```
Hash(client_ip) % num_servers = server_index

Client 192.168.1.100 â†’ Hash â†’ Always Server 2
Client 192.168.1.101 â†’ Hash â†’ Always Server 1
Client 192.168.1.102 â†’ Hash â†’ Always Server 3

Pros: Client always goes to same server (session affinity)
Cons: Uneven distribution if IPs not uniformly distributed
```

#### 4. Weighted Round Robin
```
Servers:
â”œâ”€ Server 1 (weight 5): 16 CPU, 32 GB RAM â†’ Gets 5 requests
â”œâ”€ Server 2 (weight 3): 8 CPU, 16 GB RAM  â†’ Gets 3 requests
â””â”€ Server 3 (weight 2): 4 CPU, 8 GB RAM   â†’ Gets 2 requests

Pattern: S1, S1, S1, S1, S1, S2, S2, S2, S3, S3, repeat

Pros: Account for different server capacities
Cons: Requires manual weight configuration
```

**L4 LB Example (HAProxy):**
```
frontend tcp_frontend
    bind *:443
    mode tcp                    # L4 mode (TCP)
    default_backend app_servers

backend app_servers
    mode tcp
    balance roundrobin          # Algorithm
    server app1 10.0.1.10:443 check
    server app2 10.0.1.11:443 check
    server app3 10.0.1.12:443 check
```

**L4 LB Characteristics:**

| Feature | L4 Load Balancer |
|---------|------------------|
| **Speed** | Very fast (just forwards packets) |
| **Latency** | ~1ms |
| **Throughput** | 1M+ req/sec |
| **SSL/TLS** | Passthrough (backend terminates) |
| **Content-based routing** | No |
| **Protocol support** | Any TCP/UDP |
| **Complexity** | Low |
| **Use case** | High throughput, protocol-agnostic |

### Layer 7 Load Balancing (Application Layer)

**Operates at HTTP level**

**What L7 LB sees:**
```
GET /api/users HTTP/1.1
Host: api.example.com
User-Agent: Mozilla/5.0...
Cookie: session=abc123
Authorization: Bearer xyz789
Content-Type: application/json

L7 LB CAN see and use:
- HTTP method (GET, POST)
- Path (/api/users vs /static/image.jpg)
- Headers (Host, Cookie, User-Agent)
- Query parameters (?user_id=123)
- Request body (for routing decisions)
```

**How it works:**
```
1. Client establishes TCP connection with LB
2. Client sends HTTP request
3. LB terminates TLS, reads HTTP request
4. LB makes routing decision based on HTTP content
5. LB establishes new connection to backend
6. Backend processes and responds
7. LB forwards response to client

         TCP + HTTP                    TCP + HTTP
Client â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ L7 LB â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Server

LB acts as HTTP proxy (request-level routing)
```

**L7 Load Balancing Algorithms:**

#### 1. Content-Based Routing (Path-based)
```
/api/*      â†’ API Servers (10.0.1.x)
/static/*   â†’ Static Servers (10.0.2.x)
/admin/*    â†’ Admin Servers (10.0.3.x)
/v2/*       â†’ New API Servers (10.0.4.x)

Example:
GET /api/users     â†’ Route to 10.0.1.10
GET /static/logo.png â†’ Route to 10.0.2.10
GET /admin/dashboard â†’ Route to 10.0.3.10
```

#### 2. Host-Based Routing
```
api.example.com     â†’ API Servers
www.example.com     â†’ Web Servers
cdn.example.com     â†’ CDN Servers
admin.example.com   â†’ Admin Servers

Example:
Host: api.example.com â†’ Route to API cluster
Host: www.example.com â†’ Route to Web cluster
```

#### 3. Header-Based Routing
```
User-Agent: Mobile  â†’ Mobile-optimized servers
User-Agent: Desktop â†’ Desktop servers

X-API-Version: v2   â†’ New API servers
X-API-Version: v1   â†’ Old API servers

Cookie: premium=true â†’ Premium servers (more resources)
```

#### 4. Weighted Round Robin (with health)
```
Server 1 (healthy, weight 10): 67% traffic
Server 2 (healthy, weight 5):  33% traffic
Server 3 (unhealthy):          0% traffic (removed)
```

**L7 LB Example (NGINX):**
```nginx
upstream api_servers {
    server 10.0.1.10:8080 weight=5;
    server 10.0.1.11:8080 weight=3;
    server 10.0.1.12:8080 weight=2;
}

upstream static_servers {
    server 10.0.2.10:80;
    server 10.0.2.11:80;
}

server {
    listen 443 ssl;
    server_name example.com;

    # L7 routing based on path
    location /api/ {
        proxy_pass http://api_servers;
    }

    location /static/ {
        proxy_pass http://static_servers;
    }

    location /admin/ {
        # Only allow from specific IP
        allow 203.0.113.0/24;
        deny all;
        proxy_pass http://api_servers;
    }
}
```

**L7 LB Characteristics:**

| Feature | L7 Load Balancer |
|---------|------------------|
| **Speed** | Slower (parses HTTP) |
| **Latency** | ~5-10ms |
| **Throughput** | 10K-100K req/sec |
| **SSL/TLS** | Terminates (decrypts) |
| **Content-based routing** | Yes (path, header, cookie) |
| **Protocol support** | HTTP/HTTPS only |
| **Complexity** | Higher |
| **Use case** | Smart routing, HTTP-specific |

### L4 vs L7 Comparison

| Feature | L4 (Transport) | L7 (Application) |
|---------|----------------|------------------|
| **Layer** | TCP/UDP | HTTP/HTTPS |
| **Visibility** | IP, Port | URL, Headers, Cookies |
| **Routing** | IP/Port based | Content based |
| **Performance** | Faster (1M+ req/sec) | Slower (10K-100K req/sec) |
| **Latency** | <1ms | 5-10ms |
| **SSL Termination** | No (passthrough) | Yes |
| **Complexity** | Simple | Complex |
| **Examples** | HAProxy (TCP mode), NLB | NGINX, HAProxy (HTTP mode), ALB |
| **Use Case** | High throughput, any protocol | Smart routing, HTTP features |

### When to Use L4 vs L7

**Use L4 when:**
```
âœ“ Need maximum throughput (millions of req/sec)
âœ“ Protocol-agnostic (not just HTTP)
âœ“ Simple routing (round robin, least connections)
âœ“ Backend handles TLS termination
âœ“ Cost-sensitive (cheaper, less CPU)

Examples:
- Gaming servers (UDP)
- Database load balancing
- VoIP systems
- Low-level TCP services
```

**Use L7 when:**
```
âœ“ Need content-based routing (/api vs /static)
âœ“ Want TLS termination at LB (simplify backends)
âœ“ Need request modification (add headers, rewrite URLs)
âœ“ Want advanced features (rate limiting, WAF)
âœ“ HTTP/HTTPS only

Examples:
- Web applications
- REST APIs
- Microservices (route by path)
- Multi-tenant apps (route by subdomain)
```

**Hybrid: Use both!**
```
Internet
    â†“
L4 LB (HAProxy, NLB) â† High throughput, DDoS protection
    â†“
L7 LB (NGINX, ALB) â† Smart routing, TLS termination
    â†“
Backend Servers

L4: Handles massive traffic, distributes to L7 LBs
L7: Routes intelligently to appropriate backends
```

---

## 2. Health Checks

**Purpose:** Detect unhealthy servers, stop sending traffic to them

### Types of Health Checks

#### 1. TCP Health Check (L4)
```
Check: Can I establish TCP connection?

Process:
1. LB: Try to connect to server:port
2. Server: Accept or refuse connection
3. LB: Mark server healthy/unhealthy

Example:
Health check: TCP port 8080
â”œâ”€ Success: Server is healthy âœ“
â””â”€ Fail: Server is unhealthy âœ—

Pros: Fast, simple
Cons: Doesn't verify application is working
```

#### 2. HTTP Health Check (L7)
```
Check: Does server return HTTP 200?

Process:
1. LB: GET /health
2. Server: 200 OK (healthy) or 500 Error (unhealthy)
3. LB: Mark server based on response

Example:
GET /health HTTP/1.1
Host: server1.example.com

Response: 200 OK â† Healthy
Body: {"status": "ok", "version": "1.2.3"}

Pros: Verifies application is running
Cons: Slower than TCP check
```

#### 3. Custom Health Check
```
Check: Application-specific logic

Example:
GET /health HTTP/1.1

Server checks:
â”œâ”€ Database connection: OK âœ“
â”œâ”€ Redis connection: OK âœ“
â”œâ”€ Disk space: 20% free âœ“
â”œâ”€ Memory: 60% used âœ“
â””â”€ Recent errors: 0 âœ“

Response:
200 OK if all pass
503 Service Unavailable if any fail

Pros: Comprehensive health check
Cons: More complex, slower
```

### Health Check Configuration

**Parameters:**

```
1. Interval: How often to check
   Example: Check every 10 seconds

2. Timeout: How long to wait for response
   Example: 5 second timeout

3. Healthy threshold: Consecutive successes to mark healthy
   Example: 2 successes â†’ Healthy

4. Unhealthy threshold: Consecutive failures to mark unhealthy
   Example: 3 failures â†’ Unhealthy

5. Path: Endpoint to check
   Example: /health, /ping, /status

6. Expected response: What indicates health
   Example: HTTP 200, body contains "OK"
```

**NGINX Health Check:**
```nginx
upstream backend {
    server 10.0.1.10:8080;
    server 10.0.1.11:8080;
    server 10.0.1.12:8080;
}

server {
    location / {
        proxy_pass http://backend;
        
        # Health check (NGINX Plus feature)
        health_check interval=10s
                     fails=3
                     passes=2
                     uri=/health
                     match=health_check;
    }
}

match health_check {
    status 200;
    header Content-Type = application/json;
    body ~ "ok";
}
```

**HAProxy Health Check:**
```
backend app_servers
    option httpchk GET /health HTTP/1.1\r\nHost:\ example.com
    
    server app1 10.0.1.10:8080 check inter 10s fall 3 rise 2
    server app2 10.0.1.11:8080 check inter 10s fall 3 rise 2
    server app3 10.0.1.12:8080 check inter 10s fall 3 rise 2
    
# inter 10s: Check every 10 seconds
# fall 3:    Mark unhealthy after 3 failures
# rise 2:    Mark healthy after 2 successes
```

**AWS ALB Health Check:**
```json
{
  "HealthCheckProtocol": "HTTP",
  "HealthCheckPort": "8080",
  "HealthCheckPath": "/health",
  "HealthCheckIntervalSeconds": 30,
  "HealthCheckTimeoutSeconds": 5,
  "HealthyThresholdCount": 2,
  "UnhealthyThresholdCount": 3,
  "Matcher": {
    "HttpCode": "200"
  }
}
```

### Health Check Scenarios

#### Scenario 1: Gradual Failure
```
Server performance degrades:

Time 0s:  Health check: 200 OK (50ms) âœ“
Time 10s: Health check: 200 OK (100ms) âœ“
Time 20s: Health check: 200 OK (500ms) âœ“
Time 30s: Health check: Timeout (>5s) âœ— (failure 1)
Time 40s: Health check: Timeout (>5s) âœ— (failure 2)
Time 50s: Health check: Timeout (>5s) âœ— (failure 3)
â†’ Mark UNHEALTHY, stop sending traffic

Solution: Add timeout threshold to detect slow servers
```

#### Scenario 2: Dependency Failure
```
Application is running, but database is down:

Basic check:
GET /health â†’ 200 OK âœ“ (but app can't serve real requests!)

Better check:
GET /health â†’ Server checks database connection
           â†’ Database down â†’ Return 503 âœ—
           â†’ LB marks unhealthy

Comprehensive health endpoint:
{
  "status": "unhealthy",
  "database": "down",
  "redis": "up",
  "disk_space": "ok"
}
```

#### Scenario 3: Thundering Herd
```
Problem:
All servers fail health check simultaneously
â†’ LB has no healthy servers
â†’ All traffic fails!

Solutions:
1. Staggered health checks:
   Server 1: Check at 0s, 10s, 20s...
   Server 2: Check at 2s, 12s, 22s...
   Server 3: Check at 4s, 14s, 24s...

2. Gradual traffic reduction:
   Unhealthy â†’ Reduce traffic by 50%
   Still failing â†’ Reduce by 75%
   Still failing â†’ Remove completely

3. Last resort server:
   If all unhealthy, send traffic anyway
   Better to try than return 503 to all users
```

### Health Check Best Practices

```
âœ“ Use dedicated /health endpoint (not /)
âœ“ Keep health check lightweight (< 100ms)
âœ“ Check critical dependencies (DB, cache)
âœ“ Don't check external APIs (too slow, unreliable)
âœ“ Log health check failures (debugging)
âœ“ Set reasonable thresholds (3 failures, 2 successes)
âœ“ Use different intervals: fast for critical, slow for stable
âœ— Don't perform expensive operations in health check
âœ— Don't use same endpoint as main traffic (can be cached)
```

---

## 3. Sticky Sessions (Session Affinity)

**Problem:** User session stored on specific server

```
Without sticky sessions:

Request 1: Login â†’ Server 1 â†’ Session stored on Server 1
Request 2: Get profile â†’ Server 2 â†’ No session âœ— (user appears logged out)
Request 3: Update profile â†’ Server 3 â†’ No session âœ—

User experience: Randomly logged out!
```

**Solution: Sticky sessions route user to same server**

```
With sticky sessions:

Request 1: Login â†’ Server 1 â†’ Session stored on Server 1
Request 2: Get profile â†’ Server 1 â†’ Session found âœ“
Request 3: Update profile â†’ Server 1 â†’ Session found âœ“

User always goes to Server 1 (until server fails)
```

### Sticky Session Methods

#### 1. Cookie-Based (Most Common)
```
Process:
1. User makes first request
2. LB selects server (e.g., Server 2)
3. LB sets cookie: LB_SERVER=server2
4. User's browser stores cookie
5. Subsequent requests include cookie
6. LB reads cookie, routes to Server 2

Example:
First request:
GET / HTTP/1.1

LB response:
Set-Cookie: LB_SERVER=server2; Path=/; HttpOnly

Subsequent requests:
GET /profile HTTP/1.1
Cookie: LB_SERVER=server2
â†’ LB routes to Server 2
```

**NGINX Cookie-Based:**
```nginx
upstream backend {
    server 10.0.1.10:8080;
    server 10.0.1.11:8080;
    server 10.0.1.12:8080;
    
    sticky cookie srv_id expires=1h path=/;
}

# LB sets cookie: srv_id=10.0.1.10
# Client includes cookie in requests
# LB routes based on cookie value
```

#### 2. IP Hash (Source IP)
```
Hash(client_ip) % num_servers = server_index

Client 192.168.1.100 â†’ Always Server 2
Client 192.168.1.101 â†’ Always Server 1

Pros:
+ No cookies needed
+ Works without HTTP (L4)

Cons:
- NAT/proxy: Many users share same IP
- Uneven distribution: Some IPs more active
- Can't easily change server (consistent hashing helps)
```

**NGINX IP Hash:**
```nginx
upstream backend {
    ip_hash;
    server 10.0.1.10:8080;
    server 10.0.1.11:8080;
    server 10.0.1.12:8080;
}
```

#### 3. Application Session Cookie
```
Application sets session cookie:
Set-Cookie: JSESSIONID=abc123xyz789

LB uses this cookie for routing:
Hash(JSESSIONID) % num_servers = server

Pros:
+ Uses existing session ID
+ No extra LB cookie

Cons:
- LB must understand app's cookie format
```

**HAProxy Session Cookie:**
```
backend app_servers
    cookie JSESSIONID prefix indirect nocache
    server app1 10.0.1.10:8080 cookie app1
    server app2 10.0.1.11:8080 cookie app2
    server app3 10.0.1.12:8080 cookie app3

# HAProxy inserts "app1" into JSESSIONID
# JSESSIONID=app1~abc123xyz789
# Routes based on "app1" prefix
```

### Problems with Sticky Sessions

#### 1. Uneven Load Distribution
```
Server 1: 1000 active users â† Popular users stuck here
Server 2: 500 active users
Server 3: 200 active users

Problem: Can't rebalance without breaking sessions
```

#### 2. Server Failure
```
User â†’ Server 2 (has user's session)
Server 2 fails âœ—
User â†’ Server 1 (no session) â†’ User logged out

Session lost!
```

#### 3. Scaling Challenges
```
Add new Server 4:
â”œâ”€ New users â†’ Can use Server 4 âœ“
â””â”€ Existing users â†’ Still on old servers âœ—

Takes time to rebalance (wait for sessions to expire)
```

### Better Alternative: Centralized Session Store

**Instead of sticky sessions, use shared storage**

```
                    â”Œâ”€> Server 1 â”€â”
Client â†’ Load Balancer â”€â”¼â”€> Server 2 â”€â”¼â”€> Redis (shared session store)
                    â””â”€> Server 3 â”€â”˜

No sticky sessions needed!
All servers access same session data

Request flow:
1. User logs in â†’ Server 2
2. Server 2 stores session in Redis: session:abc123 = {user_id: 42}
3. Next request â†’ Server 1 (different server)
4. Server 1 reads session from Redis: session:abc123
5. Session found! User still logged in âœ“
```

**Redis Session Store (Express.js):**
```javascript
const session = require('express-session');
const RedisStore = require('connect-redis')(session);
const redis = require('redis');

const redisClient = redis.createClient({
    host: 'redis.example.com',
    port: 6379
});

app.use(session({
    store: new RedisStore({ client: redisClient }),
    secret: 'my-secret',
    resave: false,
    saveUninitialized: false,
    cookie: {
        maxAge: 3600000  // 1 hour
    }
}));

// Session automatically stored/retrieved from Redis
// Works across all servers!
```

**Benefits:**
```
+ Load balancer can route anywhere (better distribution)
+ Server failure doesn't lose sessions
+ Easy scaling (add/remove servers)
+ Stateless servers (easier to deploy)

Drawbacks:
- Extra network hop (to Redis)
- Single point of failure (Redis) â†’ Use Redis Cluster
- Slight latency increase (~1-2ms)
```

### When to Use Sticky Sessions

```
Use sticky sessions when:
âœ“ Legacy app (can't modify to use shared sessions)
âœ“ Sessions too large for Redis
âœ“ Very low latency required (no network hop)
âœ“ Short-lived sessions (minutes, not hours)

Avoid sticky sessions when:
âœ— Need perfect load distribution
âœ— Frequent scaling up/down
âœ— High availability critical
âœ— Can use centralized session store
```

---

## 4. Reverse Proxy

**What is a Reverse Proxy?**

```
Forward Proxy (hides client):
Client â†’ Forward Proxy â†’ Internet â†’ Server
(Server doesn't see client's real IP)

Reverse Proxy (hides server):
Client â†’ Internet â†’ Reverse Proxy â†’ Backend Server
(Client doesn't know which backend server)

Load balancers are reverse proxies!
But reverse proxies can do more than load balancing.
```

### Reverse Proxy Features

#### 1. TLS Termination
```
Client (HTTPS) â†’ Reverse Proxy (decrypts) â†’ Backend (HTTP)

Benefits:
+ Centralized certificate management (one place)
+ Offload TLS from backends (CPU savings)
+ Simpler backend code (no TLS)

Process:
1. Client: Establish TLS with reverse proxy
2. Reverse proxy: Decrypt HTTPS â†’ HTTP
3. Reverse proxy â†’ Backend: HTTP (internal network)
4. Backend â†’ Reverse proxy: HTTP response
5. Reverse proxy: Encrypt HTTP â†’ HTTPS
6. Reverse proxy â†’ Client: HTTPS response
```

**NGINX TLS Termination:**
```nginx
server {
    listen 443 ssl;
    server_name example.com;
    
    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    
    location / {
        proxy_pass http://backend_servers;  # HTTP to backend
    }
}

upstream backend_servers {
    server 10.0.1.10:8080;  # Backend listens on HTTP
    server 10.0.1.11:8080;
}
```

#### 2. Caching
```
Reverse proxy caches responses:

First request:
Client â†’ Reverse Proxy â†’ Backend (slow, 100ms)
Reverse Proxy caches response

Subsequent requests:
Client â†’ Reverse Proxy â†’ Return from cache (fast, 1ms)
No backend hit needed!

Cache rules:
GET /static/logo.png â†’ Cache 1 day
GET /api/users â†’ Cache 1 minute
POST /api/users â†’ Never cache (writes)
```

**NGINX Caching:**
```nginx
# Define cache location and size
proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=my_cache:10m max_size=1g inactive=60m;

server {
    location /api/ {
        proxy_cache my_cache;
        proxy_cache_valid 200 10m;      # Cache 200 responses for 10 min
        proxy_cache_valid 404 1m;       # Cache 404 for 1 min
        proxy_cache_key "$scheme$host$request_uri";
        add_header X-Cache-Status $upstream_cache_status;
        
        proxy_pass http://backend_servers;
    }
    
    location /static/ {
        proxy_cache my_cache;
        proxy_cache_valid 200 1d;       # Cache static files for 1 day
        proxy_pass http://backend_servers;
    }
}

# X-Cache-Status header shows: HIT, MISS, BYPASS, EXPIRED
```

#### 3. Compression
```
Reverse proxy compresses responses before sending to client:

Backend â†’ Reverse Proxy: 100 KB uncompressed JSON
Reverse Proxy: Compress with gzip
Reverse Proxy â†’ Client: 20 KB compressed (80% smaller!)

Benefits:
+ Faster transfer (less bandwidth)
+ Lower costs (egress charges)
+ Better user experience

Typical compression ratios:
â”œâ”€ Text/HTML: 70-80% reduction
â”œâ”€ JSON/XML: 70-80% reduction
â”œâ”€ CSS/JavaScript: 60-70% reduction
â”œâ”€ Images (JPEG, PNG): Already compressed, ~5% gain
â””â”€ Videos: Already compressed, 0% gain
```

**NGINX Compression:**
```nginx
server {
    # Enable gzip
    gzip on;
    gzip_comp_level 6;          # 1-9, higher = better compression, more CPU
    gzip_types text/plain text/css application/json application/javascript text/xml application/xml;
    gzip_min_length 1000;       # Don't compress small files
    
    location / {
        proxy_pass http://backend_servers;
    }
}
```

#### 4. Request Modification
```
Add/modify/remove headers:

Backend doesn't see client's real IP:
Client (203.0.113.42) â†’ Reverse Proxy â†’ Backend sees proxy IP

Add X-Forwarded-For header:
Reverse Proxy adds: X-Forwarded-For: 203.0.113.42
Backend now knows client's real IP
```

**NGINX Header Modification:**
```nginx
server {
    location / {
        # Add headers
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Remove headers
        proxy_hide_header X-Powered-By;
        
        # Add custom header
        proxy_set_header X-Request-ID $request_id;
        
        proxy_pass http://backend_servers;
    }
}
```

#### 5. URL Rewriting
```
Client requests: /old-path
Reverse proxy rewrites: /new-path
Backend sees: /new-path

Use cases:
- API versioning: /v1/users â†’ /users?version=1
- Path normalization: /path/ â†’ /path
- Legacy support: /old-api â†’ /new-api
```

**NGINX URL Rewriting:**
```nginx
server {
    # Redirect old path to new path
    location /old-api/ {
        rewrite ^/old-api/(.*)$ /new-api/$1 break;
        proxy_pass http://backend_servers;
    }
    
    # Add prefix
    location /api/ {
        rewrite ^/api/(.*)$ /v1/$1 break;
        proxy_pass http://backend_servers;
    }
}
```

#### 6. Security

**Hide backend implementation:**
```
Response headers without reverse proxy:
Server: Apache/2.4.41 (Ubuntu)
X-Powered-By: PHP/7.4.3

With reverse proxy:
Server: nginx  (hides backend)
(X-Powered-By removed)

Attacker can't see backend technology!
```

**DDoS protection:**
```
Reverse proxy limits requests:
- Max 100 connections per IP
- Max 10 requests/second per IP
- Block IPs after 100 failed attempts

Backend protected from attack!
```

**WAF (Web Application Firewall):**
```
Reverse proxy inspects requests:
GET /api/users?id=1' OR '1'='1  â† SQL injection attempt âœ—
Block request, don't forward to backend
```

### Reverse Proxy Products

```
NGINX:
+ Most popular
+ Fast, lightweight
+ Great for static content
+ Load balancing, caching, compression
+ Open source (or NGINX Plus for more features)

HAProxy:
+ Best for L4/L7 load balancing
+ Very fast
+ Advanced health checks
+ Great for high-traffic sites
+ Open source

Apache (mod_proxy):
+ Part of Apache HTTP Server
+ Easy setup if already using Apache
+ Slower than NGINX/HAProxy
+ Open source

Envoy:
+ Modern, cloud-native
+ Service mesh (Istio)
+ gRPC support
+ Dynamic configuration
+ Open source

Traefik:
+ Kubernetes-native
+ Automatic service discovery
+ Let's Encrypt integration
+ Modern UI
+ Open source

AWS ALB/NLB:
+ Managed service (no maintenance)
+ Auto-scaling
+ Integrates with AWS
+ Pay per use

Cloudflare:
+ Global CDN + reverse proxy
+ DDoS protection
+ Free tier available
```

---

## 5. API Gateway

**What is an API Gateway?**

```
API Gateway = Reverse Proxy + API-specific features

                    â”Œâ”€> Auth Service
                    â”œâ”€> User Service
Client â†’ API Gateway â”¼â”€> Order Service
                    â”œâ”€> Payment Service
                    â””â”€> Notification Service

API Gateway:
â”œâ”€ Single entry point for all APIs
â”œâ”€ Authentication & Authorization
â”œâ”€ Rate limiting & quotas
â”œâ”€ Request/response transformation
â”œâ”€ Analytics & monitoring
â”œâ”€ API versioning
â””â”€ Service discovery
```

### API Gateway Features

#### 1. Authentication & Authorization
```
Without API Gateway:
Each microservice implements auth (duplicated code)

With API Gateway:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Client â†’ API Gateway         â”‚
â”‚    Authorization: Bearer token  â”‚
â”‚                                 â”‚
â”‚ 2. API Gateway validates token  â”‚
â”‚    âœ“ Valid â†’ Extract user_id    â”‚
â”‚    âœ— Invalid â†’ 401 Unauthorized â”‚
â”‚                                 â”‚
â”‚ 3. API Gateway â†’ Microservice   â”‚
â”‚    X-User-ID: 42                â”‚
â”‚    (No auth needed in service)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
+ Centralized auth (one place to update)
+ Services don't handle auth (simpler code)
+ Consistent security policy
```

**API Gateway Auth Example:**
```javascript
// Kong API Gateway config
{
  "name": "api-service",
  "plugins": [
    {
      "name": "jwt",
      "config": {
        "claims_to_verify": ["exp"],
        "key_claim_name": "kid",
        "secret_is_base64": false
      }
    }
  ],
  "routes": [
    {
      "paths": ["/api/users"]
    }
  ],
  "service": {
    "url": "http://user-service:8080"
  }
}

// Request with valid JWT â†’ Forwarded to service
// Request without JWT â†’ 401 Unauthorized
```

#### 2. Rate Limiting
```
Purpose: Prevent abuse, ensure fair usage

Rate limiting policies:
â”œâ”€ Per user: 1000 requests/hour
â”œâ”€ Per IP: 100 requests/minute
â”œâ”€ Per API key: 10,000 requests/day
â””â”€ Global: 1M requests/second

Example:
User 123: 998 requests this hour
Request 999: âœ“ Allow
Request 1000: âœ“ Allow
Request 1001: âœ— Deny (429 Too Many Requests)

Response:
HTTP/1.1 429 Too Many Requests
Retry-After: 3600
X-RateLimit-Limit: 1000
X-RateLimit-Remaining: 0
X-RateLimit-Reset: 1640995200
```

**Rate Limiting Algorithms:**

##### Token Bucket
```
Bucket has tokens, each request consumes 1 token:

Capacity: 100 tokens
Refill rate: 10 tokens/second

Time 0s:  Bucket has 100 tokens
Request:  100 - 1 = 99 tokens âœ“
Request:  99 - 1 = 98 tokens âœ“
...
Request:  1 - 1 = 0 tokens âœ“
Request:  0 - 1 = ??? âœ— RATE LIMITED

Time 1s:  Refill 10 tokens â†’ 10 tokens
10 requests: âœ“ Allow
11th request: âœ— Rate limited

Allows burst (100 immediate requests)
Then steady rate (10 req/sec)
```

##### Leaky Bucket
```
Requests go into bucket, leak out at fixed rate:

Bucket capacity: 100 requests
Leak rate: 10 requests/second

Requests arrive:
â”œâ”€ Bucket has space â†’ Add to bucket
â””â”€ Bucket full â†’ Drop request (rate limited)

Smooths out bursts to steady rate
```

##### Fixed Window
```
Count requests in fixed time windows:

Window: 1 minute
Limit: 100 requests

00:00:00 - 00:00:59: Count = 0
Request at 00:00:10: Count = 1 âœ“
Request at 00:00:20: Count = 2 âœ“
...
Request at 00:00:59: Count = 100 âœ“
Request at 00:00:59: Count = 101 âœ— RATE LIMITED

01:00:00: Counter resets to 0
```

##### Sliding Window
```
Count requests in rolling time window:

Window: 1 minute (rolling)
Limit: 100 requests

Current time: 10:30:45
Count requests from 10:29:45 to 10:30:45

As time moves, window slides
More accurate than fixed window
```

**NGINX Rate Limiting:**
```nginx
# Define rate limit zone
limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;

server {
    location /api/ {
        # Apply rate limit
        limit_req zone=api_limit burst=20 nodelay;
        
        # Return 429 when rate limited
        limit_req_status 429;
        
        proxy_pass http://backend_servers;
    }
}

# rate=10r/s: 10 requests per second
# burst=20: Allow burst of 20 extra requests
# nodelay: Don't delay burst requests
```

**Kong Rate Limiting:**
```javascript
{
  "name": "rate-limiting",
  "config": {
    "minute": 100,       // 100 requests per minute
    "hour": 1000,        // 1000 requests per hour
    "policy": "redis",   // Store counters in Redis
    "redis_host": "redis.example.com"
  }
}
```

#### 3. Quotas
```
Rate limiting: Short-term (requests/second, requests/minute)
Quotas: Long-term (requests/month, data/month)

Example pricing tiers:
â”œâ”€ Free tier:     10,000 requests/month, 1 GB data/month
â”œâ”€ Basic tier:    100,000 requests/month, 10 GB data/month
â”œâ”€ Pro tier:      1M requests/month, 100 GB data/month
â””â”€ Enterprise:    Unlimited

User on Free tier:
Requests this month: 9,999
Next request: âœ“ Allow (9,999 < 10,000)
Request after that: âœ— Deny (10,000 â‰¥ 10,000)

Response:
HTTP/1.1 429 Too Many Requests
X-Quota-Limit: 10000
X-Quota-Remaining: 0
X-Quota-Reset: 2024-02-01T00:00:00Z

Reset at beginning of next month
```

**Quota Tracking:**
```
Storage:
quota:user:123:2024-01 = {
  requests: 9999,
  data_bytes: 950000000,  // 950 MB
  limit_requests: 10000,
  limit_bytes: 1000000000  // 1 GB
}

Check before each request:
IF requests < limit_requests AND data < limit_bytes:
    Allow request
    Increment counters
ELSE:
    Deny request (quota exceeded)
```

#### 4. Request Shaping (Transformation)

**Request Transformation:**
```
Client sends:
GET /v1/users/123

API Gateway transforms:
GET /users/123
X-API-Version: v1
X-Client-ID: mobile-app
X-Request-ID: uuid-1234

Backend receives modified request
```

**Response Transformation:**
```
Backend returns:
{
  "user_id": 123,
  "name": "Alice",
  "internal_field": "secret"  â† Remove this
}

API Gateway transforms:
{
  "id": 123,                   â† Rename user_id â†’ id
  "name": "Alice"              â† internal_field removed
}

Client receives cleaned response
```

**Use Cases:**

##### 1. Protocol Translation
```
Client: REST â†’ API Gateway: gRPC
GET /api/users/123 â†’ GetUser(id=123)

API Gateway translates between protocols
```

##### 2. Response Aggregation
```
Client needs user + orders + reviews:

Without API Gateway (3 requests):
GET /api/users/123
GET /api/orders?user_id=123
GET /api/reviews?user_id=123

With API Gateway (1 request):
GET /api/users/123?include=orders,reviews

API Gateway:
1. Calls 3 microservices in parallel
2. Aggregates responses
3. Returns combined result

{
  "user": {...},
  "orders": [...],
  "reviews": [...]
}

Faster, fewer client requests!
```

##### 3. Field Filtering
```
Client requests:
GET /api/users/123?fields=id,name

API Gateway:
1. Fetches full user from backend
2. Filters to requested fields
3. Returns: {"id": 123, "name": "Alice"}

GraphQL-like filtering with REST
```

**Kong Request Transformer:**
```javascript
{
  "name": "request-transformer",
  "config": {
    "add": {
      "headers": ["X-Request-ID:$(uuid)"],
      "querystring": ["version:v1"]
    },
    "remove": {
      "headers": ["X-Internal-Token"]
    },
    "rename": {
      "headers": ["Authorization:X-Auth"]
    }
  }
}
```

#### 5. API Versioning
```
Multiple API versions coexist:

v1 (deprecated): /v1/users
v2 (current):    /v2/users
v3 (beta):       /v3/users

API Gateway routes based on version:
GET /v1/users â†’ Old backend (maintained for compatibility)
GET /v2/users â†’ Current backend
GET /v3/users â†’ New backend (beta features)

Or header-based:
GET /users
X-API-Version: 2 â†’ Route to v2 backend
```

#### 6. Service Discovery
```
Microservices register with service registry:

Service Registry (Consul, etcd):
â”œâ”€ user-service: [10.0.1.10:8080, 10.0.1.11:8080]
â”œâ”€ order-service: [10.0.2.10:8080, 10.0.2.11:8080]
â””â”€ payment-service: [10.0.3.10:8080]

API Gateway queries registry:
GET /api/users â†’ Where is user-service?
Registry: 10.0.1.10 and 10.0.1.11
Gateway: Route to 10.0.1.10 (load balance)

Services can scale up/down dynamically
Gateway automatically discovers new instances
```

#### 7. Monitoring & Analytics
```
API Gateway logs all requests:

{
  "timestamp": "2024-01-15T10:30:00Z",
  "method": "GET",
  "path": "/api/users/123",
  "client_ip": "203.0.113.42",
  "user_id": 456,
  "status_code": 200,
  "latency_ms": 45,
  "bytes_sent": 1024
}

Analytics:
â”œâ”€ Most popular endpoints
â”œâ”€ Error rates by endpoint
â”œâ”€ Latency percentiles (P50, P95, P99)
â”œâ”€ Top users by request count
â”œâ”€ Geographic distribution
â””â”€ API usage trends

Dashboard:
/api/users: 10K req/min, P95 latency 50ms, 99.9% success
/api/orders: 5K req/min, P95 latency 100ms, 99.5% success
```

### API Gateway Products

```
Kong:
+ Open source (or Kong Enterprise)
+ Plugin-based architecture
+ High performance (built on NGINX)
+ Great community
+ Kubernetes-native (Kong Ingress Controller)

AWS API Gateway:
+ Fully managed (serverless)
+ Integrates with Lambda, ECS, EC2
+ Built-in auth (Cognito, IAM)
+ Auto-scaling
+ Pay per request

Google Cloud API Gateway:
+ Fully managed
+ OpenAPI spec based
+ Integrates with Cloud Functions, GKE
+ Cloud Armor integration (DDoS protection)

Azure API Management:
+ Fully managed
+ Developer portal
+ OAuth 2.0, OpenID Connect
+ Multi-region deployment
+ Analytics dashboard

Apigee (Google):
+ Enterprise-grade
+ Advanced analytics
+ Monetization features (billing, quotas)
+ Hybrid deployment (cloud + on-prem)

Tyk:
+ Open source
+ GraphQL support
+ Multi-tenant
+ Developer portal
+ Analytics dashboard

Express Gateway:
+ Built on Express.js
+ Lightweight
+ Plugin-based
+ Good for Node.js ecosystem
```

### API Gateway vs Load Balancer

| Feature | Load Balancer | API Gateway |
|---------|--------------|-------------|
| **Primary Purpose** | Distribute traffic | API management |
| **Layer** | L4 or L7 | L7 (HTTP) only |
| **Authentication** | No | Yes |
| **Rate Limiting** | Basic | Advanced (per user, per API key) |
| **Request Transform** | Limited | Extensive |
| **Protocol** | Any (TCP/UDP) | HTTP/HTTPS, WebSocket |
| **Service Discovery** | Static config | Dynamic (registry) |
| **Analytics** | Basic | Advanced |
| **Cost** | Lower | Higher |
| **Use Case** | Traffic distribution | API management, microservices |

**When to use both:**
```
Internet
    â†“
Load Balancer (L4/L7) â† DDoS protection, TLS termination
    â†“
API Gateway â† Auth, rate limiting, routing
    â†“
Microservices â† Business logic

Separate concerns:
- Load Balancer: Network traffic distribution
- API Gateway: API-level features
```

---

## Complete Example: E-commerce Architecture

```
                          Internet
                             â†“
                    [Cloudflare CDN]
                    (DDoS protection, static files)
                             â†“
                     [L4 Load Balancer]
                     (AWS NLB - high throughput)
                             â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â†“                             â†“
      [L7 Load Balancer]            [L7 Load Balancer]
      (NGINX - Zone A)              (NGINX - Zone B)
      (TLS termination)             (TLS termination)
              â†“                             â†“
         [API Gateway]                 [API Gateway]
         (Kong - Zone A)               (Kong - Zone B)
         (Auth, rate limiting)         (Auth, rate limiting)
              â†“                             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“         â†“         â†“         â†“         â†“         â†“
[User]   [Order]   [Payment]  [User]   [Order]   [Payment]
Service  Service   Service    Service  Service   Service
  â†“         â†“         â†“         â†“         â†“         â†“
[Redis] [PostgreSQL] [Stripe] [Redis] [PostgreSQL] [Stripe]
(Cache)  (Database)   (API)    (Cache)  (Database)   (API)

Configuration:

1. Cloudflare:
   - Cache static assets (images, CSS, JS)
   - DDoS protection (block malicious traffic)
   - Global anycast (low latency worldwide)

2. L4 Load Balancer (AWS NLB):
   - Handles millions of req/sec
   - Health check: TCP port 443
   - Distributes to L7 LBs in both zones

3. L7 Load Balancer (NGINX):
   - TLS termination (offload from backends)
   - Compression (gzip)
   - Caching (API responses)
   - Health check: GET /health every 10s
   - Sticky sessions: Cookie-based

4. API Gateway (Kong):
   - JWT authentication (validate tokens)
   - Rate limiting: 1000 req/hour per user
   - Request routing:
     * /api/users â†’ User Service
     * /api/orders â†’ Order Service
     * /api/payments â†’ Payment Service
   - Request transformation (add headers)
   - Response aggregation (combine services)
   - Logging & analytics

5. Microservices:
   - Stateless (session in Redis)
   - Health endpoint: /health
   - Metrics endpoint: /metrics (Prometheus)
   - Horizontal scaling (auto-scale based on CPU)

Traffic flow example:

Client: GET /api/orders/123 (with JWT)
    â†“
Cloudflare: Cache miss, forward
    â†“
L4 LB: TCP connection to NGINX Zone A (round robin)
    â†“
L7 LB: TLS decrypt, forward HTTP to Kong
    â†“
API Gateway:
    - Validate JWT âœ“
    - Check rate limit (100/1000 requests this hour) âœ“
    - Add headers (X-User-ID: 42)
    - Route to Order Service
    â†“
Order Service:
    - Query PostgreSQL
    - Return order data
    â†“
API Gateway: Log request, forward response
    â†“
L7 LB: Compress response (gzip), encrypt (TLS)
    â†“
L4 LB: Forward to client
    â†“
Cloudflare: Cache response (if cacheable)
    â†“
Client: Receive order data

Total latency: ~50ms
- Cloudflare: 10ms
- L4 LB: 1ms
- L7 LB: 5ms
- API Gateway: 10ms
- Order Service: 20ms (DB query)
- Response path: 4ms
```

---

## Best Practices Summary

### Load Balancer
```
âœ“ Use L4 for maximum throughput, L7 for smart routing
âœ“ Implement comprehensive health checks (not just TCP)
âœ“ Use multiple availability zones (high availability)
âœ“ Configure connection timeouts appropriately
âœ“ Enable keep-alive for HTTP/HTTPS
âœ“ Use connection pooling to backends
âœ“ Monitor LB metrics (latency, error rate, throughput)
âœ“ Plan for 3-5Ã— normal traffic capacity
âœ— Don't rely on sticky sessions (use centralized sessions)
âœ— Don't skip health checks (unhealthy servers will fail requests)
```

### API Gateway
```
âœ“ Centralize authentication & authorization
âœ“ Implement rate limiting (per user, per IP)
âœ“ Use quotas for long-term limits
âœ“ Transform requests/responses as needed
âœ“ Log all requests (analytics, debugging)
âœ“ Version APIs properly (/v1, /v2)
âœ“ Use circuit breakers (prevent cascade failures)
âœ“ Implement request timeout (don't wait forever)
âœ— Don't put business logic in gateway (keep it thin)
âœ— Don't skip monitoring (track latency, errors)
```

Perfect foundation for load balancing and API gateways! âš–ï¸ğŸš€

