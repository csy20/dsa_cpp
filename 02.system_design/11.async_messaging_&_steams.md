# Async Messaging & Streams

## What Is Async Messaging?

**Decouple components with asynchronous communication**

```
Synchronous (direct call):
Client â†’ Server â†’ Process â†’ Response â†’ Client
â”œâ”€ Client waits for response
â”œâ”€ Server must be available
â””â”€ Tight coupling

Asynchronous (message queue):
Producer â†’ Queue â†’ Consumer
â”œâ”€ Producer doesn't wait
â”œâ”€ Consumer processes independently
â”œâ”€ Loose coupling
â””â”€ Buffer for load spikes

Benefits:
âœ“ Decoupling (producer/consumer independent)
âœ“ Scalability (add more consumers)
âœ“ Reliability (messages persist in queue)
âœ“ Load leveling (buffer handles spikes)
```

---

## 1. Message Queues

### 1.1 Queue Basics

**FIFO (First-In-First-Out) message delivery**

```
Producer â†’ [Message 1 | Message 2 | Message 3] â†’ Consumer
           Queue (persistent storage)

Properties:
â”œâ”€ Messages processed in order (mostly)
â”œâ”€ Each message consumed once
â”œâ”€ Consumer acknowledges after processing
â””â”€ Failed messages requeued
```

### 1.2 Amazon SQS (Simple Queue Service)

**Fully managed, scalable message queue**

#### Standard Queue

```
Characteristics:
â”œâ”€ At-least-once delivery (messages may be duplicated)
â”œâ”€ Best-effort ordering (mostly FIFO, but not guaranteed)
â”œâ”€ Nearly unlimited throughput
â””â”€ Low latency (<10ms)

Flow:
1. Producer: SendMessage("Order#123")
2. SQS: Store message in queue
3. Consumer: ReceiveMessage() â†’ Returns "Order#123"
4. Consumer: Process message
5. Consumer: DeleteMessage() â†’ Remove from queue

If consumer doesn't delete (crash, timeout):
â”œâ”€ Message becomes visible again (after visibility timeout)
â””â”€ Another consumer processes it (retry)
```

**Visibility Timeout:**
```
Timeline:
t0: Consumer A receives message (30s visibility timeout)
    Message hidden from other consumers
t0-t30: Consumer A processing
t30: Visibility timeout expires
    Message visible again
t31: Consumer B receives same message (if A didn't delete)

Purpose: Prevent duplicate processing while consumer works
Typical: 30 seconds - 12 hours

Problem: Duplicate processing if timeout too short
Solution: Set timeout > expected processing time
```

**Long Polling:**
```
Short polling (default):
Client: ReceiveMessage()
SQS: Immediately returns (empty if no messages)
Client: Waits, polls again (wasteful)

Long polling (recommended):
Client: ReceiveMessage(WaitTimeSeconds=20)
SQS: Holds connection for up to 20 seconds
SQS: Returns immediately if message arrives
Client: Receives message (efficient)

Benefits:
+ Reduced API calls (lower cost)
+ Lower latency (immediate notification)
+ Less empty responses
```

**Dead Letter Queue (DLQ):**
```
Failed message handling:

Main Queue â†’ (fails 3 times) â†’ Dead Letter Queue

Configuration:
maxReceiveCount = 3

Flow:
1. Message processed, fails (receive count = 1)
2. Message requeued (visibility timeout)
3. Message processed again, fails (receive count = 2)
4. Message processed third time, fails (receive count = 3)
5. Message moved to DLQ (manual inspection needed)

Use cases:
â”œâ”€ Poison messages (malformed data)
â”œâ”€ Bug in consumer code
â””â”€ External service down

DLQ allows debugging without blocking main queue
```

**Code Example:**

```python
import boto3
import json

sqs = boto3.client('sqs', region_name='us-east-1')
queue_url = 'https://sqs.us-east-1.amazonaws.com/123456789/my-queue'

# Producer: Send message
def send_order(order_id, customer_id, amount):
    message = {
        'order_id': order_id,
        'customer_id': customer_id,
        'amount': amount
    }
    
    response = sqs.send_message(
        QueueUrl=queue_url,
        MessageBody=json.dumps(message),
        MessageAttributes={
            'Type': {
                'StringValue': 'Order',
                'DataType': 'String'
            }
        }
    )
    
    print(f"Message sent: {response['MessageId']}")

# Consumer: Receive and process
def process_orders():
    while True:
        # Long polling (wait up to 20 seconds)
        response = sqs.receive_message(
            QueueUrl=queue_url,
            MaxNumberOfMessages=10,  # Batch up to 10
            WaitTimeSeconds=20,
            MessageAttributeNames=['All']
        )
        
        if 'Messages' not in response:
            continue
        
        for message in response['Messages']:
            try:
                body = json.loads(message['Body'])
                order_id = body['order_id']
                
                # Process order
                print(f"Processing order {order_id}")
                process_order(body)
                
                # Delete message (acknowledge)
                sqs.delete_message(
                    QueueUrl=queue_url,
                    ReceiptHandle=message['ReceiptHandle']
                )
                
                print(f"Order {order_id} completed")
                
            except Exception as e:
                print(f"Error processing message: {e}")
                # Don't delete - message will be retried

# Usage
send_order(order_id='123', customer_id='456', amount=99.99)
process_orders()
```

#### FIFO Queue

```
Characteristics:
â”œâ”€ Exactly-once processing (no duplicates)
â”œâ”€ Strict ordering (within message group)
â”œâ”€ Limited throughput (3,000 msg/sec with batching)
â””â”€ Higher latency than standard

Message Group ID:
â”œâ”€ Messages with same group ID processed in order
â”œâ”€ Different groups processed in parallel
â””â”€ Enables partial ordering

Example:
Group "user:123": [msg1, msg2, msg3] â†’ Ordered
Group "user:456": [msg4, msg5, msg6] â†’ Ordered
Groups processed in parallel (msg1 and msg4 can overlap)
```

**Deduplication:**
```
Content-based deduplication (automatic):
Message 1: Body="Order 123"
Message 2: Body="Order 123" (duplicate!)
SQS: Detects duplicate, discards Message 2 (within 5 min window)

Deduplication ID (manual):
Message 1: DeduplicationId="order-123-20240101"
Message 2: DeduplicationId="order-123-20240101" (duplicate!)
SQS: Discards duplicate

Prevents: Accidental duplicate sends (network retry, producer retry)
```

**FIFO Code Example:**

```python
# Send message to FIFO queue
sqs.send_message(
    QueueUrl='https://sqs.us-east-1.amazonaws.com/123456789/my-queue.fifo',
    MessageBody=json.dumps({'order_id': 123}),
    MessageGroupId='user:456',  # Orders for user 456 processed in order
    MessageDeduplicationId='order-123-20240101'  # Unique ID for deduplication
)
```

### 1.3 RabbitMQ

**Open-source message broker with advanced routing**

#### Architecture

```
Producer â†’ Exchange â†’ Queue â†’ Consumer
           (routing)

Exchange types:
1. Direct: Route by routing key (exact match)
2. Topic: Route by pattern matching
3. Fanout: Broadcast to all queues
4. Headers: Route by message headers
```

**Direct Exchange:**
```
Producer sends with routing key "error":
Producer â†’ Exchange â†’ Queue (binding key: "error") â†’ Consumer

Binding:
Queue "error_logs" bound to exchange with key="error"
Queue "all_logs" bound to exchange with key="*"

Message with routing key "error":
â”œâ”€ Routed to "error_logs" (exact match)
â””â”€ Not routed to "all_logs" (no match)
```

**Topic Exchange (Pattern Matching):**
```
Routing key format: word.word.word (dot-separated)

Bindings:
Queue 1: "logs.error.*" (all error logs)
Queue 2: "logs.*.critical" (all critical logs)
Queue 3: "logs.#" (all logs)

Messages:
"logs.error.database" â†’ Queue 1, Queue 3
"logs.warning.critical" â†’ Queue 2, Queue 3
"logs.info.application" â†’ Queue 3 only

Wildcards:
* (asterisk): Match exactly one word
# (hash): Match zero or more words
```

**Fanout Exchange (Pub/Sub):**
```
Producer â†’ Exchange â†’ Queue 1 â†’ Consumer 1
                   â†’ Queue 2 â†’ Consumer 2
                   â†’ Queue 3 â†’ Consumer 3

All queues receive all messages (broadcast)

Use case: Event notification (user signup â†’ email, analytics, CRM)
```

**Code Example:**

```python
import pika
import json

# Connection
connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
channel = connection.channel()

# Declare exchange and queue
channel.exchange_declare(exchange='orders', exchange_type='topic')
channel.queue_declare(queue='high_priority_orders', durable=True)

# Bind queue to exchange with routing key pattern
channel.queue_bind(
    exchange='orders',
    queue='high_priority_orders',
    routing_key='order.high.*'
)

# Producer: Send message
def send_order(priority, region, order_data):
    routing_key = f"order.{priority}.{region}"
    
    channel.basic_publish(
        exchange='orders',
        routing_key=routing_key,
        body=json.dumps(order_data),
        properties=pika.BasicProperties(
            delivery_mode=2,  # Persistent
        )
    )
    
    print(f"Sent: {routing_key}")

# Consumer: Process messages
def process_order(ch, method, properties, body):
    order = json.loads(body)
    print(f"Processing: {order}")
    
    # Acknowledge message
    ch.basic_ack(delivery_tag=method.delivery_tag)

# Start consuming
channel.basic_consume(
    queue='high_priority_orders',
    on_message_callback=process_order
)

print("Waiting for messages...")
channel.start_consuming()

# Usage
send_order(priority='high', region='us-east', order_data={'id': 123})
```

**Acknowledgments & Redelivery:**
```
Consumer acknowledgment modes:

1. Auto-ack (immediate):
   RabbitMQ â†’ Consumer: Delivers message
   RabbitMQ: Immediately removes from queue (before processing!)
   Risk: Message lost if consumer crashes
   
2. Manual ack (safe):
   RabbitMQ â†’ Consumer: Delivers message
   Consumer: Processes message
   Consumer â†’ RabbitMQ: Ack (acknowledge)
   RabbitMQ: Removes from queue
   
   If consumer crashes before ack:
   â””â”€ RabbitMQ redelivers to another consumer

3. Reject/Nack (negative acknowledgment):
   Consumer â†’ RabbitMQ: Nack(requeue=True)
   RabbitMQ: Requeues message for another consumer
   
   Consumer â†’ RabbitMQ: Nack(requeue=False)
   RabbitMQ: Discards message or sends to DLQ
```

**Prefetch (QoS - Quality of Service):**
```
Problem: Fast consumer gets all messages, slow consumer idle

channel.basic_qos(prefetch_count=10)
â”œâ”€ Consumer receives up to 10 unacknowledged messages
â”œâ”€ RabbitMQ waits for ack before sending more
â””â”€ Load balancing across consumers

Example:
Consumer A (fast): 10 messages in-flight
Consumer B (slow): 2 messages in-flight
Total: 12 messages being processed concurrently
Fair distribution based on processing speed
```

---

## 2. Pub/Sub Systems (Kafka, Pulsar)

### 2.1 Apache Kafka

**Distributed event streaming platform**

#### Architecture

```
Producers â†’ Kafka Cluster â†’ Consumers
             (Brokers)

Kafka Cluster:
â”œâ”€ Broker 1 (leader for some partitions)
â”œâ”€ Broker 2 (leader for other partitions)
â””â”€ Broker 3 (replicas)

Topic: Category of messages
â”œâ”€ Partition 0 (on Broker 1)
â”œâ”€ Partition 1 (on Broker 2)
â””â”€ Partition 2 (on Broker 3)

Consumer Group:
â”œâ”€ Consumer 1 (reads Partition 0, 1)
â””â”€ Consumer 2 (reads Partition 2)
```

#### Topics

```
Topic = Logical category (e.g., "user-signups", "orders", "clicks")

Multiple topics per cluster:
Kafka Cluster:
â”œâ”€ Topic "orders" (3 partitions)
â”œâ”€ Topic "payments" (5 partitions)
â””â”€ Topic "notifications" (2 partitions)

Topics are immutable logs (append-only)
```

#### Partitions

```
Topic split into partitions for parallelism:

Topic "orders" (3 partitions):
Partition 0: [msg0, msg3, msg6, msg9, ...]
Partition 1: [msg1, msg4, msg7, msg10, ...]
Partition 2: [msg2, msg5, msg8, msg11, ...]

Benefits:
+ Parallel processing (3 consumers, 1 per partition)
+ Scalability (add partitions for more throughput)
+ Ordering within partition (global ordering not guaranteed)

Partition assignment:
â”œâ”€ Key-based: hash(key) % num_partitions
â”œâ”€ Round-robin: If no key specified
â””â”€ Custom partitioner: User-defined logic

Example (key-based):
Message key="user:123" â†’ hash("user:123") % 3 = 0 â†’ Partition 0
Message key="user:456" â†’ hash("user:456") % 3 = 2 â†’ Partition 2

All messages for user:123 â†’ Partition 0 (ordered per user)
```

**Partition Internals:**

```
Partition = Ordered, immutable log

Segment files (1GB each):
Partition 0:
â”œâ”€ 00000000000000000000.log (offsets 0-999)
â”œâ”€ 00000000000001000000.log (offsets 1000-1999)
â””â”€ 00000000000002000000.log (offsets 2000-current)

Active segment: Current segment being written
Older segments: Read-only, can be compacted/deleted

Retention policies:
â”œâ”€ Time-based: Delete segments older than 7 days
â”œâ”€ Size-based: Delete oldest segments if total > 100GB
â””â”€ Compact: Keep only latest value per key (log compaction)
```

#### Offsets

```
Offset = Position of message in partition (monotonic, sequential)

Partition 0:
Offset 0: Message "Order 1"
Offset 1: Message "Order 2"
Offset 2: Message "Order 3"
Offset 3: Message "Order 4"
...

Consumer tracks offset:
â”œâ”€ Read from offset 0 â†’ Receives "Order 1"
â”œâ”€ Commit offset 1 (next read starts at 1)
â”œâ”€ Read from offset 1 â†’ Receives "Order 2"
â”œâ”€ Commit offset 2
â””â”€ ...

Consumer crash/restart:
â”œâ”€ Fetch last committed offset (e.g., offset 2)
â”œâ”€ Resume reading from offset 2
â””â”€ No message loss (replay from last commit)
```

**Offset Commit Strategies:**

```
1. Auto-commit (default):
   Consumer automatically commits offset every 5 seconds
   Risk: Message loss if consumer crashes between commits
   
   Timeline:
   t0: Read offset 0-100
   t5: Auto-commit offset 100
   t6: Consumer crashes (processed 0-150, but only 0-100 committed)
   t7: Restart, read from offset 100 (messages 101-150 lost!)

2. Manual commit (after processing):
   Consumer commits only after successful processing
   
   for record in consumer:
       process(record)
       consumer.commit()  # Commit after processing
   
   Safer: No message loss
   Slower: Commit overhead per message

3. Batch commit (balance):
   Commit every N messages
   
   batch_size = 100
   for i, record in enumerate(consumer):
       process(record)
       if i % batch_size == 0:
           consumer.commit()
   
   Balance: Performance + safety

4. At-least-once (commit after):
   Commit after processing â†’ May duplicate on crash
   
5. At-most-once (commit before):
   Commit before processing â†’ May lose on crash
   
   for record in consumer:
       consumer.commit()  # Commit first
       process(record)    # Process after
```

#### Consumer Groups

```
Consumer group = Set of consumers sharing workload

Topic "orders" (3 partitions):
Partition 0 â”€â”
Partition 1 â”€â”¼â”€ Consumer Group "order-processors"
Partition 2 â”€â”˜  â”œâ”€ Consumer A (assigned Partition 0)
                â”œâ”€ Consumer B (assigned Partition 1)
                â””â”€ Consumer C (assigned Partition 2)

Each partition consumed by exactly one consumer in group
Multiple groups can consume same topic (different offsets)

Example:
Group "order-processors": Processes orders
Group "analytics": Analyzes orders (same data, different offset)
Group "notifications": Sends order confirmations

Independent consumption (no interference)
```

**Rebalancing:**

```
Trigger: Consumer joins/leaves group

Initial state:
3 partitions, 2 consumers
â”œâ”€ Consumer A: Partition 0, 1
â””â”€ Consumer B: Partition 2

Consumer C joins:
Rebalance triggered
â”œâ”€ Consumer A: Partition 0
â”œâ”€ Consumer B: Partition 1
â””â”€ Consumer C: Partition 2

Rebalancing process:
1. Stop consuming
2. Reassign partitions
3. Resume consuming

Downtime: Few seconds (no processing during rebalance)

Minimize rebalances:
â”œâ”€ Stable consumer pool
â”œâ”€ Increase session timeout
â””â”€ Fast processing (don't exceed max.poll.interval.ms)
```

#### Producer Code Example

```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
    key_serializer=lambda k: k.encode('utf-8') if k else None,
    acks='all',  # Wait for all replicas to acknowledge
    retries=3
)

def send_order(user_id, order_id, amount):
    message = {
        'order_id': order_id,
        'user_id': user_id,
        'amount': amount
    }
    
    # Send with key (ensures all orders for user go to same partition)
    future = producer.send(
        topic='orders',
        key=f'user:{user_id}',
        value=message
    )
    
    # Wait for acknowledgment
    record_metadata = future.get(timeout=10)
    
    print(f"Sent to partition {record_metadata.partition}, offset {record_metadata.offset}")

# Usage
send_order(user_id='123', order_id='order-456', amount=99.99)
producer.flush()
producer.close()
```

#### Consumer Code Example

```python
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'orders',
    bootstrap_servers=['localhost:9092'],
    group_id='order-processors',
    auto_offset_reset='earliest',  # Start from beginning if no offset
    enable_auto_commit=False,  # Manual commit
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

def process_orders():
    for message in consumer:
        try:
            order = message.value
            print(f"Processing order {order['order_id']}")
            
            # Process order
            process_order(order)
            
            # Commit offset (after successful processing)
            consumer.commit()
            
            print(f"Offset {message.offset} committed")
            
        except Exception as e:
            print(f"Error: {e}")
            # Don't commit - message will be reprocessed

# Usage
process_orders()
```

#### Kafka Guarantees

```
Producer guarantees (acks config):
â”œâ”€ acks=0: No acknowledgment (fastest, may lose messages)
â”œâ”€ acks=1: Leader acknowledges (balanced, may lose if leader fails)
â””â”€ acks=all: All replicas acknowledge (safest, slowest)

Consumer guarantees:
â”œâ”€ At-most-once: Commit before processing (may lose)
â”œâ”€ At-least-once: Commit after processing (may duplicate)
â””â”€ Exactly-once: Transactional API (complex, high overhead)

Ordering guarantees:
â”œâ”€ Within partition: Strict ordering (guaranteed)
â”œâ”€ Across partitions: No ordering (independent)
â””â”€ Use key-based partitioning for related messages
```

### 2.2 Apache Pulsar

**Next-gen distributed messaging (alternative to Kafka)**

#### Differences from Kafka

| Feature | Kafka | Pulsar |
|---------|-------|--------|
| **Architecture** | Broker stores data | Broker + BookKeeper (separated) |
| **Scalability** | Rebalance on scale | Auto-scaling, no rebalance |
| **Multi-tenancy** | Single namespace | Native multi-tenancy |
| **Geo-replication** | MirrorMaker (complex) | Built-in |
| **Retention** | Time/size-based | Unlimited (tiered storage) |
| **Consumption** | Pull-based | Push/Pull |

**Layered Architecture:**
```
Pulsar Broker (stateless):
â”œâ”€ Handles clients
â”œâ”€ Routes messages
â””â”€ No data storage

BookKeeper (storage):
â”œâ”€ Stores messages (distributed)
â”œâ”€ Handles replication
â””â”€ Independent scaling

Benefits:
+ Scale brokers and storage independently
+ Faster rebalancing (brokers are stateless)
+ No data movement when scaling
```

---

## 3. Delivery Semantics

### 3.1 At-Least-Once Delivery

**Message delivered one or more times**

```
Flow:
1. Producer â†’ Broker: Send message
2. Broker: Store message
3. Broker â†’ Producer: Ack (network fails!)
4. Producer: Didn't receive ack, retry
5. Producer â†’ Broker: Send message (duplicate!)
6. Broker: Store message again

Result: Message stored twice

Consumer:
1. Consumer: Read message
2. Consumer: Process message
3. Consumer crashes (before committing offset!)
4. Consumer restarts: Read same message again
5. Consumer: Process message again (duplicate!)

Result: Message processed twice

Characteristics:
+ No message loss (safe)
- Duplicate processing (must handle)
```

**Handling Duplicates:**
```python
# Idempotent processing
processed_ids = set()  # Or database table

def process_message(message):
    message_id = message['id']
    
    if message_id in processed_ids:
        print(f"Already processed {message_id}, skipping")
        return
    
    # Process message
    process_order(message)
    
    # Track processed
    processed_ids.add(message_id)
    
    # Commit offset
    commit_offset()

# Even if message delivered twice, processed only once
```

### 3.2 At-Most-Once Delivery

**Message delivered zero or one time**

```
Flow:
1. Consumer: Read message
2. Consumer: Commit offset (before processing!)
3. Consumer: Process message
4. Consumer crashes during processing
5. Consumer restarts: Reads next message (skips failed one)

Result: Message lost

Characteristics:
+ No duplicates
- Message loss possible
- Fast (no retry overhead)

Use case: Metrics, logs (acceptable to lose some)
```

### 3.3 Exactly-Once Delivery

**Message delivered exactly one time (most complex)**

```
Challenges:
1. Producer duplicate sends
2. Consumer duplicate processing
3. Crash recovery

Solutions:
â”œâ”€ Idempotent producer (detect duplicate sends)
â”œâ”€ Transactional consumer (atomic read+process+commit)
â””â”€ Deduplication (track processed message IDs)
```

**Kafka Exactly-Once Semantics:**

```python
from kafka import KafkaProducer, KafkaConsumer

# Producer: Idempotent (prevents duplicate sends)
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    enable_idempotence=True,  # Exactly-once producer
    acks='all',
    retries=10
)

# Producer: Transactional (atomic multi-partition writes)
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    transactional_id='order-processor-1',  # Unique per instance
    enable_idempotence=True
)

producer.init_transactions()

try:
    producer.begin_transaction()
    
    # Send multiple messages atomically
    producer.send('orders', value={'order_id': 1})
    producer.send('payments', value={'payment_id': 1})
    producer.send('notifications', value={'user_id': 123})
    
    # All or nothing
    producer.commit_transaction()
    
except Exception as e:
    producer.abort_transaction()

# Consumer: Transactional read (exactly-once consumption)
consumer = KafkaConsumer(
    'orders',
    bootstrap_servers=['localhost:9092'],
    group_id='order-processors',
    isolation_level='read_committed',  # Only read committed transactions
    enable_auto_commit=False
)

# Process-then-commit pattern with transactions
for message in consumer:
    try:
        # Process message
        process_order(message.value)
        
        # Atomic commit (offset + output)
        producer.begin_transaction()
        
        # Produce output
        producer.send('order-results', value={'status': 'completed'})
        
        # Commit offset to transaction
        producer.send_offsets_to_transaction(
            {TopicPartition('orders', message.partition): message.offset + 1},
            consumer.group_id
        )
        
        # Commit transaction (atomic)
        producer.commit_transaction()
        
    except Exception as e:
        producer.abort_transaction()
```

**Guarantees:**
```
Exactly-once guarantees:
1. No duplicate messages in topic (idempotent producer)
2. Transactional reads/writes (atomic)
3. Offset committed with output (process + commit atomic)

Trade-offs:
+ True exactly-once (no duplicates, no loss)
- Higher latency (transaction overhead)
- More complex (transactional API)
- Lower throughput

When to use:
âœ“ Financial transactions (no duplicates allowed)
âœ“ Critical data pipelines (ETL)
âœ— High-throughput logs (at-least-once sufficient)
```

### 3.4 Deduplication Strategies

**1. Unique Message ID:**
```python
import hashlib

# Producer: Generate unique ID
def send_message(data):
    message_id = hashlib.sha256(json.dumps(data).encode()).hexdigest()
    
    message = {
        'id': message_id,
        'data': data
    }
    
    producer.send('orders', value=message)

# Consumer: Track processed IDs
processed_ids = {}  # In-memory (or database)

def process_message(message):
    message_id = message['id']
    
    if message_id in processed_ids:
        print(f"Duplicate detected: {message_id}")
        return  # Skip processing
    
    # Process message
    process_order(message['data'])
    
    # Store processed ID
    processed_ids[message_id] = True

# Cleanup old IDs (prevent memory leak)
def cleanup_old_ids(ttl=86400):
    cutoff = time.time() - ttl
    processed_ids = {k: v for k, v in processed_ids.items() if v > cutoff}
```

**2. Database Unique Constraint:**
```sql
CREATE TABLE processed_messages (
    message_id VARCHAR(64) PRIMARY KEY,
    processed_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_processed_at ON processed_messages(processed_at);
```

```python
def process_message(message):
    message_id = message['id']
    
    try:
        # Insert message_id (fails if duplicate)
        db.execute(
            "INSERT INTO processed_messages (message_id) VALUES (?)",
            (message_id,)
        )
        
        # Process message
        process_order(message['data'])
        
        db.commit()
        
    except IntegrityError:
        # Duplicate detected (unique constraint violation)
        print(f"Duplicate: {message_id}")
        db.rollback()
```

**3. Idempotent Operations:**
```python
# Non-idempotent (problematic):
def process_order(order_id, amount):
    current_balance = db.get_balance(user_id)
    new_balance = current_balance + amount  # Duplicate adds amount twice!
    db.update_balance(user_id, new_balance)

# Idempotent (safe):
def process_order(order_id, amount):
    # Check if already processed
    if db.order_exists(order_id):
        return  # Already processed, skip
    
    # Process once
    db.insert_order(order_id, amount)
    db.update_balance(user_id, amount)

# OR use database transaction with unique constraint
def process_order(order_id, amount):
    try:
        db.begin_transaction()
        db.insert_order(order_id, amount)  # Unique constraint on order_id
        db.update_balance(user_id, amount)
        db.commit()
    except IntegrityError:
        db.rollback()  # Duplicate, rollback safely
```

### 3.5 Idempotent Consumers

**Design consumer to handle duplicate messages safely**

```
Idempotency = Multiple executions same as single execution

Examples:

Non-idempotent:
balance = balance + 100
Execute twice: balance + 100 + 100 = balance + 200 âœ—

Idempotent:
balance = 1000
Execute twice: balance = 1000, balance = 1000 âœ“

SET operations are idempotent
INCREMENT operations are not
```

**Patterns:**

```python
# Pattern 1: Natural idempotency (SET operations)
def update_user_profile(user_id, email, name):
    db.execute(
        "UPDATE users SET email=?, name=? WHERE user_id=?",
        (email, name, user_id)
    )
    # Safe: Setting same values multiple times = same result

# Pattern 2: Check-then-set (conditional)
def process_payment(payment_id, amount):
    # Check if processed
    payment = db.get_payment(payment_id)
    
    if payment and payment['status'] == 'completed':
        return  # Already processed
    
    # Process payment
    charge_card(amount)
    
    db.execute(
        "UPDATE payments SET status='completed' WHERE payment_id=?",
        (payment_id,)
    )

# Pattern 3: Unique constraint + INSERT
def record_event(event_id, data):
    try:
        db.execute(
            "INSERT INTO events (event_id, data) VALUES (?, ?)",
            (event_id, data)
        )
    except IntegrityError:
        # Already exists, safe to ignore
        pass

# Pattern 4: Compare-and-swap (version number)
def update_inventory(product_id, quantity_delta, version):
    rows_updated = db.execute(
        "UPDATE inventory SET quantity=quantity+?, version=version+1 "
        "WHERE product_id=? AND version=?",
        (quantity_delta, product_id, version)
    )
    
    if rows_updated == 0:
        # Version mismatch (already updated), retry or fail
        raise ConcurrentModificationError()
```

---

## 4. Backpressure

### 4.1 The Problem

**Consumer can't keep up with producer rate**

```
Scenario:
Producer: 10,000 messages/sec
Consumer: 1,000 messages/sec

Queue growth:
t=1s: 9,000 messages queued
t=2s: 18,000 messages queued
t=10s: 90,000 messages queued

Result: Queue exhausts memory, system crashes
```

### 4.2 Backpressure Strategies

**1. Flow Control (Slow Down Producer):**
```
Producer checks queue depth:

def send_message(data):
    queue_depth = get_queue_depth()
    
    if queue_depth > 10000:
        # Queue too full, slow down
        time.sleep(0.1)  # Backoff
    
    send_to_queue(data)

Or broker rejects:
Producer â†’ Broker: Send message
Broker: Queue full (reject with 429 status)
Producer: Retry with exponential backoff
```

**2. Consumer Scaling (Auto-scale):**
```
Monitor queue depth:
â”œâ”€ Queue depth > 1000: Add consumer instance
â”œâ”€ Queue depth < 100: Remove consumer instance
â””â”€ Dynamic scaling based on load

Kubernetes HPA (Horizontal Pod Autoscaler):
kubectl autoscale deployment order-consumer \
  --min=2 --max=10 \
  --cpu-percent=70

Or based on queue metric:
autoscale based on SQS ApproximateNumberOfMessages
```

**3. Rate Limiting (Throttle Producer):**
```python
import time

class RateLimiter:
    def __init__(self, rate_per_second):
        self.rate = rate_per_second
        self.interval = 1.0 / rate_per_second
        self.last_time = time.time()
    
    def acquire(self):
        now = time.time()
        elapsed = now - self.last_time
        
        if elapsed < self.interval:
            # Too fast, sleep
            time.sleep(self.interval - elapsed)
        
        self.last_time = time.time()

# Usage
limiter = RateLimiter(1000)  # 1000 messages/sec max

for message in messages:
    limiter.acquire()
    send_message(message)
```

**4. Buffering (Temporary Storage):**
```
Multiple buffer levels:

Producer â†’ Local buffer (1000 msg)
         â†’ Message queue (10,000 msg)
         â†’ Consumer buffer (100 msg)
         â†’ Database (unlimited)

Buffers absorb temporary spikes
Consumer drains at steady rate
```

**5. Drop/Sample (Last Resort):**
```python
def send_message(data, priority='normal'):
    queue_depth = get_queue_depth()
    
    if queue_depth > 50000:
        if priority == 'low':
            # Drop low-priority messages
            return
        elif priority == 'normal':
            # Sample (keep 10%)
            if random.random() > 0.1:
                return
    
    send_to_queue(data)

Use case: Metrics, logs (acceptable to lose some under load)
```

**6. Prioritization (Multiple Queues):**
```
High-priority queue: Orders, payments (always processed)
Low-priority queue: Analytics, logs (processed when capacity available)

Consumer processes:
1. Check high-priority queue (always)
2. If empty, check low-priority queue
3. Ensures critical messages processed first
```

### 4.3 Monitoring Backpressure

```
Metrics to monitor:

1. Queue depth (messages waiting):
   Alert if > 10,000 messages

2. Consumer lag (offset behind latest):
   Kafka: consumer lag = latest offset - consumer offset
   Alert if lag > 1,000,000 messages

3. Processing rate (messages/sec):
   Alert if < expected rate

4. Error rate (failed messages):
   Alert if > 1% error rate

5. Consumer CPU/memory:
   Alert if > 80% utilization

Example alerts:
â”œâ”€ Queue depth > 10,000 for 5 minutes â†’ Scale consumers
â”œâ”€ Consumer lag > 1,000,000 â†’ Investigate slow processing
â””â”€ Error rate > 5% â†’ Check consumer code/dependencies
```

---

## Best Practices Summary

```
Message Queues:
âœ“ Use SQS for simple, managed queues (AWS)
âœ“ Use RabbitMQ for complex routing (on-premise)
âœ“ Set visibility timeout > processing time
âœ“ Implement DLQ for poison messages
âœ“ Use long polling (reduce costs, latency)
âœ“ Batch messages (up to 10 for SQS)
âœ— Don't use FIFO queues unless ordering critical
âœ— Don't forget to delete messages after processing

Kafka/Pulsar:
âœ“ Use key-based partitioning for ordering
âœ“ Choose partition count based on parallelism (# consumers)
âœ“ Monitor consumer lag (alert if growing)
âœ“ Commit offsets after processing (at-least-once)
âœ“ Use consumer groups for load balancing
âœ“ Set appropriate retention (time + size)
âœ— Don't create too many partitions (overhead)
âœ— Don't ignore rebalancing (impacts availability)

Delivery Semantics:
âœ“ Use at-least-once for most use cases (default)
âœ“ Implement idempotent consumers (handle duplicates)
âœ“ Use exactly-once for financial/critical data
âœ“ Track message IDs for deduplication
âœ“ Use database unique constraints (enforce once)
âœ— Don't use at-most-once (risk of loss)
âœ— Don't assume exactly-once by default (requires config)

Backpressure:
âœ“ Monitor queue depth (alert on growth)
âœ“ Auto-scale consumers (Kubernetes HPA)
âœ“ Implement rate limiting (protect downstream)
âœ“ Use buffering (absorb spikes)
âœ“ Prioritize messages (critical first)
âœ“ Test under load (chaos engineering)
âœ— Don't ignore growing lag (investigate early)
âœ— Don't drop messages without logging

General:
âœ“ Design for failures (retries, DLQ, idempotency)
âœ“ Monitor end-to-end latency (producer â†’ consumer)
âœ“ Use schemas (Avro, Protobuf) for versioning
âœ“ Encrypt sensitive data (in transit + at rest)
âœ“ Implement proper logging (message IDs, timestamps)
âœ“ Test failure scenarios (consumer crash, network partition)
```

Complete async messaging and streams foundation! ðŸ“¨ðŸŒŠ
